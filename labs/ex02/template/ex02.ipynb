{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_loss` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss by MSE\n",
    "    \n",
    "    e = y - tx @ w\n",
    "    return e.T @ e / (2*len(y))\n",
    "    \n",
    "    # ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "\n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss for each combination of w0 and w1.\n",
    "        \n",
    "    for (i, w0) in enumerate(grid_w0):\n",
    "        for (j,w1) in enumerate(grid_w1):\n",
    "            losses[i,j] = compute_loss(y, tx, [w0, w1])\n",
    "    \n",
    "    \n",
    "    # ***************************************************\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=375870.8203904647, w0*=71.42857142857142, w1*=15.306122448979579, execution time=0.131 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA14AAAIcCAYAAADmP38hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAA9hAAAPYQGoP6dpAADB6ElEQVR4nOzdeXiU5fX/8fckMwkkLAJaQiQqbV0blygWZCnYAhb3UkCFirhVBRcSUYgQHQx7lVCl7gsoAq74069LwSpgVBQwVEXrUqns0iqym0yS+f1x+2SWTJJJMjPPzOTzuq65ZnmWOfMQYE7OfZ/b4fV6vYiIiIiIiEjUpNgdgIiIiIiISLJT4iUiIiIiIhJlSrxERERERESiTImXiIiIiIhIlCnxEhERERERiTIlXiIiIiIiIlGmxEtERERERCTKlHiJiIiIiIhEmRIvERERERGRKFPiJSIiIiIiEmVKvEREktSqVas477zzyM7OxuFw8OKLLzb6HH//+9/p2bMnbdu25bDDDuOPf/wjGzdujHywIiIiSU6Jl4hIktq/fz8nn3wy8+bNa9LxX3/9NRdccAG//e1vWb9+PX//+9/53//+x5AhQyIcqYiISPJzeL1er91BiIhIdDkcDpYuXcqFF15Y81pFRQWTJ0/mqaee4ocffiA3N5dZs2bRv39/AJ577jkuueQSysvLSUkxv6d7+eWXueCCCygvL8flctnwSURERBKTKl4iIi3U5ZdfzjvvvMOSJUv46KOPGDZsGL///e/58ssvAejevTupqak8/vjjVFVVsXv3bp588kkGDRqkpEtERKSRVPESEWkBgite//73vzn66KPZsmUL2dnZNfsNGDCAX//610yfPh0w88SGDRvGd999R1VVFWeccQavvvoqhxxyiA2fQkREJHGp4iUi0gJ9+OGHeL1ejjnmGNq0aVNzW7lyJf/+978B2LFjB1dddRWXXXYZa9asYeXKlaSlpTF06FD0OzsREZHGcdodgIiIxF51dTWpqamsW7eO1NTUgG1t2rQB4G9/+xvt2rVj9uzZNdsWLlxITk4O77//Pj179oxpzCIiIolMiZeISAuUl5dHVVUVO3fupG/fviH3OXDgQK2kzHpeXV0d9RhFRESSiYYaiogkqX379rF+/XrWr18PwMaNG1m/fj2bNm3imGOOYeTIkYwaNYoXXniBjRs3smbNGmbNmsWrr74KwDnnnMOaNWu48847+fLLL/nwww+5/PLLOfLII8nLy7Pxk4mIiCQeNdcQEUlSK1as4Mwzz6z1+mWXXcb8+fPxeDxMnTqVJ554gq1bt9KpUyfOOOMMpkyZwoknngjAkiVLmD17Nl988QUZGRmcccYZzJo1i+OOOy7WH0dERCShJVTitWrVKv7yl7+wbt06tm/fXmtNmtGjR7NgwYKAY3r06MHq1atrnpeXlzN+/HgWL17MwYMH+d3vfsd9991H165dY/UxRERapK1btzJhwgRee+01Dh48yDHHHMOjjz7Kaaed1uCx77zzDv369SM3N7emgiciIpJIEmqo4f79+zn55JOZN29enfv8/ve/Z/v27TU3a8iMZdy4cSxdupQlS5ZQWlrKvn37OPfcc6mqqop2+CIiLdauXbvo3bs3LpeL1157jU8//ZS77747rLb0u3fvZtSoUfzud7+LfqAiIiJRklDNNQYPHszgwYPr3Sc9PZ2srKyQ23bv3s2jjz7Kk08+yYABAwBfh6433niDs846K+Rx5eXllJeX1zyvrq7m+++/p1OnTjgcjiZ+GhGRunm9Xvbu3Ut2djYpKU3/HdmPP/5IRUVFBCPz8Xq9tf4NTE9PJz09vda+s2bNIicnh8cff7zmtaOOOiqs97nmmmsYMWIEqampvPjii80JOalVV1ezbds22rZtq/+bRERiKOz/s70JCvAuXbo04LXLLrvM2759e+9hhx3mPfroo71XXXWV99tvv63Z/o9//MMLeL///vuA40466STv7bffXud73XHHHV5AN9100y3mt82bNzf538mDBw96O0UxtjZt2tR67Y477ggZy/HHH+8dN26cd+jQod7DDjvMe8opp3gfeuihBj/DY4895u3evbvX4/F477jjDu/JJ5/c5OuR7DZv3mz7z6tuuummW0u+NfR/dkJVvBoyePBghg0bxpFHHsnGjRspKirit7/9LevWrSM9PZ0dO3aQlpZGhw4dAo7r3LkzO3bsqPO8hYWFFBQU1DzfvXs3RxxxBJsvgHa3RO3jAPDqib+N7hsEeZTLY/p+jfXGO+fbHYIkgAG9X7I7hDpdyeNh7XdgTyVX5qyibdu2TX6viooKvgNeADKbfJbQ9gND9u1j8+bNtGvXrub1UNUugK+//pr777+fgoICbrvtNj744ANuvPFG0tPTGTVqVMhjvvzySyZOnMjbb7+N05lU/11FhfWzEvxnEi6Px8OyZcsYNGgQLpcr0uG1CLqGzadr2Hy6hs3X2Gu4Z88ecnJyGvw/O6n+J7voootqHufm5tK9e3eOPPJIXnnlFYYMGVLncd4Qw2X81TV0pt0t0K5N82Kuz0snDyIjeqcPyRXzdwzfa6uGRP7boySlN9b/icG/ecHuMEJ6grFcy4Nh7x+JIWOZRO+vTrt27cL6kl9dXU337t2ZPn06YNYR27BhA/fff3/IxKuqqooRI0YwZcoUjjnmmIjHnYysn5Vw/0yCeTweMjIyaNeunb6sNZGuYfPpGjafrmHzNfUaNvR/dkI112isLl26cOSRR/Lll18CkJWVRUVFBbt27QrYb+fOnXTu3NmOEOv00smDYv6eD3BNzN8zXK+tqjtxFgklnn9m4vnvWrR06dKFE044IeC1448/nk2bNoXcf+/evaxdu5brr78ep9OJ0+nkzjvv5J///CdOp5M333wzFmGLiIhETFInXt999x2bN2+mS5cuAJx22mm4XC6WL19es8/27dv55JNP6NWrl11hxoV4/iIYz1+gJb7pZyd+9O7dm88//zzgtS+++IIjjzwy5P7t2rXj448/rlkAev369Vx77bUce+yxrF+/nh49esQibBERkYhJqKGG+/bt46uvvqp5vnHjRtavX0/Hjh3p2LEjbrebP/7xj3Tp0oX//Oc/3HbbbRx66KH84Q9/AKB9+/ZceeWV3HzzzXTq1ImOHTsyfvx4TjzxxJouh/Eg1tUuJV2SzF5bNSQuhx0+wDWNGnKY6PLz8+nVqxfTp09n+PDhfPDBBzz00EM89NBDNfsUFhaydetWnnjiCVJSUsjNzQ04x89+9jNatWpV63UREZFEkFAVr7Vr15KXl0deXh4ABQUF5OXlcfvtt5OamsrHH3/MBRdcwDHHHMNll13GMcccw3vvvRcw0a2kpIQLL7yQ4cOH07t3bzIyMnj55ZdJTU2162NJHZR0SaTE689SPP/SI9JOP/10li5dyuLFi8nNzaW4uJi5c+cycuTImn22b99e59BDERGRRJdQFa/+/fvj9Xrr3P73v/+9wXO0atWKe++9l3vvvTeSoUWMql3x+yVZElu8Vr5aknPPPZdzzz23zu3z58+v93i3243b7Y5sUCIiIjGSUBUviSwlXdLSxOPPVzz+PRQREZHIU+IVR+zoZBhP4vFLsSSfePw5U/IlIiKS/JR4tVD6oictWTwmXyIiIpLclHjFiVhWu+Ix6dIXYYm1ePuZi8e/lyIiIhI5SrzEdvH2BVhajnj72VPyJSIikryUeMWBllztircvvtLy6GdQREREYkGJl9hGX3glXsTTz2K8/XJEREREIkOJl81aarUrnr7oioiIiIhEmxKvFkJJl0j94unnMp7+voqIiEhkOO0OoCVriet2xdOX27jlTtBzJ4HXVg1h8G9esDsMAB7lcuBNu8MQERGRCFHi1QLEy2/PlXSF4I6D94t1DHEunpIvERERSR5KvGzSEqtdQnwmOe4GnrdASr5ERERaEI8HXK6ov40SrySnalcccNsdQCO563jcwij5EhERaQEqK2HgQDj9dJg+PaoJmBIvG8Sq2qWky0ZuuwOIEHcdj0VERESSweTJsHIlfPghjBkD3bpF7a3U1VCiqsUlXW6SN0Fxk7yfrQ4t7udXRESkJXn5ZZg1yzx+7LGoJl2gilfMtaRqV4v60uq2O4AYctfxOElpyKGIiEgS2rgRRo0yj2+6CYYOjfpbquIl0hxuWkTyUSc3LeLzt6hfIoiIiCS78nIYPhx++AF69oTZs2Pytkq8YkjVriThpsUkHGFzk/TXI6l/pkVERFqAoiJo0wbe710Aa9dCx47w9NOQlhaT91fiJRGX1F9Q3XYHEOfc6BqJiIhIXCopgXP3L6HHuvvMCwsXwhFHxOz9lXjFSEupdiVt0uVGCUVjuEnK65W0P98iIiItwPRLP+MRrjJPJk2CwYNj+v5KvJKI3UlXUnKTlAlEzLjtDiDylHyJiIgkoP37uXHVUNqwH848E6ZMiXkISrxiIFbVLrsl1RdSN0mZNNjCTdJdy6T6WRcREUl2Xi9cdx18+ilkZcGiRZCaGvMwlHglCburXUn1RdRtdwBJyo2urYiIiMTeI4/Ak0+aZOvpp03yZQMlXiIWN0oMYsFtdwCRkVS/bJBmWbVqFeeddx7Z2dk4HA5efPHFmm0ej4cJEyZw4oknkpmZSXZ2NqNGjWLbtm0B5ygvL+eGG27g0EMPJTMzk/PPP58tW7bE+JOIiCShDz+EG24wj6dNg9/8xrZQlHhFWSyGGaraFQFuuwNoYdx2BxAZSfGzL822f/9+Tj75ZObNm1dr24EDB/jwww8pKiriww8/5IUXXuCLL77g/PPPD9hv3LhxLF26lCVLllBaWsq+ffs499xzqaqqitXHEBFJPj/8AMOGmXW7zj0XbrnF1nCctr67JLyk+OLptjuAFsoddC+SoAYPHszgOjpjtW/fnuXLlwe8du+99/LrX/+aTZs2ccQRR7B7924effRRnnzySQYMGADAwoULycnJ4Y033uCss86K+mcQEUk6Xi9cfjl8/TUceSQsWAAp9taclHhFUbJXuxI+6XLbHYAACT/E87VVQxj8mxfsDkMSyO7du3E4HBxyyCEArFu3Do/Hw6BBvv8zsrOzyc3N5d13360z8SovL6e8vLzm+Z49ewAzvNHj8TQ6LuuYphwrhq5h8+kaNp+uoZEydy6pL76INy2NqiVL8LZtC2Fek8Zew3D3U+IlLZPb7gAkgJuE/jNR8iXh+vHHH5k4cSIjRoygXbt2AOzYsYO0tDQ6dOgQsG/nzp3ZsWNHneeaMWMGU0K0Q162bBkZGRlNjjG4QieNp2vYfLqGzdeSr2GHf/2LPpMmAfDR6NH859tv4dVXG32ecK/hgQMHwtpPiVcCU7Wridx2ByAhuYPuRZKMx+Ph4osvprq6mvvuu6/B/b1eLw6Ho87thYWFFBQU1Dzfs2cPOTk5DBo0qCapa2x8y5cvZ+DAgbhcrkYfL7qGkaBr2Hwt/hr+9784x47FUVVF9fDhnHDvvZxQz7+loTT2GlojDhqixCtKknntroRNutx2ByBhcZOQf1aqekl9PB4Pw4cPZ+PGjbz55psBiVFWVhYVFRXs2rUroOq1c+dOevXqVec509PTSU9Pr/W6y+Vq1pet5h4vuoaRoGvYfC3yGlZVwejRsHUrHHssKY88QkpaWpNPF+41DPc6q6thgrK7k2HCcdsdgDSK2+4AmiZhfykhUWUlXV9++SVvvPEGnTp1Cth+2mmn4XK5Aoa0bN++nU8++aTexEtERIJMnQrLl0NGBjz/PLRta3dEAVTxigJVu+KM2+4ApEnc6M9OEsK+ffv46quvap5v3LiR9evX07FjR7Kzsxk6dCgffvgh//d//0dVVVXNvK2OHTuSlpZG+/btufLKK7n55pvp1KkTHTt2ZPz48Zx44ok1XQ5FRKQBy5eDNe/1gQfgV7+yN54QVPFKQKp2NYLb7gCkWdx2B9B4CfnLCWmWtWvXkpeXR15eHgAFBQXk5eVx++23s2XLFl566SW2bNnCKaecQpcuXWpu7777bs05SkpKuPDCCxk+fDi9e/cmIyODl19+mdTUVLs+lohI4ti6FUaONC3kr74aLr3U7ohCUsUrwlTtiiNuuwOQiHCjP0uJa/3798fr9da5vb5tllatWnHvvfdy7733RjI0EZHk5/HARRfBf/8Lp5wC99xjd0R1UsUrwdhV7VLSJbZyk1B/pgn390VERCRR3XYbvPMOtG8Pzz0HrVrZHVGdlHhJ8nHbHYBEjdvuAMKn5EtERCTKXnwR7rrLPH78cfjFL2wNpyFKvCIo2sMMVe0Kg9vuACTq3HYHICIiIrb7+mvTOh6goAD+8AdbwwmHEi9JHm67A5CYcdsdQHgS6pcWIiIiieLHH2HoUNi9G3r1gpkz7Y4oLEq8IiRZm2okzBdHt90BSMy57Q5AREREbHHTTVBWBoceCk8/DQmyULQSrwRhxzBDJV0S99x2B9CwhPl7JCIiYrOiImjTxtzXaeFCeOghcDjgqaega9fwj7WZEi9JbG67AxBpmJIvERGRhpWUwP795j6kDRvgmp+KEUVFMGhQ+MfGASVeEZCMTTUS4oui2+4AJC647Q5AREREIiE/HzIzIS8vRPVq3z4YNgwOHIABA+D220MeW1AQ25gbQ4mXJCa33QFIXHHbHUDDEuKXGSIiIjYqLjb5VVlZUPXK6zWVrs8+g+xsM8QwNTXksXfeaZ7H49BDJV5xTtWuENx2ByBxyW13ACIiIhIJtapXDz4IixaZZOvpp+FnP2vwHPE49FCJVzMlazfDuOW2OwCJa267A6hf3P9SQ0REJA4EVK/WrTNdDIHX+8+kze/7hFXFisehh0q84piqXSJN4LY7ABEREYmEaeN38Z/Th0JFBVxwAUPfuznsKlbw0MN4oMSrGV498bd2h9CyuO0OQBKG2+4A6tZSf7nhdrtxOBwBt6ysrHqPKS8vZ9KkSRx55JGkp6fzi1/8gsceeyxGEYuIiK28Xk6eO5qjvP9ho6MbzJ9PfoEj7qpYjaHES2rE9RdCt90BSMJx2x2ABPvVr37F9u3ba24ff/xxvfsPHz6cf/zjHzz66KN8/vnnLF68mOOOOy5G0YqISFNErKnFXXdxbtVL/Eg6r1/5HBxySFxWsRpDiVecivUwQyVdIrET13/fosjpdJKVlVVzO+yww+rc9/XXX2flypW8+uqrDBgwgKOOOopf//rX9OrVK4YRi4hIXepKsEI1tWh0MrZqFRQWAtDqgb9y3cOnRiZomynxkvjmtjsASWhuuwNIfnv27Am4lZeX17nvl19+SXZ2Nt26dePiiy/m66+/rnPfl156ie7duzN79mwOP/xwjjnmGMaPH8/Bgwej8TFERKSRghMsK7nKy6vd1KJRHQa//RYuvhiqqvhn7kjaFPw5rlrCN4fT7gCkNlW7RCLITVwmYK+tGsLg37wQk/fqORTauSJ7zj0e4DnIyckJeP2OO+7A7XbX2r9Hjx488cQTHHPMMXz77bdMnTqVXr16sWHDBjp16lRr/6+//prS0lJatWrF0qVL+d///seYMWP4/vvvNc9LRCQO5OebRMpKsKzkqqzMDAesb986VVXBiBGwfTscfzyDvn6A/QcclJSYZhmJThUviV9uuwOQpOG2O4DQkuGXHps3b2b37t01t8KfhoYEGzx4MH/84x858cQTGTBgAK+88goACxYsCLl/dXU1DoeDp556il//+tecffbZzJkzh/nz56vqJSISB4LnW9XXvj3suVlTpsCbb5oTPfccfy5ok9DNNIIp8Wrh4vaLn9vuAEQkHO3atQu4paenh3VcZmYmJ554Il9++WXI7V26dOHwww+nffv2Na8df/zxeL1etmzZEpHYRUQkcprS+CJg7tff/w5Tp5oNDz0EJ5yQ8M00ginxijN2rN0Vd9x2ByBJyW13AKHF7S8/oqy8vJzPPvuMLl26hNzeu3dvtm3bxj6/8SpffPEFKSkpdO3aNVZhiohIFFnDE5+auZnvfj8SvF649loz3DAJKfFqwVrqFz5pwdx2B9ByjR8/npUrV7Jx40bef/99hg4dyp49e7jssssAKCwsZNSoUTX7jxgxgk6dOnH55Zfz6aefsmrVKm655RauuOIKWrdubdfHEBGRCMrPh0MyKniq8iI68R1lKaeG2YEjPBFrbR8hSrziiKpd6IuxRJ/b7gBapi1btnDJJZdw7LHHMmTIENLS0li9ejVHHnkkANu3b2fTpk01+7dp04bly5fzww8/0L17d0aOHMl5553HPffcY9dHEBGRCCsuhl1/nsAZvMcPtGfFmGehVauInb9R3RRjQF0NJX647Q5AxB6x7HBolyVLltS7ff78+bVeO+6441i+fHmUIhIREds9/zzMnQvAIS8uIP+Cn0f09GF3U4wRVbxaqLgbZui2OwBpUdx2ByAiItLCffklXHGFeXzLLXDBBRF/i3hrzqHEK05omKFIjLntDiBQ3P0yREREJFoOHoShQ2HPHujTB6ZNi7v5WNGgxKsFirsveG67AxARERGRmLnhBvjoIzjsMFiyBFyuuJuPFQ1KvMRebrsDkBbNbXcAgeLulyIiItKiRaUKtWABPPooOBywaBEcfjjgW4A5L6/+90zkypgSrzgQy2GG+mInEsRtdwAiIiLxKVQVqimJj3XMvGs+huuuMy+63TBgQM0+1nyssrL6K1+JXBlT4iX2cdsdgEj80S9HREQkXoSqQoWb+PgnaCUl4Ni/l0EPDzXzu846i9srJodM4PLzweWC8nKzLTjRs2KKl06FjaHEy2YtttrltjsAET9uuwMQERGJP6GqUOEmPv4JWv44L/NTr+IY7xfQtSssXMicuSk1263kqm9f89zrhcpK8zg40Yu3ToWNocRLRATiKvmKq1+SiIhIi+efbAUnPnUNPfSvlh34y9/4Y9Uz4HTCM8/AoYcGnNNKrkpLzb3D4duWyBWuYFpAuYWIqy9ybrsDEBEREZFwFReb+zlzTDXKeg6BFSn/14uLze03rdcwo+KnrGn2bDjjjIDtYM5ZUmKStLIyk2T5V7T8z5vIEqritWrVKs477zyys7NxOBy8+OKLAdu9Xi9ut5vs7Gxat25N//792bBhQ8A+5eXl3HDDDRx66KFkZmZy/vnns2XLlhh+Ch+t3SUNeuv9yNwkPG67A/B5453z7Q5BRERasOBKVl1zu+rtRvj99/y/9GGk4WHDcUNg3LiQFTKrivb224k7jDAcCZV47d+/n5NPPpl58+aF3D579mzmzJnDvHnzWLNmDVlZWQwcOJC9e/fW7DNu3DiWLl3KkiVLKC0tZd++fZx77rlUVVXF6mO0bG67A4hj0UyYlIiJiIhIIwQnWnUN+auzG2F1NYwaRYfd38AvfsGvVj8GDkdCdyVsroQaajh48GAGDx4ccpvX62Xu3LlMmjSJIUPMsLoFCxbQuXNnFi1axDXXXMPu3bt59NFHefLJJxnwU/vKhQsXkpOTwxtvvMFZZ50V8tzl5eWUl5fXPN+zZ0+EP1l0xc0wQ7fdAcQhu5Ig//c9s4c9McQrN/pZFRGRFsvqQug/7A8ChwaGOqa83HQjrEnMZs+GV16B9HR47jlo3x4wCVxJSXLM2WqshKp41Wfjxo3s2LGDQYMG1byWnp5Ov379ePfddwFYt24dHo8nYJ/s7Gxyc3Nr9gllxowZtG/fvuaWk5PT7Hg1zLAFi7fKU7zFEw/cdgcgIiJiD6siVVYW/rC/khLThTAt7af9V6yASZPMxnnz4JRTavYtLjbJ15w5odcCS+QFkhuSNInXjh07AOjcuXPA6507d67ZtmPHDtLS0ujQoUOd+4RSWFjI7t27a26bN2+OcPTRo2pXHEmE5CYRYhQREZGoaUoXwYBjduyAiy82Qw0vuwyuvLJWMjVzpknuZs6sfa5Zs8y2WbMi8nHiStIkXhaHwxHw3Ov11notWEP7pKen065du4CbSNgSMZlJxJgjzW13ACIiIrEX7jpZ/slUzTG3V8Ill8C330JuLtx3X8h5XdbX7lBfv73ewPtkkjSJV1ZWFkCtytXOnTtrqmBZWVlUVFSwa9euOveJhRY3zNBtdwA2SYbkJRk+g4iIiERcyCYZd9xhhhm2aWPmdWVkALWraBMmmOcTJ9Y+78SJZlthYdQ/QswlTeLVrVs3srKyWL58ec1rFRUVrFy5kl69egFw2mmn4XK5AvbZvn07n3zySc0+ySRuhhm2NMmYrCTjZwqH2+4ARERE4lOtIYmvvALTp5vHjzwCxx5bs29wFc167vUGDkG0Gnvk5ydnS/mESrz27dvH+vXrWb9+PWAaaqxfv55NmzbhcDgYN24c06dPZ+nSpXzyySeMHj2ajIwMRowYAUD79u258sorufnmm/nHP/5BWVkZf/rTnzjxxBNruhxKhLntDiDGkj05aakJmIiISJIK1cwinAYX/snT8RnfcGDopQA86BxL0ScXhfWe1nwuq2rWUKv5RG+8kVCJ19q1a8nLyyMvLw+AgoIC8vLyuP322wG49dZbGTduHGPGjKF79+5s3bqVZcuW0bZt25pzlJSUcOGFFzJ8+HB69+5NRkYGL7/8MqmpqTH5DLEaZqhqV4y1tISkJX1Wt90BiIiIRI+V7Eyd2nDzi1CJz90zKph/cDgZP+5ibcrp3Fh5d63Eyf+4oiLzXvv3m6TNv2rWUGOPRF8DLKESr/79++P1emvd5s+fD5jGGm63m+3bt/Pjjz+ycuVKcnNzA87RqlUr7r33Xr777jsOHDjAyy+/HJH28BKC2+4AYqQlJSH+WlqyKSIikoR+qmcAtZtfVFZC377msX/C5J/4zPaOpwcf8D0dWDHmWVyZ6TWJk5VwWYlcSUlgMldYGHoIYl3DDJvScTGeJFTiJRJXlHgYLeEauO0OQEREpHnqGlJYWup77t/8wmJt90+2ahKfZ5/l+up7AXj14icZf++RAYmTVaFyOEzC1KGDSebALLZs7RfuEMJwOy7GKyVeMdSihhm67Q4gylpCstEYSkJFRETiUqiqk/8cK//9/CtPffqYx1bFy6o21ez3+eeU/+kKAFb1msifFp9T672tYyZONAnTli2+bf4dDUMNd0xGSrxEGkMJRv2S+dq47Q5AREQkUKhK0dSpgffBVaeCAt9r1hwr/6TLOmf//jB5Mnz4oW+trrw8cz+g1wEYOpT0in2soB+D3i+mTRuTpPnHE1yh8k/m7rzT916hhjsmIyVeSUbVrihK5qQiknSdREREoqquroBg1iz2v7eSmqoqX5t2qxJlzbHyb+vuX32yzj9rltluDTsc+d5Y+OQTvnN15srWi6lOcbJ/v9leX+Xq7bfNe61aZZ5b7/X+++B0muGHwfO3Er2ToT8lXjHS4hZNTjZKJhonWa+X2+4AREREales/JOVMWPM/dix5n71anNfXe1L0IIrUf7dAvPzfeey5mN5PL6q2eU8xuXMp4oUhnoWM+LmLjULIlsVLeucUH/iZCWAXq95r7S02vO3Er2ToT8lXhJZbrsDiIJkTSKiTddNREQkKoIrVv7JyuTJ5n7SJJPsWMkT1N0N0L9bYHGxOYfLZRIif/de9U/+hsnoiihmBWcydSqsWGHiePttX/J18GBgBc1/blnwUMSePc1zqzrnv19TOhnGa5VMiVcSiYthhslGyUPzJOOcOLfdAYiISEsXbnc//yqRNY8rVFISfL7iYlN98tchdQ9jVwyjNT/yKoOZia87Rmmp77zBFTYrccrL87WjDx6KWFYWeO+frDWlk2G8VsmUeEnkuO0OIMKSLWGwk66liIhIWCJZrQnuRFjXWlzW+6almUqXVWlyOiElBZypXlYdcyV8+SWbHEdwKU+S6vSlESkpgU08/OdrFRebc/m3rYfA9w+uajV3va54Xe9LiVcMaH5XAlKiEHnJdE3ddgcgIiLJKpLVGqta9NZbJiGaNs23LS/Pl+D17WsSMo/HDE20OiJ6PKYph+fue8j97DkqcDHtpGcoz+xEz54m4QI4/HAoLzfJ1sSJ5riKisA5ZJY+fWonRaEqbs1Zryte1/tS4pUkbB9m6Lb37SMqmRKEeKNrKyIiEiC4whWqWtPUKphVxbKqTdacrT59zLA+K8ELrkaBWferTRt48PLVMH48ADdzN49/2oN9+8zx1dVm382b626O4f+ZiopMm3r/WFoSJV4i/pQYRF+yXGO33QGIiEgyCK5wharWBO8TbiJWUmKqT8HeeQcOHDCP8/Kga1fftpQUkyQ5HJC+/zvOXjAcKit51jGMeVxPZWXttbcsHToEDle0WJ/J6617qGNLoMQrylrEMEO33QFESLIkBIlA11pERAQIbz5S8D7hJmL5+SYJCub1+ipOpaWwZYtvW3W1ScoqPdU8yaXkeDfzBUczLvMRwIHXa967rMx0P8zMNHO6wJzHf7iiFY8V38yZvveJt/lXsaDEKwnYPswwGSgRkKZw2x2AiIgkunDmIwXv01Ai5n9cRYUv+XI4zHH+Fa5QvF4oZAZn8xoHacVQnmPbvnY12/1bz+/bZ+Z1Wet4ORy+81jx+Dfe8G/20dIo8ZLmcdsdQAQo6bKHrrskiVWrVnHeeeeRnZ2Nw+HgxRdfDNju9Xpxu91kZ2fTunVr+vfvz4YNGwL2KS8v54YbbuDQQw8lMzOT888/ny3+v4IWkaTU1LlbDSViway1vLxe0wRj2zbzPCUlMFGynMmb3MntAFzH/XzMSQH75eUFJk5WPP37B87dshp45OWZ+CZOjM+mF7GixEtaNn35t5euvySB/fv3c/LJJzNv3ryQ22fPns2cOXOYN28ea9asISsri4EDB7J3796afcaNG8fSpUtZsmQJpaWl7Nu3j3PPPZeqqqpYfQwRsUFTOxiGWog4Px/mzPF1JPRnDQUEk4RZTTH8hxxastjOYi4hlWoe5QoWMJqiIrMgsyVUMw6rM6LFv4FHWVnLTrgsSryiKBbzuzTMsBn0pT8+JPqfg9vuAMRugwcPZurUqQwZUvvfY6/Xy9y5c5k0aRJDhgwhNzeXBQsWcODAARYtWgTA7t27efTRR7n77rsZMGAAeXl5LFy4kI8//pg33ngj1h9HRGLIf3HhuipfffuaqlTfvr7XrITN6jxYVFQ7ifv9733bJkwI/f7BSVcqlSxNv5jO7OSfnMT1zMPhMAlTcXHgEMXgWIOTsbIyXwOOUI04WiJnw7uI1MFtdwCSNN56H87sYXcUIhG3ceNGduzYwaBBg2peS09Pp1+/frz77rtcc801rFu3Do/HE7BPdnY2ubm5vPvuu5x11lkhz11eXk55eXnN8z179gDg8XjwhGpj1gDrmKYcK4auYfO1tGt4++3m1qmTqUL99a/mub9166B1a3Pv8ZiqUmoqtG1rEjKPB+6+2+zbujW4XObarV/voboaHnjADC1MSYG//KX+eO703EbP8lXsoS1/Sl+MI8VJa4eHww6DMWPgxx/Ne4B5z5QU02Bj6lTIyDCJXNeusGsXjB0Lf/ub2f9f/wrdXTFeNfbnMNz9lHhJy5ToVZZklMjJlxv9IkJC2rFjBwCdO3cOeL1z58588803NfukpaXRoUOHWvtYx4cyY8YMpkyZUuv1ZcuWkZGR0eSYly9f3uRjxdA1bL6Wdg2ffNL3+NVXA7ctXhy47dRT4YknGj7nY4/5rqF1nP+5gnVes4ae0+4C4F+3XsfUXl8CXwbs88gjtY+zzv1TEb/O/YM/VyII9+fwgNWbvwFKvBKYrcMM3fa9dbMp6RKRGHMEzV73er21XgvW0D6FhYUU+M2k37NnDzk5OQwaNIh27drVeVxdPB4Py5cvZ+DAgbhC9Z+WBukaNl+yXsOpU33VpsxMX3OL7GwzPNDphPR0UyXyn0tlHXvffb5tU6fC3LmmupSfb/a5+27fvK3WrT089thyrrhiICkprpr3AmjfPnR8fzhlI/e9PxqAeak3cOu9U+Fe0w2xsrL2kESHw1TdTj8dPvoITjrJd79mja+ZB8Dhh8MPP5iK2eTJjb1y9mjsz6E14qAhSryipEWs35WIlHTFN1W9JMlkZWUBpqrVpUuXmtd37txZUwXLysqioqKCXbt2BVS9du7cSa9eveo8d3p6Ounp6bVed7lczfrC2tzjRdcwEpLpGhYVBTadGD/et8Dw7t0mqRk/3td4wpqvlZdn5knl58O118Jdd0FVlZlrNX26SW6Ki80QPysBmznT1zr+xx9dHDjg4uc/N0P/8vPh4MHa8aVRTv7qkbT3/sB79KSg6i48B81JQu0PJnm86Sbf53r/fdM8o00bk0j6++orc3/33RCiSB9wnUpKTJzFxfVc0BgK9+cw3J9VNdeQxnPbHUATKelKDPpzkiTSrVs3srKyAoarVFRUsHLlypqk6rTTTsPlcgXss337dj755JN6Ey8RSQz+HQv916+aOdM378nrrd0ko7TU10DDem3qVJNY+XcltBpqzJxpkjH/c4JZ1Ng6T6j1u+ZQQHfvWv5HJy7iaTykhVx02ZKSYhZY9k8mreJ7fr6p3rlckJMTeFxDCyY3tcNjIlHiJSLxR8mXJJB9+/axfv161q9fD5iGGuvXr2fTpk04HA7GjRvH9OnTWbp0KZ988gmjR48mIyODESNGANC+fXuuvPJKbr75Zv7xj39QVlbGn/70J0488UQGDBhg4ycTkUiwOhdaSZfVCt5aLaK62iQxVnJk7W+pqvJVtMAkVylB3+ALCkKvx+WvqsokYf4uZjFjuY9qHPyJhWzmCByO2kML/bVuHbjdP5ksLjaJX0UFbNpkhha6XCYZq++c0PBaZMlAiVeCUhv5RtIXeYkFt90BiB3Wrl1LXl4eeT/1Sy4oKCAvL4/bf2pNduuttzJu3DjGjBlD9+7d2bp1K8uWLaNt27Y15ygpKeHCCy9k+PDh9O7dm4yMDF5++WVSU1Nt+UwiEjnBix1blR2n0yQa/kmUf/Jkve5wwKxZgfsUFga+x1tvNdw1MDjxOY7PeJirAZjGJP7O72v285+jFcw/CXQ661+bq7gY0tLM+RqqZAVfp2SkxCsKknp+l9vuAJpASVdi0p+bJIj+/fvj9Xpr3ebPnw+Yxhput5vt27fz448/snLlSnJzcwPO0apVK+69916+++47Dhw4wMsvv0xO8DgdEUkKVmVn4kSTaEyc6BueN3GiLzGrrjaveb2BSVVqqklOrGGDbduGXtC4LikpkMF+nmMobdjPP/gt7jC/4KWkmARp8mTzGXr2NDGmpYVeg8z/8yZzJStcSrxEJH4p+RIRkSRRVGQSFGs4YajheXfeGVhRSkurfZ7qanMua9jg3r2Ni6N1Ky8PcB2/4lO20YURLKKaVPr0qT2EMZi13apOlZX55pX5L+bsryVUssKlxCsBaZhhI+iLu8Sa2+4AREQkHpWUmASlrmF3ffuaYYRTp/qGHHboUHuIoDUnrDGcTujTx1Sebs9+hEt5kkpSuZgl7KQzDgf07+9r2mEJnjfWs2fgc/9mGg6HqdTNmhU6ARMlXtIYbrsDaCQlXclBf44iIpIE8vN9jSaCh90VFQUOF/TvSNhYoapWlZXm/MOPLuPGr24A4Dam8za/qXm/mTNNcuYveJppWZmJNTU1MEns0cOcwxoa2djuhFbDkWRP1pR4RVhSz+8SsUuiJV9uuwOIfzNmzKjp+Fefp556ipNPPpmMjAy6dOnC5ZdfznfffRebIEVEmihUIlFcbIYTejyBa3a1aRPYPMOfXw+esAVXrSzt+YFJ64fSinJe4jzuYnzAdqvq5fRb5Te4yUbHjiZW//fweExSV1lphkZOnNj4OV0toZU8KPFKOLYNM3Tb87ZNlmhf1EVakDVr1vDQQw9x0kkn1btfaWkpo0aN4sorr2TDhg08++yzrFmzhquuuipGkYqINE1wImENI+zbNzAps/bzeEy1yBoOaA3xa+z8rbp5eZzL+QVfs5GjuIwFeH9KA1JSTLLVo4epYAUnW/6J2ObNobsnOhy+ZCt4Tlc41ayW0oBDiZckHyVdyUl/rklh3759jBw5kocffpgOHTrUu+/q1as56qijuPHGG+nWrRt9+vThmmuuYe3atTGKVkSkaYITCWsYYWmpqRhZiyH7/zNoDQc8eNCXeDkc1LuYcdjxUMIfeJFy0hjGs/yA742rq02Vqqws9LE9e4ZeeLmoyNfdcPLkuhtohFPNakqyloiUeImIRIPb7gBiY8+ePQG38vLyevcfO3Ys55xzTlgLA/fq1YstW7bw6quv4vV6+fbbb3nuuec455xzIhW+iEjEWZWsvDyYM8c8t+ZO5eQEVoz853BZ87qqq00VyuUyc6kaWni4IWfwLrOYAJgEbB3da+0zdaqJN5TSUtixI/C1oiITV0mJSTK93roTpaZUs5J16KGz4V0kXEk7v8ttdwCNoKpIcnvrfTizh91RJJ5xQJsIn3Mf8By11pq64447cLvdIQ9ZsmQJH374IWvWrAnrLXr16sVTTz3FRRddxI8//khlZSXnn38+9957bzODFxGJHitpsKpcVhIGgYmW02mqSWVlZvs775gEJiXFvG4d73KZxCUvr3HrdQEcyn95huG4qGQxF3M/19W5b1mZqWyFaujhcJh4rWGIxcW+bbNm+ZLJkpLAbda+wa81JD/fnCvZhh6q4pVA1EZeBCXXcWbz5s3s3r275lZYWFjnfjfddBMLFy6kVatWYZ37008/5cYbb+T2229n3bp1vP7662zcuJFrr702kh9BRCRiiopMAw3/9u0FBb6EyesNXCz57bdNklFa6tvWurVJwiydOwcmcuFK8VaxkD/Rla38i2P5Mw8Bjjr3P3CgdtJlDXmsrDTxhuJfwYtUopSsa38p8ZLkoS/kEm/cdgcQfe3atQu4paenh9xv3bp17Ny5k9NOOw2n04nT6WTlypXcc889OJ1Oqqqqah0zY8YMevfuzS233MJJJ53EWWedxX333cdjjz3G9u3bo/3RREQazVqrKz3dJFVW8uDfnbCqyrfgMPjuwbcWlv/wwqa0lAeYUDmDs1jGAVozlOfYR9ta63L5Cx7S6HT6XrPazQfP9fJvvOFyJV+iFGlKvKR+brsDEAlBSXbC+d3vfsfHH3/M+vXra27du3dn5MiRrF+/ntTgxWKAAwcOkBK0II21n7e5kx5EpEWLRPOG4HMUFUF5uUlACgoCt/t3J7T++aqqMomWfxfB4I6C/lJSzByxcBy2fj2TKs34vmt4kA3kBrx3Q4Ljsl7bvNnXUKOoyLegssNhKmLJ2hQjUpR4SXLQF3GRuNa2bVtyc3MDbpmZmXTq1IncXPOFoLCwkFGjRtUcc9555/HCCy9w//338/XXX/POO+9w44038utf/5rs7Gy7PoqIJIGmNm8I1Qp+6lTfc2stK6/XvG69h//CxFanwlBJUH2JkddrEp+GZHu3clpJCSl4eYirWcilIfcLrl75V8NCxVFZaT6n/zDA99/37W8120jGphiRosQrQqLdWEPzu0SCJEqy7bY7gMSxfft2Nm3aVPN89OjRzJkzh3nz5pGbm8uwYcM49thjeeGFF2yMUkSSQVPXjfJPLPLzA1/3P6d/4mHNz7KGG3q9ptrVWOFUq5x4eKJiJOm7d/NPx8ncyD117hs8hLF378A1xEK9f3BC5R9T8DWQ2pR4Sd3cdgcQpkT5Ai4iAVasWMHcuXNrns+fP58VK1YE7HPDDTewYcMGDhw4wLZt21i4cCGHH354bAMVkaTT1OYN/olFcbGvauS/HpfXG5iUWfyHG1ZXNz7mcEznNnpVv4snI4MRaUsoJ7xmRmCSw/LywGQqM9P32OmsPYRy4kRTvbO2tZT1uJpK7eRFJHGpvbyIiMRQcGt0q2q0ZYuvrXpJiUk+wAw3jJULeJFbuAuAshtvZONff1Hv/g5H7Spa8Lwuq4V9nz6mWQiYRMqq+u3bV3+reP8KYWNbyicjVbwksanaJYnAbXcAIiISDf7zpLzewGF2xcWBc7uiqRtfM5/RAPzVOY7tVteLOoRKuoKlpJi1vcDcW9WrvLzwhxNq6GEgJV4JQPO7ROqh5FtERGzy7be+x4WFvmF2RUWQmtr4tbeaIp0feY6hHMJu3qEXRc5pDR4TKulyuUyiaK0z1qtXYJfGWbNM9erdd+s+R7BkXY+rqZR4RUC0G2vYwm13AGHQF24RERGJEKui07dv6HlJoeYrWcmHteaVf8fDaM3jCjaXcZxKGf/lUC7iaSodriadx+s1la2JE80i0GVlvi6Nd97p+6zV1epc2FRKvEQk8SkJFxGRZrLmI5WWmnv/hY39t8+a5UvQrO6E1dUwbZrvuLy82MQ8koVcy4NU42AkT7GVrg0fVIfKysD2+MHDBCdONM/79NHwwaZScw1JTPqiLYnGTWJUkkVEkpBVhcrPr7vJQ36+2efAAVPdCW6rbjWaqKw0TTT8hxH6V7ccDt/cqGg6gQ08+NOoqzu5neUMiti5Z840nxFgzhxzPYIbi0jjqeIltbntDkCkCZSMi4hIHcJZ2NeajzRpkqnoTJwYuN1KppxO03jCn8sFOTnmcatW5r3qWg8rEjLZx7MMI5MDLGcAxYTXrz140eS6WLHXdd3UJr5plHjFOTXWCEFfsEVERKQRGtNdz0rAvF4zv8nlMsMKKypM0jVxYmCFy+k0VbDNm81za70ur9e3aHJkeXmIP3MCn7GVbEbyFNWkhnXkrl21k8Zg1mcEc92cTvPZ/ZOscBJZqU2JVzMlZWMNkUQV70m52+4ARERapqZ01yspMcPtKivNsEKPB9LTzTn8q1nV1YEd/vy3+S+aHCnX8gAjWEwlqQznGf7Lz8I+9uDB+pt+uFwm6bKuU3Gx2d/jgenTffupTXzTKPGSxBLvX6xFREQk4RUVmVbqKSmm4mO1WQ+u/EDtRCacNutNdSrrmMs4ACYwi3fp3ajjQyVdVgUsJcUkWFOn+qp8bdr4jvE/Vm3im0bNNSSQ2+4ARERERGLLv/kGmOQDTLJlNZlo08Y3vC411VTCYukQdvEcQ0mngqVcyByaVm5q2zawEjdpkkkWZ870JVdWlQ9MQlZdbRIxaR4lXnFM87tEmuCt9+HMHnZHISIiCaSuOUtWu3irAmYtJuz1+pIzf8FJTaQ4qGYBl9GN//Bvfs7lPA40rXuHf3xOp6latWljki2n0wyVtJJNMImZKluRoaGGkjg0zFCSgdvuAEREWrZQHfmsdbfy8nxVL38zZ5rExOsNnYR07Wq23XRTdGIez12cz8v8SDrDeJbdHNLkc/nPQSssNPfWnK3CQl8TEYs1dFKdDJtPiZeIJB8l6SIiUodQ1S2rVXxZmZm/5HKZ506nSTSsYYXWULxZswLPuWWLSUqmTYt8vH1ZxXRuA+Am/koZp4Z13C23BD63PovVLr+oKLCJhv+cLf9W+tZ1UifD5lPi1QyPcrndIUSW2+4A6qEv0iIiIhIBVnXrwAFf9Sa4S9+ECeZ5jx6BQwq9XpNg+Q/Fs+zfH/nGGj/jW5ZwMU6qWMhIHuLPYR87d27gc/9uhQBvveVrl29dB6uqBTB5cuA1USfD5tMcLxERERFpMazqltcbWM3Jz689jPDddwOfO50mwYqFFKpYxAiy2c6nHM+1PEBj5nUFJ4ezZpnKllW5sppnhNpWUmIqYMXFvn2KiwOfS+Op4hWn1FhDpJniuUrqtjsAEZGWy1oU2GqUYSUbU6eaClDfvubx/v2BLdS7dvUlM46m9bVolDuYwu94k31k8keeZz9tmnW+ysrAJiF9+gRua9PGVAOtqpb/nC7N74oMVbwk/sXzF2gRERFJKMGVG/8OhR5PYCXI35YtgcdE0yD+zmRMUH/mIf7F8c0+p9PpaxLicJjKX58+5r6iwiSaZWWm0gWB7fPB91hVr6ZTxUsMt90BiESBknYREfETXLmx1u/q2rX2vv4VoVjqymaeYiQpeLmfa1nMiEYd3yZEYSwlxczxsip1Xq9JpN55x9x37lx7/pb/nC7N74oMVbxEREREJKkEL4hsPfafw+Q/p6mueVvW4sGx4qKCZxjOoXzHOk4ln8a3ELQqVv5uu83MX7PmteXlwerVvm6NW7bUruIFVwZV6Wo+VbwkvqliIcnKbXcAIiLJyz/B8n8cXLmxnvfp42shbyktjW3SBTCLCZzBanZxCEN5jnJaReS8M2cGJqNvvw3p6b7tfftqHlcsKPESkeSm5F1EpMUJXhA5M9M8njnTzGd66y3fkLz8fFP9AfuGFwIM4XnymQvAZSzgP3SL2LkdjsAmIkVFvutSVASrVmmdrlhQ4hWHYt7R0B3btxMRERGJpuAFkfft8w2tsxpoWEnGrFm+18vKzPpVzhhPxvkFX/EYVwAwm1t4mfObfC7/OV4Oh0muJk70DbuEwOTKGmKoeVzRp8RL4pcqFSIiItIEoZII/xbw1tDC8nLfPCeAjh1hxQrzWteusWkb34qDPMdQ2rOHt+nDJKY163z+89UmTzZJ5513mgTUquh16OBrmW8lYVaCGryWmUSOEi8RSX5K4kVEkpo1P6lvX1/FJziJmDDBN7Suf39T4aqshNRU3z6bN/vayYdqOBEN93Ajp/BPdnIYF7OESlwNH1QPK+Zbb62dRFmVQP/W+FZyqjle0afES0TELm67AxARSXxFRb7qjf8QwmBWReett3zrdgFUVcUu1mCjWMDVPEI1DkawiG0cHrFzP/WUqdhZt3btzPw267nLZa6dlZwFzwGTyFPiJfFJFQoREREJg3+SZa3HZTXXCFZUVHuB5FhUtULJ5WPu5zoA7mAK/2BARM+/dWvg8717TZXP6zW3tLTAilhdc8AkcpR4tXRuuwMQERERaTr/7ny7dpnXysrM87Q0X2UHAhOKnBzTRCMW87iCtWEvzzKMDA7yOmcxjUkRO3ddn6dt28DnwU00iovNnDA12IiepEu83G43Docj4JaVlVWz3ev14na7yc7OpnXr1vTv358NGzbYGHGgmHc0FGkpVEUVEUlK/k0h/JtqlJT45nFNnWoWQ87L8yVpmzZBz552VLy8PMzVHMfnbKYrf2Ih3iZ+Jc/MNPFbyZbDAePH+x5biopgzx5fc40+fUI30VCDjehKusQL4Fe/+hXbt2+vuX388cc122bPns2cOXOYN28ea9asISsri4EDB7J3714bI5YA+oIsIiIiTVBcbJKvOXNMkpXi903X6zWVMP/EInjYYSyM4T4u5mk8OBnOM3zHoY063n+OllWZmjTJJGGTJ5sbmKQSApOs998PvJfYSsrEy+l0kpWVVXM77LDDAFPtmjt3LpMmTWLIkCHk5uayYMECDhw4wKJFi+o8X3l5OXv27Am4iYhEhNvuAEREkovVJGL1aqiu9r3ucPiSMYfDdECMtdP5gBLMZKpbmc1qzmj0Oaw5Wl6vqeQ5HDB/vm+b5aOPzL3VydB/u13z2lq6pEy8vvzyS7Kzs+nWrRsXX3wxX3/9NQAbN25kx44dDBo0qGbf9PR0+vXrx7vvvlvn+WbMmEH79u1rbjk5OVH/DCISBaqmiojEhaa2Lg+et9W3b+0kyhpu6D/UzuUySVhZmS/p8K92ORzR7+TXge95huGk4eF5hjCXcc06X1WV77Ns2VK7m+OYMbXna02caF4rLGzWW0sTJV3i1aNHD5544gn+/ve/8/DDD7Njxw569erFd999x44dOwDo3LlzwDGdO3eu2RZKYWEhu3fvrrlt3rw5qp8hZtx2ByAiIiItkVWVakz3PKttvDVvq6TElzyVlvqSOTDDCSdM8A017NHD3Ofnh24+UV1thuNFq9GGg2qeYBRH8Q1f8Quu4DGgeW8Wqmrln2Tdd5/5vHfeWfvaaA6XPZIu8Ro8eDB//OMfOfHEExkwYACvvPIKAAsWLKjZxxH0t8rr9dZ6zV96ejrt2rULuEmUqCIhIiKS9PybYITLP0lzOgPncKWkwMyZtZM5a6hhaamplFmvuYLWKLYSEys5ibQJzOJcXuFH0hnKc+yhfcTfw+UKTKj274dZs8zjpiS6EnlJl3gFy8zM5MQTT+TLL7+s6W4YXN3auXNnrSqYiIiIiERHU7rn+beN93jMsEErsaquNtUq/2QuOMnweGDaNJNcWRUwy/TpJjGJRq+1fqxgKqbjxfXM45+cErFzu1ymeUZmphlGGMyqijUl0ZXIS/rEq7y8nM8++4wuXbrQrVs3srKyWL58ec32iooKVq5cSa9evWyM0lAreZEYiMeqqtvuAERE4p9/x8KiIl8y4Z94+Cdz/gsCW7xek2AFdzP0b8IRSZ3ZwRIuJpVqFjCKR7my0ecIrs5ZcnKgogLeftv3uYuKIDvbbPefy6U28fHBaXcAkTZ+/HjOO+88jjjiCHbu3MnUqVPZs2cPl112GQ6Hg3HjxjF9+nSOPvpojj76aKZPn05GRgYjRoywO3SJxy/EIiIiEjf8h8zt22cSimBFRWaInddrkjI7WsYDpFLJEi4mi2/5mFzGcB9Nmdfl8YR+fccOU73LzzfXwZoD17q12b5tW91Jm9gj6SpeW7Zs4ZJLLuHYY49lyJAhpKWlsXr1ao488kgAbr31VsaNG8eYMWPo3r07W7duZdmyZbQNXs5bREREROJKfr5JJsrLTaIRqjvirFm+BhylpdC1qz2x3snt9Gcle2nDUJ7jAJkRO3dKiq96Zw2pDB5aeeihtTs1NrWbpERG0iVeS5YsYdu2bVRUVLB161aef/55TjjhhJrtDocDt9vN9u3b+fHHH1m5ciW5ubk2RmwTt90BiIiIiDROcbFpklFZaeZrTZ1au2lEZWXgMVu2xDZGgLN5hduYAcBVPMIXHBvR87du7WsNb83bsoZeWlUuj6d2MqYmG/ZKusRLRKRBGtYqIpKwrATDv526lXwUFdm/OPARfMOTXArAPMbyDBc1+5zWPDbr3r9JxltvBbaKHzfOPHa5ajfTUJMNeyXdHC8RERERSX5du5pqVt++vqYRoSo5TmftKli0uKjgGYbTkV18wOnczN2NOt76TMFKS83n6N/fNNMAk1hZwynBfPbiYpg8GV59Ff73v9pzvIqLQ8+Lk9hQxUvigyoQIiIiEgZruNyWLSbJWLXKJF8Oh3ndn8MBVVXhnTcSc8HuYjw9+IDv6cBwnqGC9LDfKycHdu0KfM1/mVlreKU1R8vaFtxGX+KXEq84oVbyIi2c2+4AJFoqKyuZPHky3bp1o3Xr1vz85z/nzjvvpNqvf7XX68XtdpOdnU3r1q3p378/GzZssDFqkfjl3yZ+6lSTdNXVudDrDX/oYXPngg3lWW7kXgBG8QTfcFSj3uv772u3wJ882dws/g01JkwwCVfv3r5toaihRvxQ4iUiLZOqrBIjs2bN4oEHHmDevHl89tlnzJ49m7/85S/ce++9NfvMnj2bOXPmMG/ePNasWUNWVhYDBw5kbzRWcxVJEMEJg1XVmjYtsBJUWgrhNKfOyYlOnABH80XNGl0zmcArnNvoc1gVK//P9vjjJskKNcfLWpurrCywYcbUqYH3aqgRPzTHS0REJIree+89LrjgAs455xwAjjrqKBYvXszatWsBU+2aO3cukyZNYsgQM/phwYIFdO7cmUWLFnHNNdfYFruInYITBquqFaqys3evmQPlcECPHqErYJs3RyfO1hzgOYbSjr2s5DdMZmqTzhNq7pVVGSsrM0lWKHl55vPm5Znn990Hjzxi7qdMMVW0khINRYwHSrxaIrfdAYiItBx9+vThgQce4IsvvuCYY47hn//8J6WlpcydOxeAjRs3smPHDgYNGlRzTHp6Ov369ePdd9+tM/EqLy+nvLy85vmePXsA8Hg8eOpacbUe1jFNOVYMXcPm87+GPXrAe++ZROqBB3wLA1syM+Gkk8w+/j76CH75S9i6NTYxP1hxHSdVfcy3dObyVk+S5vACkfkZ6NrVzPs66SQ47DBz/9FHvvsxY+Bf/zLX5l//Mi3kr7/evPcNN3jweOD2280N6l6MWQI19u9yuPsp8RL7aciXiCSxCRMmsHv3bo477jhSU1Opqqpi2rRpXHLJJQDs2LEDgM6dOwcc17lzZ7755ps6zztjxgymTJlS6/Vly5aRkZHR5HiXL1/e5GPF0DVsvuXLl3PjjXDjjQ3vG84+0XLEG2+QN+8JvCkpfDXlekpOLAPKYhrDI4/4Hr/6Kpxyinl88snLefXVmIaSdML9u3zgwIGw9lPiJSIiMTdjxgxuu+02brrppprKTygrV66koKCADRs2kJ2dza233sq1114bu0Aj4Omnn2bhwoUsWrSIX/3qV6xfv55x48aRnZ3NZZddVrOfw39iB2YIYvBr/goLCynwGzu0Z88ecnJyGDRoEO3atWt0nB6Ph+XLlzNw4EBcwT2oJSwt8RpmZ5vhgJmZsG1b088zdaoZGnf99R5OOWU569cPZNYscw1dLtMa3dpn7FiYNAlOOCF2Va1QTqz+JyvKTdYzJeUOZk+f0KTztGlT9zDCW24xzTWszx7ctRF818cS/HNo/RlB8/+cWorG/l22Rhw0RImXiLRcb70PZ/awO4oWZ82aNTz00EOcdNJJ9e63ceNGzj77bK6++moWLlzIO++8w5gxYzjssMP44x//GKNom++WW25h4sSJXHzxxQCceOKJfPPNN8yYMYPLLruMrKwswFS+unTpUnPczp07a1XB/KWnp5OeXrtVtcvlataX/uYeLy3rGl57rZk/dN11tdeMaoy77zbJwbx5poIzb56LgwfNCcePN+t0lZSYeUx33WVaxH/1VYQ+RBO0YzcLuYTW/MirDObOysl4K5vWs+7gQd/j4HW8pk6F6mrz2fPzYeZM01be+dM3+MpKM3ww1LW3fg6vvdYc53A0/8+ppQn373K4f9/V1VBEJF647Q4g+vbt28fIkSN5+OGH6dChQ737PvDAAxxxxBHMnTuX448/nquuuoorrriCu+66K0bRRsaBAwdISQn87zY1NbWmnXy3bt3IysoKGNJSUVHBypUr6dWrV0xjFWksq7OetYBxU+Xnm2rM2LHm+Zgx5nlRkS/p2r/fNJGwGm6E08kwOrw8ypUczVd8wxFcypN4I/SVOrjNvJV0WZ954kRzXQoLAx/Xp7jYJGcVFc3/c5LmUeIlIiJNtmfPnoCbf7OHUMaOHcs555zDgAEDGjz3e++9F9BwAuCss85i7dq1CdW84LzzzmPatGm88sor/Oc//2Hp0qXMmTOHP/zhD4AZYjhu3DimT5/O0qVL+eSTTxg9ejQZGRmMGDHC5uhFYsNK4Pw7FvondMG/p9m/33QytMON3MNQnqcCF8N5hu/p1Oxz+reL9y+eOBy+pNS/hfydd0Yu6ZXY0VBDsZcaa4hE3asn/paMdpH95/7AnkrgTXKCFsa54447cLvdIY9ZsmQJH374IWvWrAnrPXbs2BGy4URlZSX/+9//AoblxbN7772XoqIixowZw86dO8nOzuaaa67hdqvNGHDrrbdy8OBBxowZw65du+jRowfLli2jrX2/0heJiqIi37C5UO3Tg1uhW5q7uHGk9GA1dzEegPHcxQc0fri60wnp6b428ADvvGOqWxbrOhUU+JIsSXxKvOLAa6uG2B2CiEiTbN68OaCRQ6g5R9Z+N910E8uWLaNVq1Zhnz9Uw4lQr8eztm3bMnfu3HqbiDgcDtxud51Jq0iy8B82FyqZGDPG3FtDDsEkIQ6HrxqWkhJYGQu1rpfLFfnW6Z34H88wHBeVPMMw7uWGJp2nZ0+zLpe/4M9QXKxkKxlpqGFL47Y7ABFJJu3atQu41ZV4rVu3jp07d3LaaafhdDpxOp2sXLmSe+65B6fTSVVVVa1jsrKyalqtW3bu3InT6aRTp+YP7RGR2PMfNhfK5MnmftIk32slJYGJSevWZrvXa26hfg8T6aTLQTUL+RNHsJkvOJqreAQI/xdA/jGuXu2br2bp2zdysUr8UuIlIiJR97vf/Y6PP/6Y9evX19y6d+/OyJEjWb9+PampqbWOOeOMM2qtobJs2TK6d+/eYjrGiSSbUPOSiopMS/WiIt9rv/+9SVb69jVD8sB0/HM6TZOImTNjG/dtTOf3/J2DtGIoz7GX8Jds6NMnMJGsrPQ9Tkkxn6lfv9rHhbouktiUeIlIy6Z5hjHRtm1bcnNzA26ZmZl06tSJ3NxcwKxLNWrUqJpjrr32Wr755hsKCgr47LPPeOyxx3j00UcZP368XR9DRKLAGn44dSoceqh57b33zH1pqa8ytGWLr326/3wor9ckMNFyJm8yhTsAGMN9fEz9S2EEKy2tO1GsrjafqaSk9jb/YZmSHJR4iYhIXNi+fTubNm2qed6tWzdeffVVVqxYwSmnnEJxcTH33HNPQq3hJSL169s3cFFga4jg4YfXf5x/4hXqeaR0YRuLuYRUqnmUK5jP5U06j3+Vy+Jw+LoZhhp62dCwTEk8aq4h9lGlQaRFW7FiRcDz+fPn19qnX79+fPjhh7EJSEQioqHOhf785zn5++GHuo9JSYleouUvlUqWcDGd2ck/OYnrmRfR8/fubZps5OeHbgmvBhvJRxUvEZF44rY7ABGR5mnMELmuXc198MoJ1gLKVkXI2g9M0hWLxqbTmMRveJs9tGUYz/Ijrevd3+lsOC7/7dZi0LNmQVqa6cRYVKS5XclMiZeIiIiIRExjhsjt2mXugxdDnjvXnAdMchK8jleoFvKRdB4vMYHZAFzBY3zJMQ0e07MnhOgTFMDrNclk8Gsej2+ul+Z2JS8lXiIiIiISMcXFJmmaM6fuqo1V1cnLM5WiYB6PSTzqGooYTUexkQVcBsBfuZHnGRrWcaWloedy+XM4Aj9TUZFJ2KxtBQWa25XMlHiJiIiISEQFV22Ch8/NnGm2r14d+viUFJN4BFeHoi2Ncp5hOB34gdX04Bb+ErFzZ2YGVsSKiszcLmsx5YwM8zxUy31JDkq8RETU6EVEJKKCqzZWIjZtmqnsWGumOxyh50VVV8Nbb8HbbwfO74q2ORRwOmv5jo4M5xk8pEXs3Pv2wcSJ5rpYSReowtWSKPFqSdx2ByAiIiLJzr+rYXByYc3N8nrN84kTYcIE8zh4yGFpqamSBc/vipaLWMJY7gPgUp5kM0c06niHo/71xIqKTKWvoiJwjpr/0My+fdVYI5kp8bLZa6uG2B2CiIiISMRY1a1Zs3xJhDV8zho66HBAebmpallJmlUN8ue/xhfU7n4YKcfxGY9wFQBTmcRrnN3oc3i99be5LynxLQAd3DjDumZWp8OpU5V8JSMlXmIPDe0SERFJOkVFJqFyuUwiYs3zsuZ4Wbxek4RYica0aWa/k04y22+5JfQQxODuh5GQwX6eYyht2M+bnMkdTIn4e/TtaxqJgK+JRlGRr418Xp6vfb5FXQ2TjxIvEREREYkIq6qTluarYBUUBFZ0LP7rXllJ2nvvmed/+Uv0W8b/9M7cz3X8ik/ZThYjWEQ1DfSED+JyweTJtRPFlBRfMvXhh/D+T79z9nrNraTE10a+rMxUBN9+25wrM9MkY3UNO9RaX4lJiZeIiIiIRIR/owj/uUv+FR2ruYTHA717N+/96ptTFY6reIRRPEklqVzE03xLVqPP0aOH+ayTJgW+3quX+fxWVc8/kbSGV7pcJgH1b6xhDcssK6t72KHW+kpMSrxEREREpF7hVlispMHrNftPm+ZrG5+fb5IJq+lG377NX6ervjlVDcnjQ+7lBgAmMY23+U2TzlNa6mso4q+sLPC1wkJfNctKTCsqTAIaqnW8tYA01D63OiEmphBL1omIiIiI+PhXWIqL697PSkCshMLicPjOMXOmabzhvz0UpxOysqLT1bA9P/Asw2hFOS9zLn/hlmadr6QEOnQwn69tW5MQFhT4hhQWFAQmV3PmmG31XUtrm3V88Lb6jpX4pIqXiIiIiNQr3AqLlVx5PGYYnTW0cOJEcw6n09fZryE9e0arlbyXx7mcX/A1/+FILmMB3mZ+JS4o8MW6d69viCX4HlvVQusahdO5UIspJxclXiIJ6BQ+5zVu4mS+sDsUERFpAcJNAPyHx6WlmWYR1nHFxZCe7tvu31wjlOYOQ6wzRkr4Ay9SThrDeJZddGz2Ob1eX0fCvn0Dk6uZMwMTrfqGEPpTA43ko8RLJAEN5x/8nvcZzj/sDkWiwW13ACIiTVNcHLorn5VEWE02rOYasXYG7zKLCQAUMIe1nB6R85aUQP/+5rP16xeYXFVVBe7nf438K4jBiZYaaCQfJV4iCegPrAi4lwjQ2nIiIhER3JWvpMTM6dq/37RU96+cpTauc3uzHMp/eYbhuKhkMRdzH2Midu68vMBEacUK3zan01cNs9byClVBDE601EAj+SjxEkkwR7GN49gEwPF8w5FsszkiERGR2vwTB6uVutfrq+y0a2fme8VCClUs5E90ZSv/4lj+zENAPeMc62ANjczJMQmVZfVq38LRBQWBwyQnTjRJKJjXc3LMefr2DTx3cKKl+V3JR4mXSII5l1KqfvrPohoH5/KOzRGJiEiiC3c+UWPmHfknDtZiyoWFvjlPe/dGJvZwTGYqZ7GMA7RmKM+xj7ZNOo/Xa4YJbtpkhkpaQwarqnxNQ4Lne3m9psujxWrCETyHTYlW8lPiJZJgLmBVzWNv0HMREZGmCHc+UXPnHXm99TfUiIYBLOcOpgBwLQ+wgdxmna+kxCSeaWkmiczLq704sjXfy+s1TTWsLo+ZmabiBbUrXpL8lHiJJJC27KcfZaRi/oVPxUt/PqQN+22OTEREElm484maOu/Iv8tfq1ZNj7OxDmcLixhBCl4e4mqeZFSzzudwmETLSqYqKwMrV06nuTbW5w0ecrhvn6mWeb2wSr83bXGUeIkkkEG8j4uqgNdcVDEINYYQEZGmC3eYW137hRqC6D+Xa7/f7wdjNcTQiYclXMxh/I8yTuFG7mn2OQ8/vPYQQauC1aePScbuvNOXoFrrmPXpE7iWl7RMSrxEEsh5vI2HwBZQHlI5jygtdiIiIkmpuXO66mp9PnOmGYLnctU/l8vlisznqM8MCunDO+ymHcN4lnKaX2oLXtC5Tx/YvNk89m+cMW2aSb6sdcz8OzxKy+VseBcRibZsdtKZ7+vdxwGcT2nIitcFvM2p/Atv6ENrfEtHtvGz5gUrIiIJz3+uVnFx6H2KisyQOmt///2C53pVVJhhdtXV5taQtDTfOl4OR+AcqUi4kKWM524ALudx/s0vm3Setm3N56moqL3uWJ8+tatfVmJmze0Cc93y8821Umv4lk2Jl0gcWEwRv+GfDe5XXUfr2/bsYx2jGzx+JafQnwcaG56IiCSZ+hKBoiKzrbzc91rwfv7Hz5ljkpLMTHNMqMTL5YIePXxVoa1bfdsinXT9nH8z/6f/E+eQz1KGNPlc+/bVHV9w0pWSYoYiWhUwMOuXFRf7btKyaaihxJ4Wqq3lES7gIGl1JlaWlDpqWnW9bqnGwUHSeJTzmxyjiIgkj/rmdFnVLIfDJFNFRXXP/fJ6AxtuWG3j2wZ1a/d44J13TPv1778PryrWFOn8yLMMoz17eJczmMCsZp0vnKSwqMh8rpQUk1A6nb7OjdbxDQ3tbEybfklcqniJxIEnOZu1HM9SJvBLtpBK5P5HqiKFL8lhCDP5jG4RO6+IiCSnvDxTzenRw8xRCsV/qOG+fea1mTNNl79Uv6nImZm+xhper9k/P9/MgQqV1DS31fxfuYlTKeN/dOIinqaSyE8ma9vWzFtzOEzCdeedJmmyFoOurjYVvrQ0X6WwoaGd4Qz9lMSnipdInPiMbpzKAp5gMECzUy/r+AWczaksUNIlIiJhKSsLvA9VjQluK19S4ls8uLLSJCUuFxw4EHjuggKTWNRVSWrOsMORLOQaHqIaByN5ii3kNP1k9bCahaSmmmTT5TLJqtNpql5Op691vFUpbKgNf1Pb9EtiUeIlEkcO0JorKOIyiignrVYHw3B5SKWcNEZxO1cymYMR6OQkIiItQ3ASYHUnnDnTPLfmgOXnm0SpTRuTePhXqyZONNv8E6m2bRtuV99UJ7CBB7kGgKlMZhlnRfw9XC6TVHXtap5XVvrW8lq92jyuqvK1lPfXULv+cNv5S2JT4iUSh57gHE5jAV9zOFWN/GtaRQr/piunsoAnOTtKEYqISLIKTgKshMq6t4bFzZplOvft328SD/8k6623fEPvLNbwvOYOJwyWyT6eZRiZHGA5A5jCHU06T0NxeTyQng67dtXeVlWlOVrSMCVeInHKGnr4Av0addwL9ONUFvAvDS0UEZEImDDBVHoqK828pQ4dzOv+iVVwkhXc8S96vDzINZzAZ2wlm5E8RXUTR4vUN8zRqnLl5QUujmxVwZxOrdMlDVPiJRLHDtCa7Rwa9pBDD6ls4zANLRQRkWbxn9dVXGwqPV6vqfpYa1XVVSFyOHyJSrRdw4OMZBGVpHIRT/PfCK9VmZJiEizrM5eWmipfXp5pPGKt7zVhguZoScOUeInEMQfVXMQbtRZNrouLKi5mOY4IdkUUEZHkEk7r8uAFkvPzTVXHX0od3yK9Xl+iEk2nso6/chMAE5nJO/SJ2LmtpDIlxddkxF9pae3ktK45WmoVLxYlXiJxrBcf0Znag8mrg+79dWYXZ/BxVOMSEZHEFZxUhRLcYKO42FR2Jk82CZjLBT17Bq5ZFUuHsIvnGEo6FbzIBdzNzWEfGyre4CqdNeywqspci+Bj+vYN7zpC+PtJ8lPiJRLHhvOPWsMMrY6Fc7g4ZOdDD6kM5x+xDFNERBJIOK3Lgys4VtXG4vGYhhrWEMTY8jKf0XTjP3xNN0YzHwg/+wsVb11VOq8XVqzwHZOZaR736wfl5SYBbWh4oVrFi0WJl0icCjXM0OpYeBoLuJlxITsfarihiIjUpymty/07GVqNNCorfY02Ymk8d3EBL1FOGsN4lt0cErX3yskJbBRSXm6SUP/r0NB1VKt4sSjxEolT/sMM61oMua5FlzXcUEREIikvz9wHdy+MxVwuf314mxkUAnAj9/Ahp0X1/bZuNc01wMz3qqw0Sah/8ikSLiVeEntn9rA7goQwnH/gBSobWAw5eNHlSlLw/nS8iIhIJFgNJuyYz2X5Gd/yNBfhpIqFjOQh/tzgMS5X896zutp0L/R64bbbfEMGrUYjwQ1HROqjxEskDlnDDB3AVz8NLWxoMWRr0eV/0xUHaLihiIhEjDVPqa5OhtGWQhWLGEE22/mU47mWBwhnXpfH07z3zckJ7FyYnw9z5kCPHuZ6TJzYvPNLy6LESyQOtaacf3M4j3FuwNDChlhDDx/nHP7N4bSmPMqRiohIsurb11S4+vb1vZaVZe5zcpp//sZUz+5gCr/jTfaTwVCeYz9tGj6oiawqVmYmfP99YEfCWbPM8/ff17wtaTwVSEXi0AFa04eH8DbhdyPW0EMH1U06XkREklNRkUkg8vNN9aahfa2mEqWlvsf795v7SMztCrcb4lm8zmSmAnA1D/MZJzT/zTHVu+qggSE5OXDZZeY6FRSYGK3H/jHHvpOjJAN9KxOJU81NmpR0iYiIv3DWk7Laxs+aVf+5ghMP/zWwIqkrm1nIn0jBy/1cy2JGROS8KSnQunXgHC2Xy1S4wFfNCu5IOHGib+0yLYgsjaVvZiIiIiItQDjrSVnJmcdjEpFwm0fs2uXr/hcpLip4huEcynes41TyidwKxCkp5nM6HL4hjx6Pr2V+XYqLzdplHo8WRJbGU+IlIiIiksT8Fz8ONS/J2l5UZJIzS1qaqfCE01CjQ4fA9a4iYRYTOIPV/EB7hvEs5UFdfZujZ0+ThPboUbt65/H4qln+18aiBZGlqSKaeK1bty6SpxMRERGRRgiVKPgPMWxo+4oV5jWHwyQWxcW150E5HLUrYZFez+uCqhfIZy4Al7GAjfw8oucvKzNJqNUmHwITTKuaFWp4pn93Qw03lMaIaOL1hz/8IZKnExEREZFGCJUo+FdoGtpuVa283tANJIqKTCIWzTbqmdu28WDF1QD8hfG8xAURf4/9+02ilZdnPntREVRVweTJgdUsa+Hojh0DE9Zw5suJBGt0V8Phw4eHfN3r9fK9NSNRRCTRaGFvEUkC+fmBXfjAVGisLobBXfr8txcVmWqWlXBZSYX1Wk6OqfJ4vTBzZnTib+U9yOmzZ9OOvbxNH25jenTeCPM5SkvNXDbrM1vXyfqcVkVs82ZzX1Liq3gFX0eRhjQ68XrjjTd48sknadMmcP0Er9fLqlWrIhaYiEiL5Qb22x2EiCQi/ySrMduLimDq1MDXCgp8CQj4ko+ZM6GyMjLxBpvjGUf7//yHnRzGxSyhEld03siPx2MaaljXxb+aZSVYeXkmCbMSrYaus0gojR5q2L9/f9q0aUO/fv0Cbv379yfPqscmgPvuu49u3brRqlUrTjvtNN5++227QxIRSVr3338/J510Eu3ataNdu3acccYZvPbaa3Xu/8ILLzBw4EAOO+ywmv3//ve/xzBikcQVah5XQ/tODyosWVUg/2YblmglXaNYwOiqx/E6HFye9gTbODw6b4SZo+byy+kqKwMbjFjDDa128m+/rQWTpfnCTrw+//xzwPxn2K9fv5D7vP7665GJKsqefvppxo0bx6RJkygrK6Nv374MHjyYTZs22R2aiEhS6tq1KzNnzmTt2rWsXbuW3/72t1xwwQVs2LAh5P6rVq1i4MCBvPrqq6xbt44zzzyT8847jzL/mfDNMHr0aI3SkKTVmPW6pk0z+wY30PB4TDIWrSGFwXL5mPu5DoB/XXwxb6X+LmLndjh864zl5Jikq6rK3BwOk4ClpvquWfDaXSKREnbiddJJJ3H22WezbNmyaMYTE3PmzOHKK6/kqquu4vjjj2fu3Lnk5ORw//33h9y/vLycPXv2BNxERIRa/zaWl5eH3O+8887j7LPP5phjjuGYY45h2rRptGnThtWrV4fcf+7cudx6662cfvrpHH300UyfPp2jjz6al19+OSJx7927l0GDBtWce+vWrRE5r0g8aMx6Xf4NNKz1rCzV1dGrbvlryx6eYygZHGR5ykC+GDYsouf3es06Y14vbNpk1uHyes3n83pN2/yePc2+CTR4SxJQ2HO8Nm7cyEMPPcTll19Ou3btuOmmmxg1ahQZGRnRjC/iKioqWLduHROD2vEMGjSId999N+QxM2bMYMqUKbEIT0Qk4h7lclxE9t9qDweAN8nJyQl4/Y477sDtdtd7bFVVFc8++yz79+/njDPOCOv9qqur2bt3Lx07dmxixIGef/55vvvuOxYuXMj8+fO54447GDBgAFdeeSUXXHABLlf055WIREs484/y8mqvu+V0mnWtIr0eV/28PMzVHMsXbOFwrkybz70payL6Di6XSUKLisxcrspK38LJKSm+uWwAq1ebSmB+vuZwSeSFXfHKzs7G7XbzzTffMGXKFJYsWULXrl259dZb+eabb6IZY0T973//o6qqis6dOwe83rlzZ3bs2BHymMLCQnbv3l1z22zNLpWmUwc5kaSwefPmgH8fCwsL69z3448/pk2bNqSnp3PttdeydOlSTjjhhLDe5+6772b//v11dtZtik6dOnHTTTdRVlbGBx98wC9/+UsuvfRSsrOzyc/P58svv4zYe23dupU//elPdOrUiYyMDE455ZSAtS+9Xi9ut5vs7Gxat25N//796xyGKdJYoeZ8hRq1W1lpEo9Q+vb1DdeLpLH8jYt4Bg9OhvMM/3McFtHzO50wYYJJrGbONEMovV7IyDBDDT0eM6TQqhI6HGoTL9ETduJ18OBBtm3bxueff052djYFBQVcddVV3H///Rx99NHRjDEqHEH1dK/XW+s1S3p6es2EcOsmIiLU+rcxPT29zn2PPfZY1q9fz+rVq7nuuuu47LLL+PTTTxt8j8WLF+N2u3n66af52c9+FsnwAdi+fTvLli1j2bJlpKamcvbZZ7NhwwZOOOEESiLw7WvXrl307t0bl8vFa6+9xqeffsrdd9/NIYccUrPP7NmzmTNnDvPmzWPNmjVkZWUxcOBA9u7d2+z3l5bLSrhmzTLJxMyZvgTMGlLn/9XH6/UNLfR/vU8fWLUq8oskd2cNczDjIW9lNu/RK7JvgEmurGGV/vPYKipMMmldD6tFvNfrq5CJRFrYQw0zMzNp164dhx12GG3btq35T/aCCy5IqETk0EMPJTU1tVZ1a+fOnbWqYLEw+Dcv8NqqITF/XxGRWEtLS+OXv/wlAN27d2fNmjX89a9/5cEHH6zzmKeffporr7ySZ599lgEDBkQsFo/Hw0svvcTjjz/OsmXLOOmkk8jPz2fkyJG0bdsWgCVLlnDdddeRH6qtWyPMmjWLnJwcHn/88ZrXjjrqqJrHXq+XuXPnMmnSJIYMMf8fLFiwgM6dO7No0SKuueaakOctLy8PmFNnzT/2eDx4PJ5Gx2kd05RjxYi3a/jAAybZaNUK2rc3yYbHY14HaN3a3Kek1G6u4W/dOlMNsvaPhA7e73mufBhpXg//L+UCHkwbS2uHh9atPT/FFplrmJJibu3aBSaWYD4XwF//Crffbq6Ly+VbUDlO/hgbLd5+DhNRY69huPuFnXgNGzaMZcuW8fvf/56bbrqp5j/PRJOWlsZpp53G8uXL+cMf/lDz+vLly7nggsivjB5X3D/dRETigNfrrbMZB5hK1xVXXMHixYs555xzIvreXbp0obq6mksuuYQPPviAU045pdY+Z511VkBVqqleeuklzjrrLIYNG8bKlSs5/PDDGTNmDFdffTVg5lDv2LGDQYMG1RyTnp5Ov379ePfdd+tMvOqaf7xs2bJmzb9evnx5k48VI16u4SOP2B1BHaqr6TF9Ollrv2FfVhbOu4ezODNweYnHHovtNXz11cDr9eqrMX37qIiXn8NEFu41PHDgQFj7hZ14Pf3002zZsoV58+bRs2dPevXqRX5+PmeeeWa4p4gbBQUFXHrppXTv3p0zzjiDhx56iE2bNnHttdfaHZqI2EFzDqPutttuY/DgweTk5LB3716WLFnCihUrapYhKSwsZOvWrTzxxBOASbpGjRrFX//6V3r27FkzSqF169a0b9++2fGUlJQwbNgwWrVqVec+HTp0YOPGjc1+r6+//pr777+fgoICbrvtNj744ANuvPFG0tPTGTVqVM1nCzX3uL451IWFhRT4jYfas2cPOTk5DBo0qEkjUTweD8uXL2fgwIFqLtJEsbqGU6fCfffBmDEweXLt1086CT76yLd96lQz3M7hgHHjTPOM997zHde1K1xyiTl2fwwWb7/ZM5sLKtfyI+kM2PUSH111Ss221q09PPbYcq64YiAHDzbuGrpcpkOh9fmDP8vhh8P27aa6d/jhYDUzdTrhu+8gO9sck5kJ27Y180PaSH+Xm6+x1zDcjudhJ17gW4fl9ttvZ8GCBVx33XWkp6czbtw4Lr/88sacylYXXXQR3333HXfeeSfbt28nNzeXV199lSOPPNLu0EREktK3337LpZdeyvbt22nfvj0nnXQSr7/+OgMHDgTMPCv/tRQffPBBKisrGTt2LGPHjq15/bLLLmP+/PnNjufSSy9t9jnCVV1dTffu3Zn+0wq1eXl5bNiwgfvvv59Ro0bV7NeYucdgqmKh5tS5XK5mfdlq7vES/Wt4990mQbj7bvAvelqvv/mmeT5jhnnNGmLov4+/L780c5z8W8tHSz9W4OZ2AK5nHu+Xnx5yv4MHXY1OvE47Dfr3N4lmKF99FfpxUZFJ2q691iSo110XuLhyotLf5eYL9xqGe53DTrz++te/snfvXvbt21dzf9xxx/Hmm29y1VVXJVTiBTBmzBjGjBljdxgiIi3Co48+Wu/24GRqxYoV0Qsmxrp06VKre+Pxxx/P888/D0BWVhYAO3bsoEuXLjX72DX3WOJffr5JEPwbQBQVQXm5SRh69DBdC8vLTZLldJqbw2GOefzx2o0yYpF0dWYHi7mEVKpZwCge5cqInr+sLLBbo8Nhqlt9+5oqX04O7NhhPmvPnmbfggLfQsnhtOEXaY6wuxouWbKEd955h02bNuH1eunatSu9e/dmzpw5PPPMM9GMUUREJGH17t2bzz//POC1L774omaURbdu3cjKygqYS1BRUcHKlSvp1SvyXd4k8RUXw759voQBfOtTAbz9tknOHA6TcFmLA1dWwrRpgUlXnz5maF2weoqtTZJKJYu5hC7s4BN+xRjuAyL7Jvv3w8GDga+1aWOqYNbiyRUVMHGiSbqsLobBrfZFoiXsitd7/oOBRUQkOtx2ByCRlp+fT69evZg+fTrDhw/ngw8+4KGHHuKhhx4CzBDDcePGMX36dI4++miOPvpopk+fTkZGBiNGjLA5eok3RUWm2hW8wK9VsbLuS0rM8MLMTLM2l5WU+Ve2iopM8lZUFHp43uTJMH16/R0PwzWFOziTFeylDUN5jgOEyPYiwD9WrzdwTS7rulnt5a3Xrceqdkm0hV3xEok4NTQQkRbg9NNPZ+nSpSxevJjc3FyKi4uZO3cuI0eOrNnn1ltvZdy4cYwZM4bu3buzdetWli1bVtPaXsQSnDRYJk40SZa1hrm1IHBBQejqlcMBb73lW+crmNdrEpFIJF1n8wqTMHMcr+ZhPue45p+0DikpviqedV9QEHjd/K+N/2ORaGtUcw0RERFpvHPPPZdzzz23zu0OhwO3243b7Y5dUJKQQs3vgtrzk6znffuGXo/K6zXznuoTieGGR/ANT2Ka2cxjLE9zcbPP6XSahZFDzUtr3doMtQzm9fqu2513+q6VNcQwFnPcRFTxEhEREUkQoeZ31Sc4uYplEdVFBc8wnI7s4gNO52bujsh5J070JUrWgseW4IYjwfO3ghOsWbNMJSxU1U8k0pR4iUjLpiGvIhJHrNbmaWnNa/hgJR3+nE7Yu7d58TXGXYynBx/wPR0YzjNUUHv5g6bwT5J69PANFywqCmyWMW2aSaqmTat7iGbw3DiRaFLiJSIiIhInSkpMIwyPp3aSUJ/g6s7MmSbRsDobgq/BRiiR7mI4nKe5kXsBGMUTfMNRETu3/9DJd94xjUEOHPANJ7QSLP+kykrO8vICr1Pw3DiRaFLi1dK47Q5ARERE6pKfbxIll6txDR+shGPWLNNgwr+LYVWVb7+uXUMfH8mKzzF8ziNcBcAMJvIKdc9vbC7/5Cq4cUafPmZb376+IZrvvx84tLCxQzdFmkPNNeLA4N+8wGurhtgdhj3O7AFvvW93FCIiInGhqYv45uWZ+VyVlbWTKP/5UDt2ND/G+rTmAM8xlLbsYwX9KCJ2PdorKsz9vn2htxcV+aplGloodlDFS0RERCTBlZWZe2fQr9RdLl9b9R496h9u2Hxe7mMMJ/IJO+jMJSymKszf8VtDHZsy5DEz03xOj8dUsupaENl/6KaGFoodlHiJSMsVb4013HYHICKJyhpiN3GiWfjYkpZm2qvv2+dLzqLlCh5jNAuoIoVLWMwOutS5b3CCZVWgevYMvb/L5UsqXa7A4/fvN8dnZtZeNNmffxMOa+HoupI0kWhQ4iUiIiKS4PznKhUX++Y35eWZxCItzTSgiJaT+CfzuB6AIopZwZn17l/XUL/33qt/f5fLPA4+vrDQfH6rWUao+XHB87nq6nQoEi1KvEREREQSQLgVmqIi3/pdpaWmw6HHY5KVlCh882vHbp5jKK35kVc4m5lMjPh7OBy++WtWp0ZrCKXVRt7lMkMN8/PDa5bh34hDJBaUeImIiIgkAP8KTX1JWPBiwNXV5t7hgEmTas8Dax4vj3IlR/MV33AEo3gCb4S/XjqdMGGCSZIcDpNEpqdD//4/ReANbMMf7mLI6mgosabEqyVy2x1AkHibZyMiIhKH/Cs0s2YFtkX3Zw3Dczh8856s12fO9CVikXAj9zCU56nAxXCe4Xs6ReS8mZmQk2MeW/O+KipMcmW12vdPRPPyfMd6vZq/JfFJiZeIiIhIAvCv0FjJVGVl7QTDmuc0ebLZ378RhccTmHhZyU1T9GA1dzEegPHcxQc07RepDocvRmso5LZt8P335nFZmUkwreGSXq+5Bv6JqH/jEIfDt4C05m9JPFHiJSItkyqtIhLn6qvaWMmV01l3gvHWW+Z4/0QruJvg5s1Ni60j3/EMw3FRybMM5V5uaNJ5HA7IyPAlklasU6cGJlb+zTSsz+CfiFr7Op0mQbOqfZq/JfFEiVecGPybF+wOQUTs5LY7ABGJN/V13bOSjh4//Q7Jf6iddVxpqbn3F4mFgx1U8ySXcgSb+YKjuZJHgSYswIWv/XtKiq8TI8B99wUmVhMnmqTK5TKPg1n7WgnpxImavyXxR4mXxAdVH0RERAL4V3z69jVVnL59A/d5//3A+759fcmWw2ESla5dQ5+/KYsVAxQyg7N5jYO0YijPsZd2jT5H8HtXV5vPcMYZ5vnYsebeqvqBqWRVVNSfTKlhhsSziPa1EREREZHIKC4293Pm+JIpq028pbLS3Hs8gW3kwVfd+vbbut/D4WhcFexM3uRObgdgDPfxMSeFf7Cf1FQTc2qqb3ihxwOrV8ONN8Jdd0FVVWDVz7oeIolKFS8RaXlUYRWRBGE1ibDk5ATO+0pN9W0rKfEN18vJ8c138nhCnzvUQsT16cI2FnMJqVTzGJczn8sb92H8WE1BevUyVTmrOuc/16ukRGttSXJR4tVSue0OQERERBriPyRv8mTT6c9/3tfEiSZhcTrNPK+yMrPfpk2mYhQpqVSymEvozE4+4kSuZ16TP4dl/34Tb0WFSbQqKnxDDVNSTLKloYOSTJR4iYiIiMSpCRN8j6dP9zWisCpAxcUmYfF4TBKzf7/pCFhUFJlGGpapTKYfq9hDW4byHAfJaPQ5MjNNRS4z0zfvzL8pCMDrr5v7XbuUbEnyUeIl8UPDv0REpAWqr218cbGpYGVm+uZCVVeb5Co11VS7+vY1xwd3NoyU83iJiZiVmq/gMb7kmHr3D1XdcjpN5ap/f/N8xw5zX1rqi7+oyHwu8CWPdV0XLZAsiUiJl4i0LPGY4LvtDkBE7FRf2/iiIt9cJ39er0nAKit9bePfecckOFC7jbzL1bTYjmIjC7gMgL9yI88zNKzjgpOvHj1MomTNWfPfbsVfUmLayIO5r++61LdNJF4p8YojWstLRESk5QluIOHfOt4/wWgoefJ6fV0OLdbwvqYMO0yjnGcYTgd+YDU9cGf8JazjnM7a7/fOO+ZzVFf71tmyKnnW8MO8PCgvN/ufdJIZQul0hm6soaYbkojUTl5ERETERsXFga3SrZbwpaW+ylBenhmmV1Lia6Jx4IBJcIJbwrtcvk6GHTqYFu3BCVk45lDA6azlOzoynGe44ea0sFq6d+5s5mhZ8YHvPiXFDDn0/+yWNm18wyk/+sh8hszM0HO9gq+ZSCJQxaslc9sdQAjxOAxMREQkhqyW8P4JVVmZr8Pf22+be6uVfHB1yb99/JYtTUu6LmERYzHj/v7EQrY4jgi7arZli6lW+Wvb1iRRhYV1H2dVsQDGjFFFS5KPEi8RaTmU2ItIAnj7bTMMz+n0rW9lJSBWU4m+faP3/sfxGQ/xZwCmMonXGYzX27j5VB5PYEK4d29gW/iGmmNMnty8NvJqviHxSImXiIid3HYHICLxqKTEJC8ZGaZ65PWaYXpTp5q5UqWlppIVqoNgc2Swn+cYShv28yZncgdTarYVFPjawDfEWhTZEpwohmqOYb0WCWq+IfFIiZfEH1UlRESkhQtuHlFSErpBhtPpq441n5cHuJZf8Snb6MIIFlFNas3WqVPNMMKG9O1r1h+z4nW5YNUq89iqROXl1R5K6D/UsLnUfEPikRKvOKPOhiJRooReRBKINZ/LGmqXn++rIPlXkqzkxuNpWsv4FL9vglfzMJeykEpSuZglfEtWwL7BiZ/VjdA/6evTBz780LSNt/jPObMqUWVltYcSFhfDtm2N/wyhBF8/kXigxKulc9sdgIiISOKK1Fwi//OEOmdxsen4Z1W3nE6TaFVWwrRpJhnzT3DCZXURzOND7uFGACYxjbf5TcB+OTm1j333XVO5qqoyzx0OeP99k1gFN/SwPosqUdKSKfGS+KTqhIiIJIDGziWqK1HzP0/wOf2PseZ+VVaaRYkzMxu3RldKiG9+7fmBZxlGK8p5xXEuf+GWWvts3lz7uOpqM9fMv2W8fyz+72V9FlWipCVT4iUiYhe33QGISHM1toITKlErKjILB1vdC/PyzOvWvXXMzJm+BYbBrM8FplW7v8mT626CYVW4fLwscFzOL/ia/3Akl3oX4G3C18OUFFPxcjjMcEOXy9cQpK5FkEVaGiVeIpL8VEEVkShpbAXHP1GzKlkzZ5oKVlqaOc/775t9rXvrmKqqwCF8lZUmIdu7N/A9wm2CAZBPCRd4X6ScNIbxLD84OoZ3oB+XC267zXRg9HhMQmi1k6+uhvR0VbhEQImXxDN9WRYRkSTjn6hZlSyHwyRWeXkmEbOSq+AhhE1pHV/f3LMzeJdZTACggDl8mHI6kyc33FkwuImHx2M+i1WhC66qqdolYijxikMx72zoju3biYiIJLPs7PCabViVrB4//Z7RakyRmmpeLyw0r8+caV6vrq7dNr6+ZMzpNAleqH26OP/LMwzHRSVLHBfzsHMMt91mkr1Qa2n5n6NHD2oSNKuzYUGB6VQIZnihy2Xev6hI1S4RixIvEUluqpyKSJRZQwanTjXPG7tw7+rV5hiv15dw+Q9f9E96/B9b3Q3r0rOnievww81zq9lFClU8XvknurKVf3EsV3sfwlPpoKQEZs0KfS7/6ltpqbnftw/eftsXq5VIFhaaRZ89HvN6Q50fI9UZUiTeKfGS+KYvzZKs3HYHICKRYg0ZvO8+8zxUs41QyUXwUEMr4fJ6A/edMMHXPt4/AcrK8jWwCKW01Jzfmu9lDQGczFTOYhkHaM30vOf40dkWl8ucJ9yW9FZi2beviT/1p3WWQ813a6jzY2M7Q4okKiVeIiIiIs1gVXrGjjXPt20LL/mwjps4MTBhCd63uNgkRBUVpopl2bLFzAer3akwkP8aXANYzh1MAeC18x/giQ9zmTjRNPbwb9LR0GLMeXlmH6v6VV1du1OjlTwGd2kMprW9pKVQ4iWG2+4ARKJAFVMRiQGrYcakSXXvk59vqlYVFb5KVl0dEYM7H6almSSnb19fohMup9NX8cpmK08xkhS8PMxVzP1+FG3amAWY/ed15eSYKltd88ecTjOfy7/DYkpKYOJkJY+zZvlituaABdPaXtJSKPGS+KcvzyIikuCKi01bdY/HNMto08YkUqHmNgV3PrQWTG4o6erTJ/C502kqZF4vOPHwNBfxM/5LGadwI/fUDEX0H77Ypw98/71JmOpamLmw0Fe9Skkx8d92G8yZ4/ssVvLofw5VtKSlU+IVp2Le2VAk2cRzwu62OwARsYOVjDgcJuGxEp9Qc5uKikyV68AB3yLEdS2KbHn//cDFlCsr4Z13zOMZFNKHd9hNO4bzLD/SOmRFK1QyBr55ZH36mITQql61bh3YGt9/eOS+fWYYZWZmdLsbqjmHJAolXiIiIiIxUFxski+v1yRV/q3Yg5WUmMTJ6zXJjccDu3aZbZmZvnbu/o01PJ7aiyl7vXABLzKeuwG4gsf5xvXLmm3BjTn8G31MnuxrC2/tZyVcwfOyQs3TKioynyM/P7rDCNWcQxKFEq9muJLH7Q4hstx2B1CPeK5eiIiIhMlKqNLSzPP9++Gtt8xj/8qNNSfM5Qqd3FjJRkpK4Fws/4oXQDe+Zj6jAZhDPi+7htSsGwa1E6/Jk33DHIuLfW3hrcqVFUvwvKxQ87RilRCpOYckCiVeIpJ8lKjHnRkzZnD66afTtm1bfvazn3HhhRfy+eefh338O++8g9Pp5JRTTolekCIxYCUJHTr45myVlppka+pUk6hY64FZnQyt9vJgkpu33vI1w7Bayluqq03y5HRCOj/yHEM5hN28yxlMYBZer1k3zFJYGJi4+c/TAl8yaL13YypXsUqI1JxDEoUSL0kc+jItycBtdwD2WLlyJWPHjmX16tUsX76cyspKBg0axH7/Vmp12L17N6NGjeJ3v/tdDCIViS4rSbA6DYJpshFcFbKe+ydks2aZJMi/yYb/eRwOk+RYjTz+yk2cShk/OA/lnRueBqeLqipfN0KXyyQrvXub5ykptStU/lWrxs6lUkIkEkiJVxxTgw2RJlCCHpdef/11Ro8eza9+9StOPvlkHn/8cTZt2sS6desaPPaaa65hxIgRnHHGGTGIVCSyrGSlb19fW/iiIl8Hwr59YdUqUx3yl5dnjps50/ea1+tbcNni/zgjw5fkzB+wkGt4iGocHPJ/C7nlnhzS0wOrYz16mFisRC4lxcRXXl67O6H/8EbNpRJpGiVeEshtdwAikkj27NkTcCsvLw/ruN27dwPQsWPHevd7/PHH+fe//80dd9zR7FhFYs2/WlVa6msLX1ICb79tkqBVq8y+xcW+hhlFRWY44P79UFXle81aPLl3b1+HQ6/XDCsMGNK3YQNDl18DQMrtRXDWWUDtBYzLygKTqMJCkxxaMVpx5efDjBlw8KB5L82lEmkap90BiDTKmT3grfftjkIkobzxzvmQ2S6yJ92/B4CcnJyAl++44w7cbne9h3q9XgoKCujTpw+5ubl17vfll18yceJE3n77bZxO/Xclicc/qenTx7R793pDJy7W4sh9+ph9rOGAXq+pQHm95ngw9x6P79jCQr/hfPv2wdChpg/9gAFw++013QWDfy9y4IBJ4srKTFI2Z465LyurvRiyFY/VPl5EGk8Vr2a6lgftDkFELPE+zNBtdwCRt3nzZnbv3l1zKywsbPCY66+/no8++ojFixfXuU9VVRUjRoxgypQpHHPMMZEMWSRmrGF6RUWmwjVhArWG+1n8G20ED+WrrDSVM/9kzOpe2Latr/lG0WQv/PnP8K9/QXY2PPUUpKYya5apnlnHW7xek2Tt22fu9+839/n5tRdDDu6wKCKNp8QrzmmeVwjx/uVapAVp165dwC09Pb3e/W+44QZeeukl3nrrLbrWsxrs3r17Wbt2Lddffz1OpxOn08mdd97JP//5T5xOJ2+++WakP4pIxAU3lwg1R6pvXzNPy0qk+vb1JWx9+gS2e7eGFRYWmu6FYIb/WcMZ9/7lAVi8mCpHKgO/f5qie39Wc5w/a22uhlrV+w83tDosqtol0nRKvKQ2t90BiDSBEvK45vV6uf7663nhhRd488036datW737t2vXjo8//pj169fX3K699lqOPfZY1q9fT48e+vOW+JWdHbrzn39yYzXdsCpde/eaBKlfP9+iw2+/bYb2+UtPN8mPdS6rucZprOWuqnEAFLlm8saPfWoSp4kTA8/Ro4c5z4QJodfh0rpYItGhxEsSk75kS6Jx2x2AvcaOHcvChQtZtGgRbdu2ZceOHezYsYODBw/W7FNYWMioUaMASElJITc3N+D2s5/9jFatWpGbm0tmZqZdH0WkQXV1/vNPbqZN863FBb4EauZM8/q0aSYxy8vzVb8yM33dDsGca8IEODxjF8sPGYazqgIuuIDUW26utdix1bijTx+T7NXXnTAabeAb24peJBkp8RKRxKdEPO7df//97N69m/79+9OlS5ea29NPP12zz/bt29m0aZONUYpERjjVIv/hf5mZJjECXwJmtY4vLfVVv/znYlnras2dU80bh19Ghx/+A926wfz5FE911EqcrGSqrMz3WiwrWmpFL6LEKyKSssGG2+4AwqAv2yIJw+v1hryNHj26Zp/58+ezYsWKOs/hdrtZv3591GMVaa5t2xquFvmv4+WfJFmjaP0bhk6davZzuQJbupeUwHUH7uK4L1+mMjUNnn2WorsPqbey5D9/zL+BRrRp+KKIEq+EoAYbIvVQAi4iCSB4qF3//iYR6dcvcD+rIrVjR+DrpaWmK2F1ta8z4q9/XMV0bgPg+qq/UvTiaQ1WlvwrX7GsQEVj+KJIolHiJYlNX7olEbjtDkBEYsm/uYaVcFlzt6xEp64EyaoM+Q9FdDpNhcq/E+HjM79lYdXFOKniKUbwINfUNOUIp7KkCpRI7CnxEpHEpcRbROJQqATL4QjsaFhR4Rsy6M+qDE2c6FsDzOMxc7w8HtNMY+7dVSyoHEE22/mU47mGBwEHFRXmHOFUloIrUGp+IRJ9Srykbm67AwiTvnyLSAKZMWMGDoeDcePG1bzm9Xpxu91kZ2fTunVr+vfvz4YNG+wLUppk6lRz759Q5eebKpXXax7feadJxjweX2v44KSnqMjXUr5mceSftpWUwPgDU/gdb7KfDIbxHOXONrhc5pxNHTqo5hci0afEK0Ki3WBD87xEgiRKwu22OwCJJ2vWrOGhhx7ipJNOCnh99uzZzJkzh3nz5rFmzRqysrIYOHAge/futSlSCRZORei++8y9lVCBqSylpZn5WVOnmuPz8sw26z446fF/Hrztb+e/zmRMhpe58CE2eE+oqYQ1Z+ighh6KRJ/T7gBEIuLMHvDW+3ZHISJSp3379jFy5EgefvhhplqlEUy1a+7cuUyaNIkhQ4YAsGDBAjp37syiRYu45pprQp6vvLyc8vLymud79uwBwOPx4PF4Gh2fdUxTjm0JHnjANLa45x7zeMwYXwt4y/XXm2t3ww0e/C/jzTfDX/7iOw+YhZH/9S9Tpbr5ZpO0jR1b+7nX67ft682MWvYnHHipuvpqqocPx3qj2283N4Cm/BE29/hI0c9h8+kaNl9jr2G4+zm8Xv/pmxKOPXv20L59exbv/i0Z7Xy56wOE/s8xUl5bNSSq56+T2563bTQlXi1HolS7oOl/f/bvgbPbs3v3btq1a9ekU1j/VvHqbshs2jmiGV9Lc9lll9GxY0dKSkro378/p5xyCnPnzuXrr7/mF7/4BR9++CF5VgkEuOCCCzjkkENYsGBByPO53W6mTJlS6/VFixaRkZERtc8h9nB4PPSZPJmOn3/ODz//OW/PnEl1WprdYYkIcODAAUaMGNHg/4mqeEnyUNVL4o3b7gAkXixZsoQPP/yQNWvW1Nq246e+4Z07dw54vXPnznzzzTd1nrOwsJACv3Fhe/bsIScnh0GDBjUpGfZ4PCxfvpyBAwficrkafXxLMXWqrwI1aVLgNusaXnHFQFJSXGzbFv55s7PNkMLMTEIelzJ+PKmff463fXsyX32V3//85/XGF6oiF852u+nnsPl0DZuvsdfQGnHQECVeCWTwb16wr+olEi8SqdolAmzevJmbbrqJZcuW0apVqzr3czgcAc+9Xm+t1/ylp6eTnp5e63WXy9WsL1vNPT7ZTZlibvVJSXFx3XUuGnMZr732pwWRr/M14MjPN3PEeP55M8YRcCxYgOvYY+s8z913mwTu7rtDx+m/vbo66H3iiH4Om0/XsPnCvYbhXuekaq5x1FFH4XA4Am4TJ04M2GfTpk2cd955ZGZmcuihh3LjjTdSYfVfbaZoN9iwjdvuABpBX8pFJM6sW7eOnTt3ctppp+F0OnE6naxcuZJ77rkHp9NZU+naEbRi7s6dO2tVwSS+BDfcsKbujRnT+IWC/du7BzTU+OoruOIKAEp7jqfNyAvqbfDRUJMM/+3qZCgSW0mVeAHceeedbN++veY22a+OXlVVxTnnnMP+/fspLS1lyZIlPP/889x88802RiwiYUukxNptdwASL373u9/x8ccfs379+ppb9+7dGTlyJOvXr+fnP/85WVlZLF++vOaYiooKVq5cSa9evWyMXBoSnLhYXQ2te2ja+lhWcnTrDQdh6FDYswf69OHcj6Y3mCgFr89V33Z1MhSJraRLvNq2bUtWVlbNrU2bNjXbli1bxqeffsrChQvJy8tjwIAB3H333Tz88MP1js0sLy9nz549ATeJY4n05VzCpz9XSVBt27YlNzc34JaZmUmnTp3Izc2tWdNr+vTpLF26lE8++YTRo0eTkZHBiBEj7A5f6hGcuIwZY+7HjvXt45+c9e1rFlLOyak/GbOSo9v/ewP8859w2GGwZAk3FLgimig1lKSJSGQlXeI1a9YsOnXqxCmnnMK0adMChhG+99575Obmkp2dXfPaWWedRXl5OevWravznDNmzKB9+/Y1t5ycnKh+hvrYtp6X2563bTJ9SReRBHLrrbcybtw4xowZQ/fu3dm6dSvLli2jbdu2docm9QhOXKxBNv5NN/yTs9JS89qWLWEM8VuwAB591GRqixbB4YcrURJJcEnVXOOmm27i1FNPpUOHDnzwwQcUFhayceNGHnnkEcCMnw8eL9+hQwfS0tJqja33V1fnqFCu5cGot5UXaXESLZF22x2AxLsVK1YEPHc4HLjdbtxuty3xSPQUF/saV7z1lkm+cnLg++/rrlzNu+ZjrnjoOjIA3G4YMCBG0YpINMV9xcvtdtdqmBF8W7t2LQD5+fn069ePk046iauuuooHHniARx99lO+++67mfKE6RIXTOapdu3YBtxbJbXcAjZRoX9YlNP05ikiSePttsyDypk31VK727mXQw0PJ4CDLU8+Kas/3psw/E5Gmi/vE6/rrr+ezzz6r95abmxvy2J49ewLw1VdfAZCVlVWrsrVr1y48Hk9CdY6ybbhhItKXdhERSRReL1x1Fcd4v2CLoyvrxi2ElOh9VVNXQ5HYivuhhoceeiiHHnpok44tKysDoEuXLgCcccYZTJs2je3bt9e8tmzZMtLT0znttNMiE7CIRE4iJs5uuwMQkYT1t7/BM8+A00nXVc8w8Yymff8JV36+SbrU1VAkNuK+4hWu9957j5KSEtavX8/GjRt55plnuOaaazj//PM54ogjABg0aBAnnHACl156KWVlZfzjH/9g/PjxXH311REdPpi063lBYn6pTMQv76I/NxFJWNnZTRi+98EHvgxo9mw444yIxxVMzTpEYitpEq/09HSefvpp+vfvzwknnMDtt9/O1VdfzeLFi2v2SU1N5ZVXXqFVq1b07t2b4cOHc+GFF3LXXXfZGLnEhL7ESyy47Q5AROJBo4fvff89DB8OHg8MGQLjxkUrNBGxUdwPNQzXqaeeyurVqxvc74gjjuD//u//YhBRdA3+zQu8tmqI3WGIRIcSZRFJYJmZcN11ga8VFZlkLD/f1+UQgOpqGDUKvvkGfvELeOwx00JeRJJO0lS8JIbcdgfQRPoynxj05yQicaKpXf+2bas9fK/ORhazZsErr0B6Ojz3HLRv36yYRSR+KfGKkqSe55XI9KU+viXyn4/b7gBEJNKa2vUv1Bwv/4WUa6xY4WsXP28enHJKyPOp7btIclDilcBsbSvvtu+tmy2Rv9yLiEjMhEyWwhAqWavVyGLHDrjkEt9QwyuvrPN8avsukhyUeIlIfEjkhNhtdwAiEg1N7frXYLJWWWmSrh074Fe/gvvuq3deV1MTQBGJL0nTXEOkUc7sAW+9b3cUYknkpEtEJMi2beBy1bPDHXeYYYZt2ph5XZmZ9Z6vuDioIYeIJCRVvKIoFvO8NNywGfRlPz7oz0FEWpJXX4Xp083jhx+G446zNx4RiRklXtKy6Uu/vZLh+rvtDkBEEsamTXDppebx2LFw8cX2xiMiMaXES5rHbXcAEZAMX/5FRCS+VVSYRZK//x5OPx3uvtvuiEQkxpR4JQFbhxsmCyVfsZcM19xtdwAikjBuuQXefx8OOQSeecas24VaxYu0JEq8oqxFrOfltjuACEmGRCBR6FqLSEvy7LNwzz3m8ZNPwlFH1WxSq3iRlkOJl4g/JQTRlyzX2G13ACKSEL74wrdG18SJcO65AZvVKl6k5VDilSRsH27otvftIypZEoN4pGsrIgkgYsP/DhyAoUNh717o1y9kT/imrhUmIolHiVcMtIjhhslGCULkJdM1ddsdgIhEU8SG/11/PXz8MXTuDIsXg1PLp4q0ZEq8ROqSTImC3XQtRSSBRGT43+OPm1tKikm6unSJWHwikpiUeCURDTeMAiUMzZds19BtdwAiEm3hDv+rc0jiP/8JY8aYx3feCWeeGZU4RSSxKPESaUiyJQ6xpGsnIkks1JBE54EDOC+5BH78EQYPhsJC+wIUkbiixCtGWsw8L7fdAUSJEojGObNHcl4zt90BiEg8qTUk0evllHnzcHz1FRxxhGkdn6KvWiJi6F+DJGP7cMNkloyJRDToOolICxE8JDHlb3/j8HffxetymUWSO3WyN0ARiStKvCTy3HYHEEXJWsmJlGS+Nm67AxCRuLZ6NSm33gpA9axZ0COJ/z0UkSZR4hVDsRpuqKpXDCRzgtEUSkhFpCX77jsYPhxHZSVbe/WieuxYuyMSkTikxEuiw213ADGgRMNoCdfBbXcAIhK3qqvh0kth82a8v/wl66+/HhwOu6MSkTikxEuix213ADHQ0is9Lfmzi0jSqLMtfDhmzIDXXoNWrahcsoTKjIyIxyciyUGJV4xpuGGSamkJSEtKON12ByAi0RaqLXxY3nwTbr/dPL7vPjjppIjHJiLJQ4mXRJfb7gBiqCUkIy3hM4pIi1OrLXw4tm+HSy4xQw2vuAIuvzxq8YlIcnDaHYBI0rESk7fetzeOSGuJCZfb7gBEJBaKi80tbJWVcPHFsHOnqXLNmxe12EQkeajiZYMWN9zQbXcANkmW6lCyfA4RkUiZPBlWrYK2beG556B1a7sjEpEEoIqXSLQlagWspSdbbrsDEJG49H//B7NmmcePPQZHH21vPCKSMFTxSnKqesWRRKgcWTHGe5ySkFatWsV5551HdnY2DoeDF198scFjysvLmTRpEkceeSTp6en84he/4LHHHot+sCKh/Oc/MGqUeXzjjTB0qK3hiEhiUcXLJtfyIA9wjd1hiB38k5p4qIIpyarNbXcAyWn//v2cfPLJXH755fzxj38M65jhw4fz7bff8uijj/LLX/6SnTt3UllZGeVIRUIoL4dhw2DXLujRA/7yF7sjEpEEo8SrBRj8mxd4bdUQu8MwX2bdNscQb+xKwpRs1c1tdwDJa/DgwQwePDjs/V9//XVWrlzJ119/TceOHQE46qijohSdSANuvhnWroWOHeGZZyAtze6IRCTBKPGS2HKjL7Z1CU6GIpmIKdGSKNmzZ0/A8/T0dNLT0yNy7pdeeonu3bsze/ZsnnzySTIzMzn//PMpLi6mtZoZSCwtWQJ/+5t5/OSTcMQR9sYjIglJiZeNNNxQ6qVkKfbcdgfgM6D3S7wRqZPNIPL/2v802i8nJyfg5TvuuAO32x2Rt/j6668pLS2lVatWLF26lP/973+MGTOG77//XvO8JHb+9S+46irzeNIkOPtse+MRkYSlxKuFiJvhhqCql0gS2bx5M+3atat5HqlqF0B1dTUOh4OnnnqK9u3bAzBnzhyGDh3K3/72N1W9JPr27zcNNPbvhzPPhClT7I5IRBKYuhqKiEBc/TIgbrqRhqFdu3YBt0gmXl26dOHwww+vSboAjj/+eLxeL1u2bInY+4iE5PXCmDGwYQNkZcGiRZCaandUIpLAlHjZLFaLKUOcfZlz2x2AiB+33QFIKL1792bbtm3s27ev5rUvvviClJQUunbtamNk0iI8+ig88QSkpJg5XllZdkckIglOiZfYx213ACLxJ65+QRJh+/btY/369axfvx6AjRs3sn79ejZt2gRAYWEho6w1koARI0bQqVMnLr/8cj799FNWrVrFLbfcwhVXXKFhhhJdZWVw/fXm8bRp0K+fvfGISFJQ4tXCJPOXOpEmcdsdQMuxdu1a8vLyyMvLA6CgoIC8vDxuv/12ALZv316ThAG0adOG5cuX88MPP9C9e3dGjhzJeeedxz333GNL/NJC7N5t1usqL4dzz4Vbb7U7IhFJEmquEQdadHdDN/riK/Zx2x1AoGT/xUj//v3xer11bp8/f36t14477jiWL18exahE/Hi9cPnl8O9/w5FHwoIFZqihiEgE6F+TFijuvty57Q5AREQEmDsXli41iyM/+6xZLFlEJEKUeMWJWDbZEBHiLuGPu1+IiLQ0777rG1ZYUgKnn25vPCKSdJR4SXxw2x2AtChuuwMQkbjy3//C8OFQWQkXXwzXXWd3RCKShJR4tVBx+dt1t90BiIhIi1NVBX/6E2zdCsceCw89BA6H3VGJSBJS4hVHNNxQJAbcdgdQW1z+IkSkpZg2DZYtg9at4bnnoG1buyMSkSSlxKsFi8sve267A5Ck5rY7ABGJK8uXg9ttHj/4IOTm2hqOiCQ3JV5xRlUv9OVYWpS4/AWISEuwdSuMHGlayF99NVx6qd0RiUiSU+LVwsXtlz633QFI0nHbHYCIxA2PBy66yDTVOOUU0KLcIhIDSrxEJPm57Q4gtLj9xYdIsrvtNnjnHWjXzqzX1aqV3RGJSAugxCsOxXq4Ydx++XPbHYAkBbfdAYhIXPl//w/uuss8fvxx+OUv7Y1HRFoMJV4S39x2ByAJzW13AHWL2194iCSzr7+Gyy4zj/PzYcgQe+MRkRZFiVecUtXLj9vuAEQiK67/vknEzZgxg9NPP522bdvys5/9jAsvvJDPP/88YB+v14vb7SY7O5vWrVvTv39/NmzYYFPESerHH2HYMNi9G844A2bNsjsiEWlhlHhJYnDbHYAkHLfdAYgYK1euZOzYsaxevZrly5dTWVnJoEGD2L9/f80+s2fPZs6cOcybN481a9aQlZXFwIED2bt3r42RJ5n8fPjwQ+jUCZ5+GlwuuyMSkRZGiVcznP3xm3aHEFH6LbwkDbfdAdRNf89antdff53Ro0fzq1/9ipNPPpnHH3+cTZs2sW7dOsBUu+bOncukSZMYMmQIubm5LFiwgAMHDrBo0SKbo08STz0FDzwADod5nJNjd0Qi0gI57Q5A6nYtD/IA18T0PQf/5gVeWxWnY97dxPUXaokTbrsDEKnf7t27AejYsSMAGzduZMeOHQwaNKhmn/T0dPr168e7777LNdeE/n+gvLyc8vLymud79uwBwOPx4PF4Gh2XdUxTjo1rn36K889/xgFU3XYb1b/9rWknHwVJew1jSNew+XQNm6+x1zDc/ZR4NdP5/1zGSycPanhHiQw3+mItdXPbHUD9VO0Sr9dLQUEBffr0ITc3F4AdO3YA0Llz54B9O3fuzDfffFPnuWbMmMGUKVNqvb5s2TIyMjKaHOPy5cubfGy8ST14kH633ELbAwfYefLJvHfqqfDqq1F/32S6hnbRNWw+XcPmC/caHjhwIKz9lHjFOVW9QnAT91+wxQZuuwMQadj111/PRx99RGlpaa1tDocj4LnX6631mr/CwkIKCgpqnu/Zs4ecnBwGDRpEu3btGh2bx+Nh+fLlDBw4EFcyzH/yekkdPZqULVvwZmfT4ZVXOPtnP4vqWybdNbSBrmHz6Ro2X2OvoTXioCFKvCQxudEXbfFx2x1Aw1TtkhtuuIGXXnqJVatW0bVr15rXs7KyAFP56tKlS83rO3furFUF85eenk56enqt110uV7O+bDX3+LjxwAOweDGkpuJ4+mlchx8es7dOmmtoI13D5tM1bL5wr2G411nNNSLg/H8ui+r5Y91aHhLkS6Lb7gBERBrm9Xq5/vrreeGFF3jzzTfp1q1bwPZu3bqRlZUVMKSloqKClStX0qtXr1iHmxzWrYObbjKPZ86EPn3sjUdEBFW8JNG5UQLW0rntDqBhCfGLDImasWPHsmjRIv7f//t/tG3btmZOV/v27WndujUOh4Nx48Yxffp0jj76aI4++mimT59ORkYGI0aMsDn6BLRrl1mvq6ICLrgAbr7Z7ohERABVvKQeCfNl0W13AGIbt90BiDTs/vvvZ/fu3fTv358uXbrU3J5++umafW699VbGjRvHmDFj6N69O1u3bmXZsmW0bdvWxsgTkNcLo0fDxo3QrRvMn29ayIuIxAElXhGSjMMNE4rb7gAk5tx2BxCehPkFhkSN1+sNeRs9enTNPg6HA7fbzfbt2/nxxx9ZuXJlTddDaYS77oKXXoL0dHjuOTjkELsjEhGpocRL6pVQXxrddgcgMeO2O4DwJNTfH5FEV1oKhYXm8V//Cqeeam88IiJBlHhFULJWvRLqy6Pb7gAk6tx2ByAicWfnTrjoIqiqgpEj4c9/tjsiEZFalHhJ8nHbHYBEjdvuAMKXUL+wEElkVVUwYgRs2wbHH2/ayGtel4jEISVeCUZVrzC57Q5AIs5tdwAiEpemTIF//AMyMsy8rjZt7I5IRCQkJV4RFu3hhnZS8iW2cJNwf5YJ93dFJFH9/e8wdap5/PDDcMIJ9sYjIlIPJV4JSB0OG8FtdwDSLG67A2g8JV0iMbJ5s5nP5fXCtdea4YYiInEsYRKvadOm0atXLzIyMjikjvawmzZt4rzzziMzM5NDDz2UG2+8kYqKioB9Pv74Y/r160fr1q05/PDDufPOO/F6vRGNVVWvOOO2OwBpErfdAYhI3KqoMM00vvvOdC8sKbE7IhGRBiVM4lVRUcGwYcO47rrrQm6vqqrinHPOYf/+/ZSWlrJkyRKef/55bvZbsX7Pnj0MHDiQ7Oxs1qxZw7333stdd93FnDlzYvUxIkZVr0Zy2x2ANIrb7gCaJiF/MSGSiCZMgPfeg/bt4dlnoVUruyMSEWmQ0+4AwjVlyhQA5s+fH3L7smXL+PTTT9m8eTPZ2dkA3H333YwePZpp06bRrl07nnrqKX788Ufmz59Peno6ubm5fPHFF8yZM4eCggIcdXRBKi8vp7y8vOb5nj17IvvhEszg37zAa6uG2B1G47mD7iU+ue0OoGmUdInEyPPPw9y55vGCBfDzn9sajohIuBKm4tWQ9957j9zc3JqkC+Css86ivLycdevW1ezT7/+3d+9xUZX5H8A/yGUGL+AFAwYUcV+ZFt7CVFCiTDE0ra0My/XSD0pUJEVrVfzJ6KaUueSmodsi6u6amrf9WZFCpqiZJQSFl1eZoGBCrqaAqFyf3x+zzDYw4AAzc86Z+bxfr/OCOfPMOd/zzDkzz3eec54TGgqVSmVQ5sqVK7h48WKTy05MTIS7u7t+6tGjxz3jscbphlL2eim6kamVOgBqklbqAIhI1n76Cfif/9H9v3Ah8PTT0sZDRNQCNpN4lZSUwNPT02Bely5d4OLigpKSkibL1D+uL2PM4sWLUVpaqp+KiorMHD1ZnVbqAMiAFop+TxT9QwSRUty5Azz/PFBWBowcCaxaJXVEREQtImnipdVq4eDg0OyUlZVl8vKMnSoohDCY37BM/cAaTZ1mCAAqlQpubm4GkynY6yVzWqkDIACKfx8UfxwQKcXcucB33wHduwM7dgDOzlJHRETUIpJe4xUTE4PJkyc3W6ZXr14mLcvLywtff/21wbwbN26gurpa36vl5eXVqGfr6tWrANCoJ4xMo9jrveppG/wl69JKHQARKcLWrcCmTYCDA/Dhh4CPj9QRERG1mKSJl4eHBzw8PMyyrKCgIKxcuRLFxcXw9vYGoBtwQ6VSITAwUF9myZIlqKqqgouLi76MRqMxOcFrqYnfpWP/wDCLLLteNP6KjZhp0XXYPC2YBFiTVuoAzIO9XURWkJcH1I9orNUCo0dLGg4RUWsp5hqvwsJC5ObmorCwELW1tcjNzUVubi5u3boFAAgLC8ODDz6IqVOnIicnB4cOHcLChQvxyiuv6E8NfOmll6BSqTBjxgycPn0a+/btw6pVq5od0ZDuzWYan1rYTEIga1qpAzAPm9nvieSsvByYNEl3fVdYGLB0qdQRERG1mmISr2XLlmHw4MFISEjArVu3MHjwYAwePFh/DZijoyM+/fRTqNVqjBgxAi+88AKeeeYZrFmzRr8Md3d3ZGRk4PLlyxgyZAhmz56NuLg4xMXFSbVZZiP1fb1sqhGqlToAG6UF65aITCcE8MorwA8/6E4t/Oc/gXaKabYQETWimPt4bdmypcl7eNXr2bMnPvnkk2bL9O/fH0ePHjVjZPdmjdMN5UDx13v9lrbBX2o9rdQBmJ9N/dBAJFfJycDOnYCTE/DRR7pBNYiIFIw/HdkQqXu9bJIWNpk4WI1W6gDMj0kXkRWcOgXMn6/7f/VqIDhY2niIiMyAiZeVWGNoeTmw2UapVuoAFEYLm6wzm92/ieTk119113VVVwO//z0wb57UERERmQUTLxsjh14vm22camGTyYRZacE6IqLWq6sDpk8HLl0Cfvc7YPNm3RDyREQ2gImXFVmr14vJl4VpweSiIS1svk5sep8mkovVq4FPPgFUKmD3bsDdXeqIiIjMRjGDaxDJjrbBX3uklToA62DSRWQFmZlAfLzu//XrgUGDJA2HiMjc2ONlZez1skFa2EWPjwEt7GZ77WY/JpJSSQkwebLuVMNp04DISKkjIiIyO/Z4kUXZ1BDzptA2+GtLtFIHQEQ2qaYGeOklXfL10EO6YeR5XRcR2SD2eEnAnnq9ADvtMdDCNnqFtLCN7Wglu9x3iawtIQE4fBjo2BHYswfo0EHqiIiILIKJF1mFXTdgtVBe8qKFsuK1ALveZy0oOTkZ/v7+UKvVCAwMxLFjx5otv23bNgwcOBDt27eHt7c3Xn75ZVy/ft1K0ZLFpaUBq1bp/k9JAR54QNp4iIgsiImXROyt14v+Qwt5JmJayDMuiTDpsoydO3di3rx5iI+PR05ODkJCQhAeHo7CwkKj5Y8fP45p06YhMjISZ86cwa5du3Dq1ClERUVZOXKyiMJCYOpU3f9z5gAREdLGQ0RkYUy87IBcki82Zo3QwvoJT8N1Wmu9CsH91HKSkpIQGRmJqKgo9OvXD2vXrkWPHj2wYcMGo+VPnjyJXr16ITY2Fv7+/hg5ciRmzpyJrKwsK0dOZldVpbtJ8q+/Ao88Avz5z1JHRERkcRxcQ0ITv0vH/oFhUodhVXY32EZLac1QxpRlkFFySroisRmfSx2ECcrKygweq1QqqFSqRuWqqqqQnZ2NRYsWGcwPCwvDiRMnjC47ODgY8fHxSEtLQ3h4OK5evYrdu3dj/Pjx5tsAksbChcA33wBdugAffaS7bxcRkY1j4mUnovFXbMRMqcMAwOSrzbRSB0CKcywLgLkHLKgAAPTo0cNgbkJCArRabaPS165dQ21tLTw9PQ3me3p6oqSkxOgagoODsW3bNkRERODu3buoqanBxIkTsW7dOvNsAklj1y6g/j38xz+AXr0kDYeIyFp4qqHErHWtl9zIqWeBCJDXPimX04NNUVRUhNLSUv20ePHiZss7NBgmXAjRaF69s2fPIjY2FsuWLUN2djYOHDiAgoICREdHmy1+srIff/zvPboWLQLYe0lEdoSJlx2RW2NOTg1dsm9y2hfldpzei5ubm8Fk7DRDAPDw8ICjo2Oj3q2rV6826gWrl5iYiBEjRuD111/HgAEDMHbsWCQnJyM1NRXFxcVm3xaysNu3geefB8rLgdBQ4E9/kjoiIiKrYuIlA9bs9ZJbo05ODV6yT9wHrcPFxQWBgYHIyMgwmJ+RkYHg4GCjr7l9+zbatTP8mnJ0dASg6ykjhZkzB8jLAzw9ge3bASde7UBE9oWJF0mODV+Sitz2Pbn9MGJucXFxSElJQWpqKs6dO4f58+ejsLBQf+rg4sWLMW3aNH35CRMmYO/evdiwYQPy8/Px5ZdfIjY2FkOHDoVGo5FqM6g1UlOBLVuAdu10SZe3t9QRERFZHX9ukglrjnAop4E2iKTCpMv6IiIicP36daxYsQLFxcUICAhAWloa/Pz8AADFxcUG9/SaMWMGysvLsX79eixYsACdO3fGqFGj8Pbbb0u1CdQa332n6+0CgBUrgMcflzYeIiKJMPGyU3JLvjjSIVmT3JIuezJ79mzMnj3b6HNbtmxpNG/u3LmYO3euhaMiiykr092v6+5dIDwcuMfgK0REtoynGsqIvY5wWI+NYbIGOe5n9tDbRXZICN0IhufPAz166IaOb8dmBxHZL34C2jE5Nvbk2Cgm2yHH/UuOxyGRWaxbB+zeDTg76+7d1a2b1BEREUmKiZfMWLvXS46NPjk2jkn5uF8RWdHXXwMLF+r+X7MGGDZM2niIiGSAiRfJEhvJZE5y3Z/k+MMHUZtdv667rqu6WnffLl6jR0QEgImXLLHXS0eujWVSFrnuR3I97ojapK4OmDoVKCoC7r8f2LQJcHCQOioiIllg4kUA5NsIDH90r2wbziR/3HeIrCwxEfjsM0Ct1l3f5eYmdURERLLBxEum7H2Ew4bYgKaWkvM+I9cfOoja5PBhYNky3f/JycCAAdLGQ0QkM0y82mKtZRfPUw4NybkhTfIi531F7scZUasUFwMvvqg71fDll3UTEREZYOJFBuTeKJRzg5rkgfsIkZXV1ACTJwO//AL07w+sXy91REREssTEq63etuzipTjlkMkXKZESrgeU+7FF1CpLlwJHjwKdOgF79gDt20sdERGRLDHxIkWSewObrEsJ+wOTLrJJn3wCvP2fXyBTU3UjGRIRkVFMvMyBvV6SUEJjmyxPCfuBEo4nohYrKNANHQ8AsbG6e3YREVGTmHhRk5TQWFTC6WVkOXzviSRSWQm88AJw8yYwbBjwzjtSR0REJHtMvMzFBnu9AGUkXwAb4PZIKe+5Uo4hohaJiwOysoCuXYGPPgJcXKSOiIhI9ph4KQjv7dU8pTTEqW2U1MvJpIts0o4duvt0AcA//wn07CltPERECsHEy5ws3OslFSU1HpXUKKeWU9J7q6Tjhshk584BUVG6/+PjgfBwaeMhIlIQJl4Kw1MOTaOkBjrdGxNqIhmoqNANoFFRATz+OLB8udQREREpChMvc7PRXi+AyRdJQ4nvo9KOFaJ7EgKYNQs4exbw9gY+/BBwdJQ6KiIiRWHipUBSXuultAYle0qUTYnvndKOESKTpKQA//iHLtnasQPw8pI6IiIixWHiZQlW6PXiQBsto8QGvD1TasLMpItsUk4OMHeu7v+VK4FHH5U2HiIihWLiZSk85VB2lNqYtzdKfY+UelyQvCQnJ8Pf3x9qtRqBgYE4duyYtAGVlgKTJunu2/XUU8Drr0sbDxGRgjHxUjCectg6Sm3Y2zolJ8ZKPh5IPnbu3Il58+YhPj4eOTk5CAkJQXh4OAoLC6UJSAjg5ZeBCxcAPz9g61agHZsNREStxU9QS7LxUw6V3NhUciPf1vC9INJJSkpCZGQkoqKi0K9fP6xduxY9evTAhg0bpAlo7Vpg3z7dzZF37dLdLJmIiFrNSeoASNmi8VdsxEypw2i1+gb/Z0eflTgS+2QLCZeSf4Ag+aiqqkJ2djYWLVpkMD8sLAwnTpww+prKykpUVlbqH5eVlQEAqqurUV1d3eIY6l9TXV0Nh6++guMbb8ABQO0776Bu0CCgFcu0N7+tQ2od1mHbsQ7brqV1aGo5Jl6W9jaAP1p2FRO/S8f+gWGWXUkzlJ58AUzArM0WEi6ASReZz7Vr11BbWwtPT0+D+Z6enigpKTH6msTERCw3ci+t9PR0tG/fvtWxZO7ejcfi4uBUU4PLI0ciu2dPIC2t1cuzRxkZGVKHoHisw7ZjHbadqXV4+/Ztk8ox8bIRTL7MgwmYZdlKwgUw6SLLcHBwMHgshGg0r97ixYsRFxenf1xWVoYePXogLCwMbm5uLV53dXU1Mg4cQNjf/w7H69ch+vSB5//9H8Z16tTiZdmr6upqZGRkYMyYMXB2dpY6HEViHbYd67DtWlqH9Wcc3AsTL2uwQq8XmRcTMPOypYQLYNJF5ufh4QFHR8dGvVtXr15t1AtWT6VSQaVSNZrv7Ozc6sZWn9274XjoEODqCoc9e+DM67papS3vAemwDtuOddh2ptahqfXMwTVsiNT39rLFxigHfmgbW6w/W9zPSXouLi4IDAxsdFpLRkYGgoODrRKDw6FD6Ltjh+7Bxo1AQIBV1ktEZC+YeFmLle7rxeTLMmwxgbAkW60vW92/SR7i4uKQkpKC1NRUnDt3DvPnz0dhYSGio6Mtv/Kff4bjtGlwEAJ1kZHAtGmWXycRkZ3hqYbWZCenHNrK9V7G8BTEptliovVbTLrI0iIiInD9+nWsWLECxcXFCAgIQFpaGvz8/Cy/8rVr4fDvf+Omvz86vPsuf5UlIrIAJl42SOqBNgDbTr4AwyTD3pMwW0+4ACZdZD2zZ8/G7Nmzrb/it95CbceOOOXpicfUauuvn4jIDjDxsjYr9Xox+bIee0zC7CHZqseki+yCoyPqlizBbQ4bT0RkMUy8yKLsJfmqZ8tJmD0lW/WYdBEREZG5MPGSgh31egH2l3zVU3oSZo+J1m8x6SIiIiJzYuJl45h8yYOxJEZuyZi9J1q/xaSLiIiIzI2Jl1SsOMIhky95kioZY4LVPCZdREREZAlMvKRkJ8PL/xaTr+YxKZKWnJKucXlfSB0CERERmRFv1WEnpL6x8m/JqXFLVE9O+6WcjlciIiIyDyZeUnvbequSU2NOTo1cIu6PREREZGlMvOSAyReRZOS2H8rpGCUiIiLzYeJFkorGX2XX8CX7Ibd9j0kXERGR7WLiJRd22utVT24NYLJ9ctvn5HhcEhERkfkw8bJTcmzkya0hTLZLbvuaHI9HIiIiMi8mXnJixV4vQJ6NPbk1iMm28NRWIiIikgoTL7lh8sWGMVmEXPcrOR6DREREZH5MvEiW2DNB5iTXfYlJFxERkf1g4iVH7PXSk2uDmZRDrvuQnI87IiIiMj/FJF4rV65EcHAw2rdvj86dOxst4+Dg0GjauHGjQZm8vDyEhobC1dUVPj4+WLFiBYQQVtiCFmLypSfXhjPJm5x7TeV8vFlacnIy/P39oVarERgYiGPHjjVbPjMzE4GBgVCr1ejdu3ejz3QiIiKlUEziVVVVhUmTJmHWrFnNltu8eTOKi4v10/Tp0/XPlZWVYcyYMdBoNDh16hTWrVuHNWvWICkpqVUxndzdqpfJlpwbg3JtQJM8yXl/kfNxZmk7d+7EvHnzEB8fj5ycHISEhCA8PByFhYVGyxcUFGDcuHEICQlBTk4OlixZgtjYWOzZs8fKkRMREbWdk9QBmGr58uUAgC1btjRbrnPnzvDy8jL63LZt23D37l1s2bIFKpUKAQEB+PHHH5GUlIS4uDg4ODgYfV1lZSUqKyv1j0tLSwEAFQDKqlu+LSZ7E8A8Cy7fiMe+TEda/1HWXamJpuF9AMAmvCxxJCRnkdiM21IH0YyyWyaWq9D9NU+PfIUZlmF8mWVlZQZzVSoVVCqV0VckJSUhMjISUVFRAIC1a9fi4MGD2LBhAxITExuV37hxI3r27Im1a9cCAPr164esrCysWbMGzz33nBm3xTbU7ysN3xNTVVdX4/bt2ygrK4Ozs7M5Q7MbrMO2Yx22Heuw7Vpah/Wfu/f8zhYKs3nzZuHu7m70OQDCx8dHdOvWTQwZMkRs2LBB1NbW6p+fOnWqmDhxosFrvv32WwFA5OfnN7nOhIQEAYATJ06crD5duHCh1Z+Xd+7cEV5eXhaLrWPHjo3mJSQkGI2lsrJSODo6ir179xrMj42NFY8++qjR14SEhIjY2FiDeXv37hVOTk6iqqqq1fViq4qKiiTfXzlx4sTJnqeioqJmP6cV0+Nlij/96U944okn4OrqikOHDmHBggW4du0ali5dCgAoKSlBr169DF7j6empf87f39/ochcvXoy4uDj945s3b8LPzw+FhYVwd3e3zMZYSFlZGXr06IGioiK4ublJHU6LMHZpMHZplJaWomfPnujatWurl6FWq1FQUICqqiozRvZfQohGZwo01dt17do11NbW6j9z63l6eqKkpMToa0pKSoyWr6mpwbVr1+Dt7d2G6G2PRqNBUVEROnXq1OQZHM1R8vEiF6zDtmMdth3rsO1aWodCCJSXl0Oj0TRbTtLES6vV6k8hbMqpU6cwZMgQk5ZXn2ABwKBBgwAAK1asMJjf8MtI/KdLsLkvqaZOnXF3d1fsDu3m5sbYJcDYpaHk2Nu1a9uluGq1Gmq12kzRtJ2xz+DmPn9b85ltr9q1awdfX982L0fJx4tcsA7bjnXYdqzDtmtJHZrSGSNp4hUTE4PJkyc3W6ZhD1VLDB8+HGVlZfjll1/g6ekJLy+vRr+sXr16FQAa/apKRETm4+HhAUdHR6OfwU19/jb1me3k5IRu3bpZLFYiIiJLkDTx8vDwgIeHh8WWn5OTA7VarR9+PigoCEuWLEFVVRVcXFwAAOnp6dBoNG1K8IiIqHkuLi4IDAxERkYGfv/73+vnZ2Rk4Omnnzb6mqCgIHz88ccG89LT0zFkyBBeME5ERIqjmOHkCwsLkZubi8LCQtTW1iI3Nxe5ubm4dUs3RNjHH3+Mv/3tbzh9+jQuXLiAlJQUxMfH49VXX9WfJvjSSy9BpVJhxowZOH36NPbt24dVq1Y1O6KhMSqVCgkJCU1eyyBnjF0ajF0ajF1e4uLikJKSgtTUVJw7dw7z589HYWEhoqOjAeiup502bZq+fHR0NC5duoS4uDicO3cOqamp2LRpExYuXCjVJtg0W9znrI112Hasw7ZjHbadperQQQg53j24sRkzZmDr1q2N5h8+fBiPPfYYDhw4gMWLF+Onn35CXV0devfujaioKMyZMwdOTv/t2MvLy8OcOXPwzTffoEuXLoiOjsayZct4vQARkRUkJydj9erVKC4uRkBAAN599108+uijAHSf8xcvXsSRI0f05TMzMzF//nycOXMGGo0Gf/zjH/WJGhERkZIoJvEiIiIiIiJSKsWcakhERERERKRUTLyIiIiIiIgsjIkXERERERGRhTHxIiIiIiIisjAmXs1YuXIlgoOD0b59e/29wBoqLCzEhAkT0KFDB3h4eCA2NhZVVVUGZfLy8hAaGgpXV1f4+PhgxYoVkGJMk169esHBwcFgWrRokUEZU7ZHCsnJyfD394darUZgYCCOHTsmdUiNaLXaRvXr5eWlf14IAa1WC41GA1dXVzz22GM4c+aMJLEePXoUEyZMgEajgYODA/71r38ZPG9KrJWVlZg7dy48PDzQoUMHTJw4EZcvX5Y89hkzZjR6H4YPHy557ImJiXjkkUfQqVMn3HfffXjmmWfwww8/GJSRc72Tst3ruGlo7969GDNmDLp37w43NzcEBQXh4MGD1glWplpah7/15ZdfwsnJCYMGDbJYfErQmjqsrKxEfHw8/Pz8oFKp8Lvf/Q6pqamWD1amWlOH27Ztw8CBA9G+fXt4e3vj5ZdfxvXr1y0frEyZ8n1sTGZmJgIDA6FWq9G7d29s3Lixxetm4tWMqqoqTJo0CbNmzTL6fG1tLcaPH4+KigocP34cO3bswJ49e7BgwQJ9mbKyMowZMwYajQanTp3CunXrsGbNGiQlJVlrMwysWLECxcXF+mnp0qX650zZHins3LkT8+bNQ3x8PHJychASEoLw8HAUFhZKGpcxDz30kEH95uXl6Z9bvXo1kpKSsH79epw6dQpeXl4YM2YMysvLrR5nRUUFBg4ciPXr1xt93pRY582bh3379mHHjh04fvw4bt26haeeegq1tbWSxg4ATz75pMH7kJaWZvC8FLFnZmZizpw5OHnyJDIyMlBTU4OwsDBUVFToy8i53knZTDlufuvo0aMYM2YM0tLSkJ2djccffxwTJkxATk6OhSOVr5bWYb3S0lJMmzYNTzzxhIUiU47W1OELL7yAQ4cOYdOmTfjhhx+wfft29O3b14JRyltL6/D48eOYNm0aIiMjcebMGezatQunTp1CVFSUhSOVL1O+jxsqKCjAuHHjEBISgpycHCxZsgSxsbHYs2dPy1Yu6J42b94s3N3dG81PS0sT7dq1Ez///LN+3vbt24VKpRKlpaVCCCGSk5OFu7u7uHv3rr5MYmKi0Gg0oq6uzuKx/5afn5949913m3zelO2RwtChQ0V0dLTBvL59+4pFixZJFJFxCQkJYuDAgUafq6urE15eXuKtt97Sz7t7965wd3cXGzdutFKExgEQ+/bt0z82JdabN28KZ2dnsWPHDn2Zn3/+WbRr104cOHBAstiFEGL69Oni6aefbvI1con96tWrAoDIzMwUQiir3knZjB03pnjwwQfF8uXLzR+QArWkDiMiIsTSpUub/Y6wR6bU4WeffSbc3d3F9evXrROUwphSh++8847o3bu3wbz33ntP+Pr6WjAyZWn4fWzMG2+8Ifr27Wswb+bMmWL48OEtWhd7vNrgq6++QkBAADQajX7e2LFjUVlZiezsbH2Z0NBQgztfjx07FleuXMHFixetHTLefvttdOvWDYMGDcLKlSsNTiM0ZXusraqqCtnZ2QgLCzOYHxYWhhMnTkgSU3POnz8PjUYDf39/TJ48Gfn5+QB0v5SUlJQYbIdKpUJoaKjstsOUWLOzs1FdXW1QRqPRICAgQBbbc+TIEdx3333o06cPXnnlFVy9elX/nFxiLy0tBQB07doVgG3UO9muuro6lJeX6/dXMs3mzZtx4cIFJCQkSB2KIu3fvx9DhgzB6tWr4ePjgz59+mDhwoW4c+eO1KEpRnBwMC5fvoy0tDQIIfDLL79g9+7dGD9+vNShyUbD72Njvvrqq0Zt0bFjxyIrKwvV1dUmr8updSESAJSUlMDT09NgXpcuXeDi4oKSkhJ9mV69ehmUqX9NSUkJ/P39rRIrALz22mt4+OGH0aVLF3zzzTdYvHgxCgoKkJKSoo/nXttjbdeuXUNtbW2juDw9PSWLqSnDhg3D3//+d/Tp0we//PIL3nzzTQQHB+PMmTP6WI1tx6VLl6QIt0mmxFpSUgIXFxd06dKlURmp35fw8HBMmjQJfn5+KCgowP/+7/9i1KhRyM7OhkqlkkXsQgjExcVh5MiRCAgIAKD8eifb9uc//xkVFRV44YUXpA5FMc6fP49Fixbh2LFjcHJic6s18vPzcfz4cajVauzbtw/Xrl3D7Nmz8euvv9r1dV4tERwcjG3btiEiIgJ3795FTU0NJk6ciHXr1kkdmiwY+z42xlgb2dPTEzU1Nbh27Rq8vb1NWp/d9XgZGwCh4ZSVlWXy8hwcHBrNE0IYzG9YRvxnYA1jr22plmzP/PnzERoaigEDBiAqKgobN27Epk2bDC6wNGV7pGCsDqWOqaHw8HA899xz6N+/P0aPHo1PP/0UALB161Z9GSVsR73WxCqH7YmIiMD48eMREBCACRMm4LPPPsOPP/6ofz+aYs3YY2Ji8P3332P79u2NnlNqvZPt2r59O7RaLXbu3In77rtP6nAUoba2Fi+99BKWL1+OPn36SB2OYtXV1cHBwQHbtm3D0KFDMW7cOCQlJWHLli3s9TLR2bNnERsbi2XLliE7OxsHDhxAQUEBoqOjpQ5NFpr7Pm7IHO15u/sJJiYmBpMnT262TMMeqqZ4eXnh66+/Nph348YNVFdX67NiLy+vRr9E15/21DBzbo22bE/9SG8//fQTunXrZtL2WJuHhwccHR2N1qFUMZmqQ4cO6N+/P86fP49nnnkGgO4Xk9/+KiLH7agfibG5WL28vFBVVYUbN24Y9L5cvXoVwcHB1g34Hry9veHn54fz588DkD72uXPnYv/+/Th69Ch8fX31822t3sk27Ny5E5GRkdi1axdGjx4tdTiKUV5ejqysLOTk5CAmJgaALokQQsDJyQnp6ekYNWqUxFHKn7e3N3x8fODu7q6f169fPwghcPnyZdx///0SRqcMiYmJGDFiBF5//XUAwIABA9ChQweEhITgzTffNLmnxhY19X1sTFPteScnJ3Tr1s3kddpdj5eHhwf69u3b7KRWq01aVlBQEE6fPo3i4mL9vPT0dKhUKgQGBurLHD161OBaqvT0dGg0GpMTPEttT/3oVPUHnSnbY20uLi4IDAxERkaGwfyMjAzZNzQrKytx7tw5eHt7w9/fH15eXgbbUVVVhczMTNlthymxBgYGwtnZ2aBMcXExTp8+LbvtuX79OoqKivT7uVSxCyEQExODvXv34osvvmh0mrGt1Tsp3/bt2zFjxgx8+OGHvB6khdzc3JCXl4fc3Fz9FB0djQceeAC5ubkYNmyY1CEqwogRI3DlyhXcunVLP+/HH39Eu3bt7tlQJp3bt2+jXTvD5r6joyMASHJrIzm41/exMUFBQY3aounp6RgyZAicnZ1btHJqwqVLl0ROTo5Yvny56Nixo8jJyRE5OTmivLxcCCFETU2NCAgIEE888YT49ttvxeeffy58fX1FTEyMfhk3b94Unp6e4sUXXxR5eXli7969ws3NTaxZs8aq23LixAmRlJQkcnJyRH5+vti5c6fQaDRi4sSJ+jKmbI8UduzYIZydncWmTZvE2bNnxbx580SHDh3ExYsXJY2roQULFogjR46I/Px8cfLkSfHUU0+JTp066eN86623hLu7u9i7d6/Iy8sTL774ovD29hZlZWVWj7W8vFy/PwPQ7xuXLl0yOdbo6Gjh6+srPv/8c/Htt9+KUaNGiYEDB4qamhrJYi8vLxcLFiwQJ06cEAUFBeLw4cMiKChI+Pj4SB77rFmzhLu7uzhy5IgoLi7WT7dv39aXkXO9k7Ld65hftGiRmDp1qr78hx9+KJycnMT7779vsL/evHlTqk2QXEvrsCGOatjyOiwvLxe+vr7i+eefF2fOnBGZmZni/vvvF1FRUVJtguRaWoebN28WTk5OIjk5WVy4cEEcP35cDBkyRAwdOlSqTZCcKd/HDesxPz9ftG/fXsyfP1+cPXtWbNq0STg7O4vdu3e3aN1MvJoxffp0AaDRdPjwYX2ZS5cuifHjxwtXV1fRtWtXERMTYzB0vBBCfP/99yIkJESoVCrh5eUltFqt1YeSz87OFsOGDRPu7u5CrVaLBx54QCQkJIiKigqDcqZsjxTef/994efnJ1xcXMTDDz/c7JCfUomIiBDe3t7C2dlZaDQa8eyzz4ozZ87on6+rqxMJCQnCy8tLqFQq8eijj4q8vDxJYj18+LDRfXv69Okmx3rnzh0RExMjunbtKlxdXcVTTz0lCgsLJY399u3bIiwsTHTv3l04OzuLnj17iunTpzeKS4rYjcUMQGzevFlfRs71Tsp2r2N++vTpIjQ0VF8+NDS02fL2qKV12BATr9bV4blz58To0aOFq6ur8PX1FXFxcQYNZHvTmjp87733xIMPPihcXV2Ft7e3mDJlirh8+bL1g5cJU76PjdXjkSNHxODBg4WLi4vo1auX2LBhQ4vX7fCfAIiIiIiIiMhC7O4aLyIiIiIiImtj4kVERERERGRhTLyIiIiIiIgsjIkXERERERGRhTHxIiIiIiIisjAmXkRERERERBbGxIuIiIiIiMjCmHgRERERERFZGBMvIiIiIiIiC2PiRWQmw4cPx7vvvqt/HBERAQcHB1RUVAAArly5AhcXF5w7d06qEImIiIhIIky8iMykc+fOKC8vBwAUFRXh4MGD6NSpE27cuAEA+OCDDzBq1Cj069dPyjCJiIhk4d///je8vLywatUq/byvv/4aLi4uSE9PlzAyIstg4kVkJl26dMGtW7cAAOvXr8eUKVPQvXt33LhxA9XV1fjggw/w2muvAQA++eQTPPDAA7j//vuRkpIiZdhERESS6N69O1JTU6HVapGVlYVbt27hD3/4A2bPno2wsDCpwyMyOyepAyCyFfU9XhUVFUhJScFXX32FEydO4MaNG9i3bx86deqEJ598EjU1NYiLi8Phw4fh5uaGhx9+GM8++yy6du0q9SYQERFZ1bhx4/DKK69gypQpeOSRR6BWq/HWW29JHRaRRbDHi8hM6nu8tm7diqCgIPTp0wdubm64ceMG3n//fcTGxsLBwQHffPMNHnroIfj4+KBTp04YN24cDh48KHX4REREklizZg1qamrw0UcfYdu2bVCr1VKHRGQRTLyIzKRz584oKyvDX/7yF8ybNw8A4ObmhuPHj+O7777D9OnTAegG2fDx8dG/ztfXFz///LMUIRMREUkuPz8fV65cQV1dHS5duiR1OEQWw1MNicykS5cu+OKLL9CrVy+MHj0agC7x2rBhA2bOnImOHTsCAIQQjV7r4OBg1ViJiIjkoKqqClOmTEFERAT69u2LyMhI5OXlwdPTU+rQiMyOPV5EZlJ/qmH9ABqALvG6c+cOYmJi9PN8fHwMerguX74Mb29vq8ZKREQkB/Hx8SgtLcV7772HN954A/369UNkZKTUYRFZhIMw9vM7EVlMTU0N+vXrhyNHjugH1zh58iS6desmdWhERERWc+TIEYwZMwaHDx/GyJEjAQCFhYUYMGAAEhMTMWvWLIkjJDIvJl5EEti/fz8WLlyIuro6vPHGG3j11VelDomIiIiILIiJFxERERERkYXxGi8iIiIiIiILY+JFRERERERkYUy8iIiIiIiILIyJFxERERERkYUx8SIiIiIiIrIwJl5EREREREQWxsSLiIiIiIjIwph4ERERERERWRgTLyIiIiIiIgtj4kVERERERGRhTLyIiIiIiIgs7P8BMFBfMaykCKQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=50)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\n",
    "    \"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "        l=loss_star, w0=w0_star, w1=w1_star, t=execution_time\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26.706078    6.52028757] 27.490521129292457\n",
      "[-23.293922    -3.47971243] 23.552392678247628\n"
     ]
    }
   ],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute gradient vector\n",
    "    e = y - tx @ w\n",
    "    grad = - 1/len(y) * tx.T @ e\n",
    "    return grad\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "\n",
    "grad1 = compute_gradient(y, tx, np.array([100, 20]))\n",
    "grad2 = compute_gradient(y, tx, np.array([50, 10]))\n",
    "print(grad1, np.linalg.norm(grad1))\n",
    "print(grad2, np.linalg.norm(grad2))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        \n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        # TODO: compute gradient and loss\n",
    "        # ***************************************************\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        \n",
    "        w = w - gamma * grad\n",
    "        # ***************************************************\n",
    "\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=55844734.255183354, w0=51.30574540147362, w1=9.435798704492344\n",
      "GD iter. 1/49: loss=5306049.2421792, w0=66.69746902191567, w1=12.26653831584001\n",
      "GD iter. 2/49: loss=757567.5910088306, w0=71.31498610804833, w1=13.115760199244328\n",
      "GD iter. 3/49: loss=348204.24240349006, w0=72.70024123388814, w1=13.37052676426563\n",
      "GD iter. 4/49: loss=311361.54102900915, w0=73.11581777164007, w1=13.446956733772021\n",
      "GD iter. 5/49: loss=308045.6979053059, w0=73.24049073296565, w1=13.469885724623941\n",
      "GD iter. 6/49: loss=307747.2720241726, w0=73.27789262136334, w1=13.476764421879516\n",
      "GD iter. 7/49: loss=307720.41369487054, w0=73.28911318788263, w1=13.478828031056189\n",
      "GD iter. 8/49: loss=307717.9964452334, w0=73.29247935783843, w1=13.47944711380919\n",
      "GD iter. 9/49: loss=307717.778892766, w0=73.29348920882516, w1=13.47963283863509\n",
      "GD iter. 10/49: loss=307717.759313044, w0=73.29379216412119, w1=13.479688556082861\n",
      "GD iter. 11/49: loss=307717.757550869, w0=73.29388305071, w1=13.479705271317192\n",
      "GD iter. 12/49: loss=307717.7573922733, w0=73.29391031668663, w1=13.479710285887492\n",
      "GD iter. 13/49: loss=307717.75737799966, w0=73.29391849647962, w1=13.479711790258582\n",
      "GD iter. 14/49: loss=307717.7573767151, w0=73.29392095041752, w1=13.479712241569908\n",
      "GD iter. 15/49: loss=307717.7573765995, w0=73.29392168659889, w1=13.479712376963306\n",
      "GD iter. 16/49: loss=307717.75737658906, w0=73.2939219074533, w1=13.479712417581325\n",
      "GD iter. 17/49: loss=307717.7573765882, w0=73.29392197370963, w1=13.479712429766732\n",
      "GD iter. 18/49: loss=307717.75737658807, w0=73.29392199358652, w1=13.479712433422353\n",
      "GD iter. 19/49: loss=307717.7573765881, w0=73.2939219995496, w1=13.47971243451904\n",
      "GD iter. 20/49: loss=307717.75737658807, w0=73.29392200133852, w1=13.479712434848047\n",
      "GD iter. 21/49: loss=307717.75737658807, w0=73.29392200187519, w1=13.479712434946748\n",
      "GD iter. 22/49: loss=307717.75737658795, w0=73.29392200203618, w1=13.479712434976358\n",
      "GD iter. 23/49: loss=307717.75737658795, w0=73.29392200208449, w1=13.479712434985242\n",
      "GD iter. 24/49: loss=307717.75737658795, w0=73.29392200209898, w1=13.479712434987906\n",
      "GD iter. 25/49: loss=307717.75737658807, w0=73.29392200210333, w1=13.479712434988706\n",
      "GD iter. 26/49: loss=307717.757376588, w0=73.29392200210464, w1=13.479712434988945\n",
      "GD iter. 27/49: loss=307717.757376588, w0=73.29392200210502, w1=13.479712434989018\n",
      "GD iter. 28/49: loss=307717.75737658795, w0=73.29392200210513, w1=13.47971243498904\n",
      "GD iter. 29/49: loss=307717.75737658795, w0=73.29392200210518, w1=13.479712434989047\n",
      "GD iter. 30/49: loss=307717.75737658783, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 31/49: loss=307717.7573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 32/49: loss=307717.7573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 33/49: loss=307717.7573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 34/49: loss=307717.7573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 35/49: loss=307717.7573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 36/49: loss=307717.7573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 37/49: loss=307717.7573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 38/49: loss=307717.7573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 39/49: loss=307717.7573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 40/49: loss=307717.7573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 41/49: loss=307717.7573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 42/49: loss=307717.7573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 43/49: loss=307717.7573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 44/49: loss=307717.7573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 45/49: loss=307717.7573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 46/49: loss=307717.7573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 47/49: loss=307717.7573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 48/49: loss=307717.7573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 49/49: loss=307717.7573765879, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD: execution time=0.007 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters =50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53dade7b70f34f979534840d2f9c7a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient computation. It's the same as the usual gradient.\n",
    "    e = y - tx @ w\n",
    "    grad = - 1/len(y) * tx.T @ e\n",
    "    return grad\n",
    "    \n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic gradient descent.\n",
    "        \n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "            \n",
    "            grad = compute_gradient(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * grad\n",
    "            \n",
    "            loss = compute_loss(y, tx, w)\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "        \n",
    "        \n",
    "        # ***************************************************\n",
    "\n",
    "        print(\n",
    "            \"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=12072874.702998899, w0=47.391341904542486, w1=-9.005216714367439\n",
      "SGD iter. 1/49: loss=1707177.7610604023, w0=65.35619213819925, w1=4.708256171857062\n",
      "SGD iter. 2/49: loss=330085.25924965093, w0=71.8007218981712, w1=13.395429358883362\n",
      "SGD iter. 3/49: loss=335016.93586420675, w0=71.64174320771593, w1=13.464776567677742\n",
      "SGD iter. 4/49: loss=315055.92723899137, w0=73.591404661915, w1=12.676393660558372\n",
      "SGD iter. 5/49: loss=309578.6735356567, w0=73.49512408789874, w1=13.098124520375402\n",
      "SGD iter. 6/49: loss=322409.89681407437, w0=72.6281156984145, w1=14.492587506117962\n",
      "SGD iter. 7/49: loss=309394.398862555, w0=72.9301652207214, w1=13.667715500373644\n",
      "SGD iter. 8/49: loss=319619.7432317749, w0=74.02373689848606, w1=12.668806273624334\n",
      "SGD iter. 9/49: loss=330301.2975000565, w0=73.52385955683576, w1=11.994625658645198\n",
      "SGD iter. 10/49: loss=335738.29572752584, w0=72.80789047628758, w1=11.877892472881822\n",
      "SGD iter. 11/49: loss=310918.54342612403, w0=72.7357962849487, w1=13.387114979288778\n",
      "SGD iter. 12/49: loss=309055.631167867, w0=73.64394238674386, w1=13.585887335343598\n",
      "SGD iter. 13/49: loss=320362.0720416931, w0=72.17036056631066, w1=13.434533164235047\n",
      "SGD iter. 14/49: loss=311262.8812529389, w0=73.78595384251014, w1=13.81499896269753\n",
      "SGD iter. 15/49: loss=310832.92527128954, w0=72.73661466382461, w1=13.4492933592508\n",
      "SGD iter. 16/49: loss=308897.62577392283, w0=72.96547919190337, w1=13.580271672059649\n",
      "SGD iter. 17/49: loss=319398.7193678971, w0=74.04825329220282, w1=12.705709525491564\n",
      "SGD iter. 18/49: loss=314508.662130047, w0=73.88916819604617, w1=14.049600532728135\n",
      "SGD iter. 19/49: loss=314233.75991613965, w0=73.72886309050514, w1=14.159731922633792\n",
      "SGD iter. 20/49: loss=335818.8967628424, w0=74.24638744529027, w1=14.859177374073653\n",
      "SGD iter. 21/49: loss=310357.36726176925, w0=73.18266584939133, w1=13.981292993824244\n",
      "SGD iter. 22/49: loss=322533.3374131791, w0=74.43634136486574, w1=13.059669576165222\n",
      "SGD iter. 23/49: loss=315988.2884727457, w0=72.5574867908705, w1=14.013300561983538\n",
      "SGD iter. 24/49: loss=308657.32547079364, w0=73.52911619465227, w1=13.676284307882096\n",
      "SGD iter. 25/49: loss=307785.40870580974, w0=73.34571551866627, w1=13.41581748554537\n",
      "SGD iter. 26/49: loss=309796.6197888586, w0=73.09574781200307, w1=13.890337850184482\n",
      "SGD iter. 27/49: loss=313356.68254495296, w0=72.74711415503927, w1=12.965034190623081\n",
      "SGD iter. 28/49: loss=319144.1148638071, w0=73.09744373185245, w1=12.428983146332644\n",
      "SGD iter. 29/49: loss=322338.4245700189, w0=72.09642507516209, w1=13.647246996512711\n",
      "SGD iter. 30/49: loss=320026.8407981184, w0=74.36868657778894, w1=13.204413599160825\n",
      "SGD iter. 31/49: loss=320658.83430235554, w0=74.39394939258659, w1=13.76962178766231\n",
      "SGD iter. 32/49: loss=309544.26331328525, w0=73.07378613893557, w1=13.113391816655465\n",
      "SGD iter. 33/49: loss=321418.50864421116, w0=72.18237683455014, w1=13.846512744759012\n",
      "SGD iter. 34/49: loss=326670.7769581861, w0=72.9142295815431, w1=12.156407623568976\n",
      "SGD iter. 35/49: loss=317724.74718008685, w0=73.41240585134594, w1=12.486404548269629\n",
      "SGD iter. 36/49: loss=310471.875068233, w0=72.90090296357926, w1=13.131936926189677\n",
      "SGD iter. 37/49: loss=308094.49511623464, w0=73.2576588652974, w1=13.28903270645578\n",
      "SGD iter. 38/49: loss=312652.1000272884, w0=73.29968402917982, w1=14.182137565600158\n",
      "SGD iter. 39/49: loss=333813.3995378831, w0=74.86247393579116, w1=13.865988304620496\n",
      "SGD iter. 40/49: loss=329708.528177669, w0=74.65849460243219, w1=14.060245550792889\n",
      "SGD iter. 41/49: loss=329918.6983752463, w0=74.62818746746153, w1=12.816515806662839\n",
      "SGD iter. 42/49: loss=310655.70350685215, w0=73.82521509862744, w1=13.372370650886932\n",
      "SGD iter. 43/49: loss=336938.22639599466, w0=74.98723778549765, w1=13.713653794424745\n",
      "SGD iter. 44/49: loss=318830.76956014556, w0=74.24788823453893, w1=13.928321001260263\n",
      "SGD iter. 45/49: loss=316421.9494262779, w0=73.78461944575153, w1=12.686216862368802\n",
      "SGD iter. 46/49: loss=315415.8821311558, w0=72.92582747929056, w1=14.276153831282992\n",
      "SGD iter. 47/49: loss=308763.82395901287, w0=73.18352577656835, w1=13.175706955384303\n",
      "SGD iter. 48/49: loss=316283.67201076215, w0=72.53379097891298, w1=12.951704583723407\n",
      "SGD iter. 49/49: loss=310065.92579444236, w0=73.72664197849569, w1=13.697818518852026\n",
      "SGD: execution time=0.027 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604424ed721a4b7c9a678a24763e8a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses,\n",
    "        sgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: reload the data by subsampling first, then by subsampling and adding outliers\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=False)\n",
    "\n",
    "# ***************************************************\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200,), (200, 2))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=96349.81315132366, w0=54.20172049419092, w1=23.2458728593218\n",
      "SGD iter. 1/49: loss=14046.18762086947, w0=67.18238501824072, w1=15.634592478151173\n",
      "SGD iter. 2/49: loss=5965.990158788591, w0=72.11260764058092, w1=14.959130163668588\n",
      "SGD iter. 3/49: loss=5527.501613722667, w0=73.0891898184845, w1=14.711712694834322\n",
      "SGD iter. 4/49: loss=5587.9283965738705, w0=73.71148320044898, w1=15.279755881273827\n",
      "SGD iter. 5/49: loss=5560.980873656795, w0=73.30387585648893, w1=15.116165973005543\n",
      "SGD iter. 6/49: loss=5666.832069669113, w0=72.60966081489771, w1=14.43625682006725\n",
      "SGD iter. 7/49: loss=5500.051771267669, w0=73.67372258745569, w1=14.015627578361373\n",
      "SGD iter. 8/49: loss=5499.060035448594, w0=74.08834331580083, w1=14.45497016058431\n",
      "SGD iter. 9/49: loss=5707.259217020493, w0=72.64749707627755, w1=13.946891198284732\n",
      "SGD iter. 10/49: loss=5505.325155086365, w0=73.44734816735306, w1=14.929091200089433\n",
      "SGD iter. 11/49: loss=5518.26453677789, w0=74.17283715357497, w1=14.365057361265567\n",
      "SGD iter. 12/49: loss=5460.905378416704, w0=73.52175846954606, w1=14.549487848805917\n",
      "SGD iter. 13/49: loss=5518.714515269179, w0=73.15750114479128, w1=14.760827923227215\n",
      "SGD iter. 14/49: loss=5666.012130095331, w0=73.37658279028709, w1=13.486860186725432\n",
      "SGD iter. 15/49: loss=5709.127271818363, w0=73.88704065654841, w1=15.568343838737524\n",
      "SGD iter. 16/49: loss=5553.57860669929, w0=74.28722971090153, w1=14.247550521689185\n",
      "SGD iter. 17/49: loss=5622.06866777231, w0=74.4121879094791, w1=14.939607460026185\n",
      "SGD iter. 18/49: loss=5507.772396976235, w0=74.12835406205106, w1=14.39892333128764\n",
      "SGD iter. 19/49: loss=5533.81939004101, w0=73.14935989784867, w1=14.861739570131672\n",
      "SGD iter. 20/49: loss=5534.075904875672, w0=73.60341839647153, w1=15.094314078087425\n",
      "SGD iter. 21/49: loss=5613.785883011924, w0=73.47737220274814, w1=13.605035190275386\n",
      "SGD iter. 22/49: loss=5938.593217456657, w0=74.10274719695887, w1=12.997614897488434\n",
      "SGD iter. 23/49: loss=5492.401847008382, w0=73.74603190383652, w1=14.072966250435782\n",
      "SGD iter. 24/49: loss=5526.733809772215, w0=73.71330208178493, w1=13.892409388570451\n",
      "SGD iter. 25/49: loss=5567.568208994017, w0=74.34717144131453, w1=14.67543186023189\n",
      "SGD iter. 26/49: loss=5663.54571029414, w0=74.4213344615525, w1=15.114641560081882\n",
      "SGD iter. 27/49: loss=5936.892371686277, w0=72.51051368849795, w1=15.543061187397749\n",
      "SGD iter. 28/49: loss=5721.291559733412, w0=72.5625283788658, w1=14.894316602354321\n",
      "SGD iter. 29/49: loss=5613.051503429073, w0=72.75558774651557, w1=14.574759445812989\n",
      "SGD iter. 30/49: loss=5702.34378289967, w0=73.07051874795529, w1=15.4292677001457\n",
      "SGD iter. 31/49: loss=5595.468213747873, w0=74.3058026853047, w1=13.989054610914868\n",
      "SGD iter. 32/49: loss=5793.433371395298, w0=74.28810116201261, w1=15.593822020208242\n",
      "SGD iter. 33/49: loss=5607.747561304292, w0=73.74066398011006, w1=15.336006836092004\n",
      "SGD iter. 34/49: loss=5851.927361538883, w0=74.67746574977951, w1=15.413968256883594\n",
      "SGD iter. 35/49: loss=5569.779740320307, w0=74.30454094631018, w1=14.807494207954717\n",
      "SGD iter. 36/49: loss=5745.101089851035, w0=72.47495620064475, w1=14.16065741546619\n",
      "SGD iter. 37/49: loss=5872.30420482199, w0=72.34374943156354, w1=13.831997018052173\n",
      "SGD iter. 38/49: loss=5611.883821619905, w0=72.86008458237575, w1=14.055853275130602\n",
      "SGD iter. 39/49: loss=5719.659840829688, w0=73.00250665694605, w1=15.432169520188033\n",
      "SGD iter. 40/49: loss=5507.299128605194, w0=73.9247215945133, w1=14.880796870366627\n",
      "SGD iter. 41/49: loss=5767.129353397505, w0=74.18571583103822, w1=15.590369659543848\n",
      "SGD iter. 42/49: loss=5638.505305657632, w0=73.46891391430148, w1=15.413239350938147\n",
      "SGD iter. 43/49: loss=5564.078853508107, w0=74.04607427601852, w1=15.077612428584827\n",
      "SGD iter. 44/49: loss=5597.543176782185, w0=73.9457438533004, w1=15.251966395694192\n",
      "SGD iter. 45/49: loss=5524.513160202429, w0=73.44800537825776, w1=13.926394562814522\n",
      "SGD iter. 46/49: loss=5472.932229996815, w0=73.9092242000847, w1=14.442700930402072\n",
      "SGD iter. 47/49: loss=5680.359372187274, w0=74.34937070751238, w1=13.700658206280966\n",
      "SGD iter. 48/49: loss=5458.130074632558, w0=73.62191844256179, w1=14.415095104568763\n",
      "SGD iter. 49/49: loss=5551.4027210092245, w0=73.77882442238122, w1=15.145530603208021\n",
      "GD: execution time=0.004 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: fit the model to the subsampled data / subsampled data with outliers and visualize the cloud of points\n",
    "#       and the model fit\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "\n",
    "# ***************************************************\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65fca6f2ce7d42189f1948e659ff7489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute subgradient gradient vector for MAE\n",
    "\n",
    "    e = y - tx @ w\n",
    " \n",
    "    return - 1/ y.shape[0] * tx.T @ np.sign(e)\n",
    "\n",
    "\n",
    "    # ***************************************************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute subgradient and loss\n",
    "        grad = compute_subgradient_mae(y, tx, w)\n",
    "        loss = compute_loss(y, tx, w)\n",
    "\n",
    "        # ***************************************************\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by subgradient\n",
    "\n",
    "        w = w - gamma * grad\n",
    "\n",
    "        # ***************************************************\n",
    "\n",
    "\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=1131708.8897753665, w0=0.7000000000000004, w1=1.6186704754339585e-15\n",
      "SubGD iter. 1/499: loss=1111189.8534866415, w0=1.4000000000000008, w1=3.237340950867917e-15\n",
      "SubGD iter. 2/499: loss=1090866.8171979168, w0=2.1000000000000014, w1=4.856011426301876e-15\n",
      "SubGD iter. 3/499: loss=1070739.7809091923, w0=2.8000000000000016, w1=6.474681901735834e-15\n",
      "SubGD iter. 4/499: loss=1050808.7446204673, w0=3.5000000000000018, w1=8.093352377169793e-15\n",
      "SubGD iter. 5/499: loss=1031073.7083317426, w0=4.200000000000002, w1=9.712022852603753e-15\n",
      "SubGD iter. 6/499: loss=1011534.672043018, w0=4.900000000000002, w1=1.1330693328037712e-14\n",
      "SubGD iter. 7/499: loss=992191.6357542932, w0=5.600000000000002, w1=1.2949363803471671e-14\n",
      "SubGD iter. 8/499: loss=973044.5994655687, w0=6.3000000000000025, w1=1.456803427890563e-14\n",
      "SubGD iter. 9/499: loss=954093.5631768438, w0=7.000000000000003, w1=1.618670475433959e-14\n",
      "SubGD iter. 10/499: loss=935338.526888119, w0=7.700000000000003, w1=1.780537522977355e-14\n",
      "SubGD iter. 11/499: loss=916779.4905993944, w0=8.400000000000004, w1=1.942404570520751e-14\n",
      "SubGD iter. 12/499: loss=898416.4543106697, w0=9.100000000000005, w1=2.1042716180641468e-14\n",
      "SubGD iter. 13/499: loss=880249.4180219448, w0=9.800000000000006, w1=2.2661386656075427e-14\n",
      "SubGD iter. 14/499: loss=862278.3817332201, w0=10.500000000000007, w1=2.4280057131509387e-14\n",
      "SubGD iter. 15/499: loss=844503.3454444951, w0=11.200000000000008, w1=2.5898727606943346e-14\n",
      "SubGD iter. 16/499: loss=826924.3091557703, w0=11.90000000000001, w1=2.7517398082377305e-14\n",
      "SubGD iter. 17/499: loss=809541.2728670456, w0=12.60000000000001, w1=2.913606855781126e-14\n",
      "SubGD iter. 18/499: loss=792354.2365783209, w0=13.300000000000011, w1=3.075473903324522e-14\n",
      "SubGD iter. 19/499: loss=775363.2002895962, w0=14.000000000000012, w1=3.2373409508679174e-14\n",
      "SubGD iter. 20/499: loss=758568.1640008716, w0=14.700000000000014, w1=3.399207998411313e-14\n",
      "SubGD iter. 21/499: loss=741969.1277121466, w0=15.400000000000015, w1=3.5610750459547086e-14\n",
      "SubGD iter. 22/499: loss=725566.0914234219, w0=16.100000000000016, w1=3.722942093498104e-14\n",
      "SubGD iter. 23/499: loss=709359.0551346974, w0=16.800000000000015, w1=3.8848091410415e-14\n",
      "SubGD iter. 24/499: loss=693348.0188459724, w0=17.500000000000014, w1=4.0466761885848955e-14\n",
      "SubGD iter. 25/499: loss=677532.9825572479, w0=18.200000000000014, w1=4.208543236128291e-14\n",
      "SubGD iter. 26/499: loss=661913.9462685231, w0=18.900000000000013, w1=4.370410283671687e-14\n",
      "SubGD iter. 27/499: loss=646490.9099797986, w0=19.600000000000012, w1=4.5322773312150823e-14\n",
      "SubGD iter. 28/499: loss=631263.8736910739, w0=20.30000000000001, w1=4.694144378758478e-14\n",
      "SubGD iter. 29/499: loss=616232.8374023489, w0=21.00000000000001, w1=4.8560114263018736e-14\n",
      "SubGD iter. 30/499: loss=601397.8011136242, w0=21.70000000000001, w1=5.017878473845269e-14\n",
      "SubGD iter. 31/499: loss=586758.7648248995, w0=22.40000000000001, w1=5.179745521388665e-14\n",
      "SubGD iter. 32/499: loss=572315.7285361749, w0=23.10000000000001, w1=5.3416125689320604e-14\n",
      "SubGD iter. 33/499: loss=558068.69224745, w0=23.800000000000008, w1=5.503479616475456e-14\n",
      "SubGD iter. 34/499: loss=544017.6559587254, w0=24.500000000000007, w1=5.6653466640188517e-14\n",
      "SubGD iter. 35/499: loss=530162.6196700009, w0=25.200000000000006, w1=5.827213711562247e-14\n",
      "SubGD iter. 36/499: loss=516503.583381276, w0=25.900000000000006, w1=5.989080759105643e-14\n",
      "SubGD iter. 37/499: loss=503040.5470925513, w0=26.600000000000005, w1=6.150947806649039e-14\n",
      "SubGD iter. 38/499: loss=489773.51080382656, w0=27.300000000000004, w1=6.312814854192434e-14\n",
      "SubGD iter. 39/499: loss=476702.4745151018, w0=28.000000000000004, w1=6.47468190173583e-14\n",
      "SubGD iter. 40/499: loss=463827.43822637707, w0=28.700000000000003, w1=6.636548949279225e-14\n",
      "SubGD iter. 41/499: loss=451148.4019376523, w0=29.400000000000002, w1=6.798415996822621e-14\n",
      "SubGD iter. 42/499: loss=438665.3656489277, w0=30.1, w1=6.960283044366017e-14\n",
      "SubGD iter. 43/499: loss=426378.32936020294, w0=30.8, w1=7.122150091909412e-14\n",
      "SubGD iter. 44/499: loss=414287.2930714782, w0=31.5, w1=7.284017139452808e-14\n",
      "SubGD iter. 45/499: loss=402392.25678275357, w0=32.2, w1=7.445884186996203e-14\n",
      "SubGD iter. 46/499: loss=390693.22049402876, w0=32.900000000000006, w1=7.607751234539599e-14\n",
      "SubGD iter. 47/499: loss=379190.18420530396, w0=33.60000000000001, w1=7.769618282082995e-14\n",
      "SubGD iter. 48/499: loss=367883.14791657915, w0=34.30000000000001, w1=7.93148532962639e-14\n",
      "SubGD iter. 49/499: loss=356772.1116278544, w0=35.000000000000014, w1=8.093352377169786e-14\n",
      "SubGD iter. 50/499: loss=345857.0753391297, w0=35.70000000000002, w1=8.255219424713182e-14\n",
      "SubGD iter. 51/499: loss=335138.039050405, w0=36.40000000000002, w1=8.417086472256577e-14\n",
      "SubGD iter. 52/499: loss=324615.0027616802, w0=37.10000000000002, w1=8.578953519799973e-14\n",
      "SubGD iter. 53/499: loss=314287.96647295536, w0=37.800000000000026, w1=8.740820567343368e-14\n",
      "SubGD iter. 54/499: loss=304156.9301842306, w0=38.50000000000003, w1=8.902687614886764e-14\n",
      "SubGD iter. 55/499: loss=294221.89389550587, w0=39.20000000000003, w1=9.06455466243016e-14\n",
      "SubGD iter. 56/499: loss=284482.85760678095, w0=39.900000000000034, w1=9.226421709973555e-14\n",
      "SubGD iter. 57/499: loss=274939.8213180562, w0=40.60000000000004, w1=9.388288757516951e-14\n",
      "SubGD iter. 58/499: loss=265592.7850293315, w0=41.30000000000004, w1=9.550155805060346e-14\n",
      "SubGD iter. 59/499: loss=256441.7487406068, w0=42.00000000000004, w1=9.712022852603742e-14\n",
      "SubGD iter. 60/499: loss=247486.712451882, w0=42.700000000000045, w1=9.873889900147138e-14\n",
      "SubGD iter. 61/499: loss=238727.67616315727, w0=43.40000000000005, w1=1.0035756947690533e-13\n",
      "SubGD iter. 62/499: loss=230164.6398744325, w0=44.10000000000005, w1=1.0197623995233929e-13\n",
      "SubGD iter. 63/499: loss=221797.60358570775, w0=44.800000000000054, w1=1.0359491042777325e-13\n",
      "SubGD iter. 64/499: loss=213626.567296983, w0=45.50000000000006, w1=1.052135809032072e-13\n",
      "SubGD iter. 65/499: loss=205651.53100825823, w0=46.20000000000006, w1=1.0683225137864116e-13\n",
      "SubGD iter. 66/499: loss=197872.49471953354, w0=46.90000000000006, w1=1.0845092185407511e-13\n",
      "SubGD iter. 67/499: loss=190289.45843080877, w0=47.59300000000006, w1=0.012822729222836499\n",
      "SubGD iter. 68/499: loss=182901.1079718728, w0=48.27900000000006, w1=0.038067565076881145\n",
      "SubGD iter. 69/499: loss=175704.1330349569, w0=48.96500000000006, w1=0.0633124009309258\n",
      "SubGD iter. 70/499: loss=168695.65141873603, w0=49.63000000000006, w1=0.1214025307684515\n",
      "SubGD iter. 71/499: loss=161888.3897370158, w0=50.28800000000006, w1=0.19295915708007633\n",
      "SubGD iter. 72/499: loss=155247.7503162031, w0=50.94600000000006, w1=0.2645157833917011\n",
      "SubGD iter. 73/499: loss=148782.3446356982, w0=51.59000000000006, w1=0.36003710381024256\n",
      "SubGD iter. 74/499: loss=142480.1442958482, w0=52.22000000000006, w1=0.47488016550950574\n",
      "SubGD iter. 75/499: loss=136359.07481978484, w0=52.84300000000006, w1=0.6005235649987695\n",
      "SubGD iter. 76/499: loss=130400.2750906532, w0=53.45900000000006, w1=0.7354663252923087\n",
      "SubGD iter. 77/499: loss=124608.38935999108, w0=54.06800000000006, w1=0.879825345160365\n",
      "SubGD iter. 78/499: loss=118979.11334612538, w0=54.66300000000006, w1=1.0417652853159098\n",
      "SubGD iter. 79/499: loss=113518.18004249014, w0=55.25800000000006, w1=1.2037052254714546\n",
      "SubGD iter. 80/499: loss=108209.34655654192, w0=55.83900000000006, w1=1.3822638190738537\n",
      "SubGD iter. 81/499: loss=103065.12263438305, w0=56.41300000000006, w1=1.5664626351241209\n",
      "SubGD iter. 82/499: loss=98087.74978207957, w0=56.95200000000006, w1=1.7871599649710048\n",
      "SubGD iter. 83/499: loss=93303.50618484008, w0=57.484000000000066, w1=2.0138351212184635\n",
      "SubGD iter. 84/499: loss=88670.35417055746, w0=58.01600000000007, w1=2.2405102774659222\n",
      "SubGD iter. 85/499: loss=84170.96440685877, w0=58.51300000000007, w1=2.4977608864129444\n",
      "SubGD iter. 86/499: loss=79870.0823125567, w0=59.01000000000007, w1=2.7550114953599665\n",
      "SubGD iter. 87/499: loss=75694.47496857606, w0=59.493000000000066, w1=3.0191142997242246\n",
      "SubGD iter. 88/499: loss=71691.87259119621, w0=59.97600000000006, w1=3.2832171040884828\n",
      "SubGD iter. 89/499: loss=67810.48593032558, w0=60.45200000000006, w1=3.5478129339056137\n",
      "SubGD iter. 90/499: loss=64085.054808687055, w0=60.893000000000065, w1=3.842709112855976\n",
      "SubGD iter. 91/499: loss=60527.306266928375, w0=61.320000000000064, w1=4.148442254175174\n",
      "SubGD iter. 92/499: loss=57106.25520297314, w0=61.740000000000066, w1=4.4587872266138335\n",
      "SubGD iter. 93/499: loss=53810.33061657282, w0=62.16000000000007, w1=4.769132199052493\n",
      "SubGD iter. 94/499: loss=50623.491630939716, w0=62.58000000000007, w1=5.079477171491152\n",
      "SubGD iter. 95/499: loss=47545.73824607376, w0=62.97900000000007, w1=5.401328404320416\n",
      "SubGD iter. 96/499: loss=44624.67841640288, w0=63.37100000000007, w1=5.725928703251695\n",
      "SubGD iter. 97/499: loss=41827.833039344674, w0=63.74900000000007, w1=6.061744039727737\n",
      "SubGD iter. 98/499: loss=39152.136219600514, w0=64.12000000000008, w1=6.401747716728312\n",
      "SubGD iter. 99/499: loss=36591.79623760319, w0=64.49100000000007, w1=6.741751393728887\n",
      "SubGD iter. 100/499: loss=34132.75365575545, w0=64.87600000000008, w1=7.061378131166473\n",
      "SubGD iter. 101/499: loss=31786.283619545025, w0=65.24700000000007, w1=7.3875235938807595\n",
      "SubGD iter. 102/499: loss=29568.395548270284, w0=65.61800000000007, w1=7.713669056595046\n",
      "SubGD iter. 103/499: loss=27448.112222135205, w0=65.98200000000007, w1=8.040435506798621\n",
      "SubGD iter. 104/499: loss=25445.246038194386, w0=66.34600000000007, w1=8.353859837656545\n",
      "SubGD iter. 105/499: loss=23570.724531864507, w0=66.71700000000007, w1=8.653513683656943\n",
      "SubGD iter. 106/499: loss=21801.154620129375, w0=67.08100000000007, w1=8.95552422377146\n",
      "SubGD iter. 107/499: loss=20135.68698054797, w0=67.43800000000007, w1=9.254985017110497\n",
      "SubGD iter. 108/499: loss=18582.359355599, w0=67.78100000000008, w1=9.560846653933988\n",
      "SubGD iter. 109/499: loss=17136.018073671745, w0=68.11700000000008, w1=9.86033726712715\n",
      "SubGD iter. 110/499: loss=15801.343619436093, w0=68.43200000000007, w1=10.156521139455196\n",
      "SubGD iter. 111/499: loss=14597.010286458291, w0=68.74700000000007, w1=10.444453477348478\n",
      "SubGD iter. 112/499: loss=13480.748894301185, w0=69.03400000000008, w1=10.743406876764533\n",
      "SubGD iter. 113/499: loss=12472.205519065215, w0=69.31400000000008, w1=11.038005391074622\n",
      "SubGD iter. 114/499: loss=11550.424882235662, w0=69.59400000000008, w1=11.313287847901115\n",
      "SubGD iter. 115/499: loss=10719.079108888365, w0=69.86700000000008, w1=11.578413278202836\n",
      "SubGD iter. 116/499: loss=9971.688984850638, w0=70.14000000000007, w1=11.843538708504557\n",
      "SubGD iter. 117/499: loss=9282.227058329981, w0=70.39900000000007, w1=12.104101240231243\n",
      "SubGD iter. 118/499: loss=8673.084845833075, w0=70.65100000000007, w1=12.367229834689166\n",
      "SubGD iter. 119/499: loss=8124.104420774013, w0=70.90300000000006, w1=12.60741061920911\n",
      "SubGD iter. 120/499: loss=7645.264242816554, w0=71.14800000000007, w1=12.829819894660037\n",
      "SubGD iter. 121/499: loss=7233.483330092546, w0=71.37900000000006, w1=13.051299546685456\n",
      "SubGD iter. 122/499: loss=6878.607388850523, w0=71.59600000000006, w1=13.253978279130111\n",
      "SubGD iter. 123/499: loss=6585.178206374354, w0=71.80600000000005, w1=13.443752555762323\n",
      "SubGD iter. 124/499: loss=6337.4132624727, w0=71.98100000000005, w1=13.600012567339114\n",
      "SubGD iter. 125/499: loss=6156.081475904989, w0=72.12100000000005, w1=13.709638751573939\n",
      "SubGD iter. 126/499: loss=6031.53427564995, w0=72.25400000000005, w1=13.82371029755365\n",
      "SubGD iter. 127/499: loss=5922.3203054383675, w0=72.35900000000005, w1=13.928950177753476\n",
      "SubGD iter. 128/499: loss=5841.406616057503, w0=72.45700000000005, w1=14.033063640618627\n",
      "SubGD iter. 129/499: loss=5772.8132482589035, w0=72.53400000000005, w1=14.118590076651559\n",
      "SubGD iter. 130/499: loss=5724.120599814981, w0=72.61100000000005, w1=14.204116512684491\n",
      "SubGD iter. 131/499: loss=5680.725459875253, w0=72.66700000000004, w1=14.262362322468713\n",
      "SubGD iter. 132/499: loss=5652.827117978867, w0=72.71600000000005, w1=14.312508189781875\n",
      "SubGD iter. 133/499: loss=5630.611600408432, w0=72.76500000000006, w1=14.362654057095037\n",
      "SubGD iter. 134/499: loss=5610.362326041436, w0=72.81400000000006, w1=14.4127999244082\n",
      "SubGD iter. 135/499: loss=5592.079294877874, w0=72.86300000000007, w1=14.462945791721362\n",
      "SubGD iter. 136/499: loss=5575.7625069177475, w0=72.91900000000007, w1=14.500669552403208\n",
      "SubGD iter. 137/499: loss=5559.250091980438, w0=72.96100000000007, w1=14.528246426173592\n",
      "SubGD iter. 138/499: loss=5548.047390537479, w0=73.01000000000008, w1=14.543000570721249\n",
      "SubGD iter. 139/499: loss=5535.724269530843, w0=73.04500000000007, w1=14.568377913364827\n",
      "SubGD iter. 140/499: loss=5528.069371355454, w0=73.07300000000008, w1=14.586058146365003\n",
      "SubGD iter. 141/499: loss=5522.366630288539, w0=73.10100000000008, w1=14.60373837936518\n",
      "SubGD iter. 142/499: loss=5517.102525477199, w0=73.12900000000009, w1=14.621418612365355\n",
      "SubGD iter. 143/499: loss=5512.277056921438, w0=73.16400000000009, w1=14.626276116142806\n",
      "SubGD iter. 144/499: loss=5505.764083562875, w0=73.18500000000009, w1=14.637490968842966\n",
      "SubGD iter. 145/499: loss=5502.619403195024, w0=73.20600000000009, w1=14.648705821543127\n",
      "SubGD iter. 146/499: loss=5499.701431995606, w0=73.22700000000009, w1=14.659920674243287\n",
      "SubGD iter. 147/499: loss=5497.010169964624, w0=73.24800000000009, w1=14.671135526943447\n",
      "SubGD iter. 148/499: loss=5494.545617102076, w0=73.26900000000009, w1=14.682350379643607\n",
      "SubGD iter. 149/499: loss=5492.307773407964, w0=73.29000000000009, w1=14.693565232343767\n",
      "SubGD iter. 150/499: loss=5490.296638882284, w0=73.30400000000009, w1=14.70389782227299\n",
      "SubGD iter. 151/499: loss=5489.340890278487, w0=73.30400000000009, w1=14.699292575283678\n",
      "SubGD iter. 152/499: loss=5488.924777987736, w0=73.30400000000009, w1=14.712641216925142\n",
      "SubGD iter. 153/499: loss=5490.154252061406, w0=73.30400000000009, w1=14.70803596993583\n",
      "SubGD iter. 154/499: loss=5489.7220335738975, w0=73.30400000000009, w1=14.703430722946518\n",
      "SubGD iter. 155/499: loss=5489.29829840632, w0=73.31100000000009, w1=14.704357257956666\n",
      "SubGD iter. 156/499: loss=5488.473505600301, w0=73.30400000000009, w1=14.71217411759867\n",
      "SubGD iter. 157/499: loss=5490.1100265757395, w0=73.30400000000009, w1=14.707568870609357\n",
      "SubGD iter. 158/499: loss=5489.6786685313355, w0=73.30400000000009, w1=14.702963623620045\n",
      "SubGD iter. 159/499: loss=5489.255793806868, w0=73.31100000000009, w1=14.703890158630193\n",
      "SubGD iter. 160/499: loss=5488.430827887295, w0=73.30400000000009, w1=14.711707018272197\n",
      "SubGD iter. 161/499: loss=5490.065888362785, w0=73.30400000000009, w1=14.707101771282884\n",
      "SubGD iter. 162/499: loss=5489.6353907614875, w0=73.30400000000009, w1=14.702496524293572\n",
      "SubGD iter. 163/499: loss=5489.213376480124, w0=73.31100000000009, w1=14.70342305930372\n",
      "SubGD iter. 164/499: loss=5488.388237447003, w0=73.30400000000009, w1=14.711239918945724\n",
      "SubGD iter. 165/499: loss=5490.021837422543, w0=73.30400000000009, w1=14.706634671956412\n",
      "SubGD iter. 166/499: loss=5489.592200264352, w0=73.30400000000009, w1=14.7020294249671\n",
      "SubGD iter. 167/499: loss=5489.171046426097, w0=73.31100000000009, w1=14.702955959977247\n",
      "SubGD iter. 168/499: loss=5488.345734279422, w0=73.30400000000009, w1=14.710772819619251\n",
      "SubGD iter. 169/499: loss=5489.977873755011, w0=73.30400000000009, w1=14.706167572629939\n",
      "SubGD iter. 170/499: loss=5489.54909703993, w0=73.30400000000009, w1=14.701562325640626\n",
      "SubGD iter. 171/499: loss=5489.128803644777, w0=73.31100000000009, w1=14.702488860650774\n",
      "SubGD iter. 172/499: loss=5488.303318384551, w0=73.30400000000009, w1=14.710305720292778\n",
      "SubGD iter. 173/499: loss=5489.933997360194, w0=73.30400000000009, w1=14.705700473303466\n",
      "SubGD iter. 174/499: loss=5489.506081088218, w0=73.30400000000009, w1=14.701095226314154\n",
      "SubGD iter. 175/499: loss=5489.086648136176, w0=73.30400000000009, w1=14.714443867955618\n",
      "SubGD iter. 176/499: loss=5490.325747386889, w0=73.30400000000009, w1=14.709838620966305\n",
      "SubGD iter. 177/499: loss=5489.890208238087, w0=73.30400000000009, w1=14.705233373976993\n",
      "SubGD iter. 178/499: loss=5489.463152409219, w0=73.30400000000009, w1=14.70062812698768\n",
      "SubGD iter. 179/499: loss=5489.044579900283, w0=73.30400000000009, w1=14.713976768629145\n",
      "SubGD iter. 180/499: loss=5490.28118509439, w0=73.30400000000009, w1=14.709371521639833\n",
      "SubGD iter. 181/499: loss=5489.846506388694, w0=73.30400000000009, w1=14.70476627465052\n",
      "SubGD iter. 182/499: loss=5489.420311002931, w0=73.30400000000009, w1=14.700161027661208\n",
      "SubGD iter. 183/499: loss=5489.0025989371015, w0=73.30400000000009, w1=14.713509669302672\n",
      "SubGD iter. 184/499: loss=5490.2367100746005, w0=73.30400000000009, w1=14.70890442231336\n",
      "SubGD iter. 185/499: loss=5489.8028918120135, w0=73.30400000000009, w1=14.704299175324048\n",
      "SubGD iter. 186/499: loss=5489.377556869356, w0=73.30400000000009, w1=14.699693928334735\n",
      "SubGD iter. 187/499: loss=5488.960705246634, w0=73.30400000000009, w1=14.7130425699762\n",
      "SubGD iter. 188/499: loss=5490.192322327527, w0=73.30400000000009, w1=14.708437322986887\n",
      "SubGD iter. 189/499: loss=5489.759364508044, w0=73.30400000000009, w1=14.703832075997575\n",
      "SubGD iter. 190/499: loss=5489.334890008496, w0=73.31100000000009, w1=14.704758611007723\n",
      "SubGD iter. 191/499: loss=5488.5102459495365, w0=73.30400000000009, w1=14.712575470649726\n",
      "SubGD iter. 192/499: loss=5490.148021853162, w0=73.30400000000009, w1=14.707970223660414\n",
      "SubGD iter. 193/499: loss=5489.715924476786, w0=73.30400000000009, w1=14.703364976671102\n",
      "SubGD iter. 194/499: loss=5489.292310420346, w0=73.31100000000009, w1=14.70429151168125\n",
      "SubGD iter. 195/499: loss=5488.467493247836, w0=73.30400000000009, w1=14.712108371323254\n",
      "SubGD iter. 196/499: loss=5490.10380865151, w0=73.30400000000009, w1=14.707503124333941\n",
      "SubGD iter. 197/499: loss=5489.672571718244, w0=73.30400000000009, w1=14.702897877344629\n",
      "SubGD iter. 198/499: loss=5489.249818104909, w0=73.31100000000009, w1=14.703824412354777\n",
      "SubGD iter. 199/499: loss=5488.424827818845, w0=73.30400000000009, w1=14.71164127199678\n",
      "SubGD iter. 200/499: loss=5490.059682722573, w0=73.30400000000009, w1=14.707036025007469\n",
      "SubGD iter. 201/499: loss=5489.629306232411, w0=73.30400000000009, w1=14.702430778018156\n",
      "SubGD iter. 202/499: loss=5489.207413062181, w0=73.31100000000009, w1=14.703357313028304\n",
      "SubGD iter. 203/499: loss=5488.38224966257, w0=73.30400000000009, w1=14.711174172670308\n",
      "SubGD iter. 204/499: loss=5490.015644066347, w0=73.30400000000009, w1=14.706568925680996\n",
      "SubGD iter. 205/499: loss=5489.586128019292, w0=73.30400000000009, w1=14.701963678691683\n",
      "SubGD iter. 206/499: loss=5489.165095292169, w0=73.31100000000009, w1=14.702890213701831\n",
      "SubGD iter. 207/499: loss=5488.339758779005, w0=73.30400000000009, w1=14.710707073343835\n",
      "SubGD iter. 208/499: loss=5489.971692682833, w0=73.30400000000009, w1=14.706101826354523\n",
      "SubGD iter. 209/499: loss=5489.543037078886, w0=73.30400000000009, w1=14.70149657936521\n",
      "SubGD iter. 210/499: loss=5489.12286479487, w0=73.31100000000009, w1=14.702423114375359\n",
      "SubGD iter. 211/499: loss=5488.297355168152, w0=73.30400000000009, w1=14.710239974017362\n",
      "SubGD iter. 212/499: loss=5489.927828572032, w0=73.30400000000009, w1=14.70563472702805\n",
      "SubGD iter. 213/499: loss=5489.500033411188, w0=73.30400000000009, w1=14.701029480038738\n",
      "SubGD iter. 214/499: loss=5489.080721570279, w0=73.30400000000009, w1=14.714378121680202\n",
      "SubGD iter. 215/499: loss=5490.319469771608, w0=73.30400000000009, w1=14.70977287469089\n",
      "SubGD iter. 216/499: loss=5489.88405173394, w0=73.30400000000009, w1=14.705167627701577\n",
      "SubGD iter. 217/499: loss=5489.457117016207, w0=73.30400000000009, w1=14.700562380712265\n",
      "SubGD iter. 218/499: loss=5489.0386656184055, w0=73.30400000000009, w1=14.713911022353729\n",
      "SubGD iter. 219/499: loss=5490.274919763124, w0=73.30400000000009, w1=14.709305775364417\n",
      "SubGD iter. 220/499: loss=5489.840362168563, w0=73.30400000000009, w1=14.704700528375104\n",
      "SubGD iter. 221/499: loss=5489.414287893936, w0=73.30400000000009, w1=14.700095281385792\n",
      "SubGD iter. 222/499: loss=5488.996696939242, w0=73.30400000000009, w1=14.713443923027256\n",
      "SubGD iter. 223/499: loss=5490.230457027353, w0=73.30400000000009, w1=14.708838676037944\n",
      "SubGD iter. 224/499: loss=5489.7967598759, w0=73.30400000000009, w1=14.704233429048632\n",
      "SubGD iter. 225/499: loss=5489.371546044378, w0=73.30400000000009, w1=14.69962818205932\n",
      "SubGD iter. 226/499: loss=5488.954815532791, w0=73.30400000000009, w1=14.712976823700783\n",
      "SubGD iter. 227/499: loss=5490.186081564294, w0=73.30400000000009, w1=14.708371576711471\n",
      "SubGD iter. 228/499: loss=5489.753244855948, w0=73.30400000000009, w1=14.703766329722159\n",
      "SubGD iter. 229/499: loss=5489.328891467534, w0=73.31100000000009, w1=14.704692864732307\n",
      "SubGD iter. 230/499: loss=5488.504223042083, w0=73.30400000000009, w1=14.71250972437431\n",
      "SubGD iter. 231/499: loss=5490.1417933739485, w0=73.30400000000009, w1=14.707904477384998\n",
      "SubGD iter. 232/499: loss=5489.709817108706, w0=73.30400000000009, w1=14.703299230395686\n",
      "SubGD iter. 233/499: loss=5489.286324163399, w0=73.31100000000009, w1=14.704225765405834\n",
      "SubGD iter. 234/499: loss=5488.461482624401, w0=73.30400000000009, w1=14.712042625047838\n",
      "SubGD iter. 235/499: loss=5490.097592456314, w0=73.30400000000009, w1=14.707437378058525\n",
      "SubGD iter. 236/499: loss=5489.666476634178, w0=73.30400000000009, w1=14.702832131069213\n",
      "SubGD iter. 237/499: loss=5489.243844131977, w0=73.31100000000009, w1=14.703758666079361\n",
      "SubGD iter. 238/499: loss=5488.418829479427, w0=73.30400000000009, w1=14.711575525721365\n",
      "SubGD iter. 239/499: loss=5490.053478811389, w0=73.30400000000009, w1=14.706970278732053\n",
      "SubGD iter. 240/499: loss=5489.623223432365, w0=73.30400000000009, w1=14.70236503174274\n",
      "SubGD iter. 241/499: loss=5489.20145137327, w0=73.31100000000009, w1=14.703291566752888\n",
      "SubGD iter. 242/499: loss=5488.376263607167, w0=73.30400000000009, w1=14.711108426394892\n",
      "SubGD iter. 243/499: loss=5490.00945243918, w0=73.30400000000009, w1=14.70650317940558\n",
      "SubGD iter. 244/499: loss=5489.580057503261, w0=73.30400000000009, w1=14.701897932416268\n",
      "SubGD iter. 245/499: loss=5489.159145887274, w0=73.31100000000009, w1=14.702824467426415\n",
      "SubGD iter. 246/499: loss=5488.333785007617, w0=73.30400000000009, w1=14.71064132706842\n",
      "SubGD iter. 247/499: loss=5489.965513339684, w0=73.30400000000009, w1=14.706036080079107\n",
      "SubGD iter. 248/499: loss=5489.536978846871, w0=73.30400000000009, w1=14.701430833089795\n",
      "SubGD iter. 249/499: loss=5489.116927673989, w0=73.31100000000009, w1=14.702357368099943\n",
      "SubGD iter. 250/499: loss=5488.291393680784, w0=73.30400000000009, w1=14.710174227741946\n",
      "SubGD iter. 251/499: loss=5489.921661512899, w0=73.30400000000009, w1=14.705568980752634\n",
      "SubGD iter. 252/499: loss=5489.493987463189, w0=73.30400000000009, w1=14.700963733763322\n",
      "SubGD iter. 253/499: loss=5489.074796733418, w0=73.30400000000009, w1=14.714312375404786\n",
      "SubGD iter. 254/499: loss=5490.313193885358, w0=73.30400000000009, w1=14.709707128415474\n",
      "SubGD iter. 255/499: loss=5489.877896958824, w0=73.30400000000009, w1=14.705101881426161\n",
      "SubGD iter. 256/499: loss=5489.451083352225, w0=73.30400000000009, w1=14.700496634436849\n",
      "SubGD iter. 257/499: loss=5489.032753065559, w0=73.30400000000009, w1=14.713845276078313\n",
      "SubGD iter. 258/499: loss=5490.268656160891, w0=73.30400000000009, w1=14.709240029089\n",
      "SubGD iter. 259/499: loss=5489.834219677463, w0=73.30400000000009, w1=14.704634782099689\n",
      "SubGD iter. 260/499: loss=5489.408266513971, w0=73.30400000000009, w1=14.700029535110376\n",
      "SubGD iter. 261/499: loss=5488.990796670411, w0=73.30400000000009, w1=14.71337817675184\n",
      "SubGD iter. 262/499: loss=5490.2242057091335, w0=73.30400000000009, w1=14.708772929762528\n",
      "SubGD iter. 263/499: loss=5489.790629668815, w0=73.30400000000009, w1=14.704167682773216\n",
      "SubGD iter. 264/499: loss=5489.365536948429, w0=73.30400000000009, w1=14.699562435783903\n",
      "SubGD iter. 265/499: loss=5488.948927547977, w0=73.30400000000009, w1=14.712911077425368\n",
      "SubGD iter. 266/499: loss=5490.179842530092, w0=73.30400000000009, w1=14.708305830436055\n",
      "SubGD iter. 267/499: loss=5489.747126932879, w0=73.30400000000009, w1=14.703700583446743\n",
      "SubGD iter. 268/499: loss=5489.322894655597, w0=73.31100000000009, w1=14.70462711845689\n",
      "SubGD iter. 269/499: loss=5488.498201863662, w0=73.30400000000009, w1=14.712443978098895\n",
      "SubGD iter. 270/499: loss=5490.135566623763, w0=73.30400000000009, w1=14.707838731109582\n",
      "SubGD iter. 271/499: loss=5489.703711469657, w0=73.30400000000009, w1=14.70323348412027\n",
      "SubGD iter. 272/499: loss=5489.280339635484, w0=73.31100000000009, w1=14.704160019130418\n",
      "SubGD iter. 273/499: loss=5488.455473729992, w0=73.30400000000009, w1=14.711976878772422\n",
      "SubGD iter. 274/499: loss=5490.091377990143, w0=73.30400000000009, w1=14.70737163178311\n",
      "SubGD iter. 275/499: loss=5489.660383279145, w0=73.30400000000009, w1=14.702766384793797\n",
      "SubGD iter. 276/499: loss=5489.237871888079, w0=73.31100000000009, w1=14.703692919803945\n",
      "SubGD iter. 277/499: loss=5488.412832869035, w0=73.30400000000009, w1=14.711509779445949\n",
      "SubGD iter. 278/499: loss=5490.047276629237, w0=73.30400000000009, w1=14.706904532456637\n",
      "SubGD iter. 279/499: loss=5489.617142361345, w0=73.30400000000009, w1=14.702299285467324\n",
      "SubGD iter. 280/499: loss=5489.195491413385, w0=73.31100000000009, w1=14.703225820477472\n",
      "SubGD iter. 281/499: loss=5488.370279280789, w0=73.30400000000009, w1=14.711042680119476\n",
      "SubGD iter. 282/499: loss=5490.003262541041, w0=73.30400000000009, w1=14.706437433130164\n",
      "SubGD iter. 283/499: loss=5489.573988716261, w0=73.30400000000009, w1=14.701832186140852\n",
      "SubGD iter. 284/499: loss=5489.153198211406, w0=73.31100000000009, w1=14.702758721151\n",
      "SubGD iter. 285/499: loss=5488.32781296526, w0=73.30400000000009, w1=14.710575580793003\n",
      "SubGD iter. 286/499: loss=5489.959335725562, w0=73.30400000000009, w1=14.705970333803691\n",
      "SubGD iter. 287/499: loss=5489.530922343883, w0=73.30400000000009, w1=14.701365086814379\n",
      "SubGD iter. 288/499: loss=5489.11099228214, w0=73.31100000000009, w1=14.702291621824527\n",
      "SubGD iter. 289/499: loss=5488.285433922441, w0=73.30400000000009, w1=14.71010848146653\n",
      "SubGD iter. 290/499: loss=5489.915496182793, w0=73.30400000000009, w1=14.705503234477218\n",
      "SubGD iter. 291/499: loss=5489.487943244223, w0=73.30400000000009, w1=14.700897987487906\n",
      "SubGD iter. 292/499: loss=5489.068873625584, w0=73.30400000000009, w1=14.71424662912937\n",
      "SubGD iter. 293/499: loss=5490.306919728136, w0=73.30400000000009, w1=14.709641382140058\n",
      "SubGD iter. 294/499: loss=5489.871743912736, w0=73.30400000000009, w1=14.705036135150745\n",
      "SubGD iter. 295/499: loss=5489.445051417274, w0=73.30400000000009, w1=14.700430888161433\n",
      "SubGD iter. 296/499: loss=5489.026842241738, w0=73.30400000000009, w1=14.713779529802897\n",
      "SubGD iter. 297/499: loss=5490.262394287684, w0=73.30400000000009, w1=14.709174282813585\n",
      "SubGD iter. 298/499: loss=5489.828078915392, w0=73.30400000000009, w1=14.704569035824273\n",
      "SubGD iter. 299/499: loss=5489.402246863035, w0=73.30400000000009, w1=14.69996378883496\n",
      "SubGD iter. 300/499: loss=5488.984898130609, w0=73.30400000000009, w1=14.713312430476424\n",
      "SubGD iter. 301/499: loss=5490.217956119945, w0=73.30400000000009, w1=14.708707183487112\n",
      "SubGD iter. 302/499: loss=5489.78450119076, w0=73.30400000000009, w1=14.7041019364978\n",
      "SubGD iter. 303/499: loss=5489.359529581511, w0=73.30400000000009, w1=14.699496689508488\n",
      "SubGD iter. 304/499: loss=5488.94304129219, w0=73.30400000000009, w1=14.712845331149952\n",
      "SubGD iter. 305/499: loss=5490.17360522492, w0=73.30400000000009, w1=14.70824008416064\n",
      "SubGD iter. 306/499: loss=5489.7410107388405, w0=73.30400000000009, w1=14.703634837171327\n",
      "SubGD iter. 307/499: loss=5489.316899572697, w0=73.31100000000009, w1=14.704561372181475\n",
      "SubGD iter. 308/499: loss=5488.4921824142675, w0=73.30400000000009, w1=14.712378231823479\n",
      "SubGD iter. 309/499: loss=5490.129341602605, w0=73.30400000000009, w1=14.707772984834167\n",
      "SubGD iter. 310/499: loss=5489.697607559633, w0=73.30400000000009, w1=14.703167737844854\n",
      "SubGD iter. 311/499: loss=5489.274356836595, w0=73.31100000000009, w1=14.704094272855002\n",
      "SubGD iter. 312/499: loss=5488.449466564613, w0=73.30400000000009, w1=14.711911132497006\n",
      "SubGD iter. 313/499: loss=5490.085165253002, w0=73.30400000000009, w1=14.707305885507694\n",
      "SubGD iter. 314/499: loss=5489.654291653138, w0=73.30400000000009, w1=14.702700638518381\n",
      "SubGD iter. 315/499: loss=5489.231901373208, w0=73.31100000000009, w1=14.70362717352853\n",
      "SubGD iter. 316/499: loss=5488.406837987674, w0=73.30400000000009, w1=14.711444033170533\n",
      "SubGD iter. 317/499: loss=5490.041076176112, w0=73.30400000000009, w1=14.70683878618122\n",
      "SubGD iter. 318/499: loss=5489.611063019354, w0=73.30400000000009, w1=14.702233539191909\n",
      "SubGD iter. 319/499: loss=5489.189533182531, w0=73.31100000000009, w1=14.703160074202057\n",
      "SubGD iter. 320/499: loss=5488.364296683446, w0=73.30400000000009, w1=14.71097693384406\n",
      "SubGD iter. 321/499: loss=5489.997074371937, w0=73.30400000000009, w1=14.706371686854748\n",
      "SubGD iter. 322/499: loss=5489.567921658284, w0=73.30400000000009, w1=14.701766439865436\n",
      "SubGD iter. 323/499: loss=5489.147252264566, w0=73.31100000000009, w1=14.702692974875584\n",
      "SubGD iter. 324/499: loss=5488.3218426519325, w0=73.30400000000009, w1=14.710509834517588\n",
      "SubGD iter. 325/499: loss=5489.953159840468, w0=73.30400000000009, w1=14.705904587528275\n",
      "SubGD iter. 326/499: loss=5489.524867569926, w0=73.30400000000009, w1=14.701299340538963\n",
      "SubGD iter. 327/499: loss=5489.105058619317, w0=73.31100000000009, w1=14.70222587554911\n",
      "SubGD iter. 328/499: loss=5488.279475893127, w0=73.30400000000009, w1=14.710042735191115\n",
      "SubGD iter. 329/499: loss=5489.909332581718, w0=73.30400000000009, w1=14.705437488201802\n",
      "SubGD iter. 330/499: loss=5489.481900754281, w0=73.30400000000009, w1=14.70083224121249\n",
      "SubGD iter. 331/499: loss=5489.062952246777, w0=73.30400000000009, w1=14.714180882853954\n",
      "SubGD iter. 332/499: loss=5490.30064729994, w0=73.30400000000009, w1=14.709575635864642\n",
      "SubGD iter. 333/499: loss=5489.865592595677, w0=73.30400000000009, w1=14.70497038887533\n",
      "SubGD iter. 334/499: loss=5489.439021211346, w0=73.30400000000009, w1=14.700365141886017\n",
      "SubGD iter. 335/499: loss=5489.02093314695, w0=73.30400000000009, w1=14.713713783527481\n",
      "SubGD iter. 336/499: loss=5490.256134143507, w0=73.30400000000009, w1=14.709108536538169\n",
      "SubGD iter. 337/499: loss=5489.82193988235, w0=73.30400000000009, w1=14.704503289548857\n",
      "SubGD iter. 338/499: loss=5489.396228941126, w0=73.30400000000009, w1=14.699898042559544\n",
      "SubGD iter. 339/499: loss=5488.979001319838, w0=73.30400000000009, w1=14.713246684201009\n",
      "SubGD iter. 340/499: loss=5490.211708259783, w0=73.30400000000009, w1=14.708641437211696\n",
      "SubGD iter. 341/499: loss=5489.778374441736, w0=73.30400000000009, w1=14.704036190222384\n",
      "SubGD iter. 342/499: loss=5489.353523943617, w0=73.30400000000009, w1=14.699430943233072\n",
      "SubGD iter. 343/499: loss=5488.937156765431, w0=73.30400000000009, w1=14.712779584874536\n",
      "SubGD iter. 344/499: loss=5490.1673696487715, w0=73.30400000000009, w1=14.708174337885223\n",
      "SubGD iter. 345/499: loss=5489.73489627383, w0=73.30400000000009, w1=14.703569090895911\n",
      "SubGD iter. 346/499: loss=5489.31090621882, w0=73.31100000000009, w1=14.704495625906059\n",
      "SubGD iter. 347/499: loss=5488.4861646939, w0=73.30400000000009, w1=14.712312485548063\n",
      "SubGD iter. 348/499: loss=5490.123118310476, w0=73.30400000000009, w1=14.70770723855875\n",
      "SubGD iter. 349/499: loss=5489.6915053786415, w0=73.30400000000009, w1=14.703101991569438\n",
      "SubGD iter. 350/499: loss=5489.2683757667355, w0=73.31100000000009, w1=14.704028526579586\n",
      "SubGD iter. 351/499: loss=5488.443461128264, w0=73.30400000000009, w1=14.71184538622159\n",
      "SubGD iter. 352/499: loss=5490.078954244891, w0=73.30400000000009, w1=14.707240139232278\n",
      "SubGD iter. 353/499: loss=5489.648201756161, w0=73.30400000000009, w1=14.702634892242965\n",
      "SubGD iter. 354/499: loss=5489.225932587365, w0=73.31100000000009, w1=14.703561427253113\n",
      "SubGD iter. 355/499: loss=5488.400844835341, w0=73.30400000000009, w1=14.711378286895117\n",
      "SubGD iter. 356/499: loss=5490.034877452018, w0=73.30400000000009, w1=14.706773039905805\n",
      "SubGD iter. 357/499: loss=5489.604985406396, w0=73.30400000000009, w1=14.702167792916493\n",
      "SubGD iter. 358/499: loss=5489.183576680704, w0=73.31100000000009, w1=14.70309432792664\n",
      "SubGD iter. 359/499: loss=5488.358315815131, w0=73.30400000000009, w1=14.710911187568644\n",
      "SubGD iter. 360/499: loss=5489.990887931856, w0=73.30400000000009, w1=14.706305940579332\n",
      "SubGD iter. 361/499: loss=5489.561856329342, w0=73.30400000000009, w1=14.70170069359002\n",
      "SubGD iter. 362/499: loss=5489.141308046758, w0=73.31100000000009, w1=14.702627228600168\n",
      "SubGD iter. 363/499: loss=5488.315874067632, w0=73.30400000000009, w1=14.710444088242172\n",
      "SubGD iter. 364/499: loss=5489.946985684408, w0=73.30400000000009, w1=14.70583884125286\n",
      "SubGD iter. 365/499: loss=5489.5188145249995, w0=73.30400000000009, w1=14.701233594263547\n",
      "SubGD iter. 366/499: loss=5489.099126685522, w0=73.31100000000009, w1=14.702160129273695\n",
      "SubGD iter. 367/499: loss=5488.273519592843, w0=73.30400000000009, w1=14.709976988915699\n",
      "SubGD iter. 368/499: loss=5489.903170709671, w0=73.30400000000009, w1=14.705371741926387\n",
      "SubGD iter. 369/499: loss=5489.475859993369, w0=73.30400000000009, w1=14.700766494937074\n",
      "SubGD iter. 370/499: loss=5489.057032596999, w0=73.30400000000009, w1=14.714115136578538\n",
      "SubGD iter. 371/499: loss=5490.294376600777, w0=73.30400000000009, w1=14.709509889589226\n",
      "SubGD iter. 372/499: loss=5489.859443007646, w0=73.30400000000009, w1=14.704904642599914\n",
      "SubGD iter. 373/499: loss=5489.4329927344525, w0=73.30400000000009, w1=14.700299395610601\n",
      "SubGD iter. 374/499: loss=5489.015025781191, w0=73.30400000000009, w1=14.713648037252065\n",
      "SubGD iter. 375/499: loss=5490.249875728358, w0=73.30400000000009, w1=14.709042790262753\n",
      "SubGD iter. 376/499: loss=5489.815802578334, w0=73.30400000000009, w1=14.70443754327344\n",
      "SubGD iter. 377/499: loss=5489.390212748247, w0=73.30400000000009, w1=14.699832296284129\n",
      "SubGD iter. 378/499: loss=5488.973106238091, w0=73.30400000000009, w1=14.713180937925593\n",
      "SubGD iter. 379/499: loss=5490.2054621286525, w0=73.30400000000009, w1=14.70857569093628\n",
      "SubGD iter. 380/499: loss=5489.772249421737, w0=73.30400000000009, w1=14.703970443946968\n",
      "SubGD iter. 381/499: loss=5489.347520034757, w0=73.30400000000009, w1=14.699365196957656\n",
      "SubGD iter. 382/499: loss=5488.931273967707, w0=73.30400000000009, w1=14.71271383859912\n",
      "SubGD iter. 383/499: loss=5490.161135801658, w0=73.30400000000009, w1=14.708108591609808\n",
      "SubGD iter. 384/499: loss=5489.728783537851, w0=73.30400000000009, w1=14.703503344620495\n",
      "SubGD iter. 385/499: loss=5489.304914593976, w0=73.31100000000009, w1=14.704429879630643\n",
      "SubGD iter. 386/499: loss=5488.480148702563, w0=73.30400000000009, w1=14.712246739272647\n",
      "SubGD iter. 387/499: loss=5490.116896747377, w0=73.30400000000009, w1=14.707641492283335\n",
      "SubGD iter. 388/499: loss=5489.685404926676, w0=73.30400000000009, w1=14.703036245294022\n",
      "SubGD iter. 389/499: loss=5489.262396425907, w0=73.31100000000009, w1=14.70396278030417\n",
      "SubGD iter. 390/499: loss=5488.437457420946, w0=73.30400000000009, w1=14.711779639946174\n",
      "SubGD iter. 391/499: loss=5490.072744965809, w0=73.30400000000009, w1=14.707174392956862\n",
      "SubGD iter. 392/499: loss=5489.6421135882165, w0=73.30400000000009, w1=14.70256914596755\n",
      "SubGD iter. 393/499: loss=5489.21996553055, w0=73.31100000000009, w1=14.703495680977698\n",
      "SubGD iter. 394/499: loss=5488.394853412038, w0=73.30400000000009, w1=14.711312540619701\n",
      "SubGD iter. 395/499: loss=5490.028680456951, w0=73.30400000000009, w1=14.706707293630389\n",
      "SubGD iter. 396/499: loss=5489.598909522463, w0=73.30400000000009, w1=14.702102046641077\n",
      "SubGD iter. 397/499: loss=5489.1776219079065, w0=73.31100000000009, w1=14.703028581651225\n",
      "SubGD iter. 398/499: loss=5488.3523366758445, w0=73.30400000000009, w1=14.710845441293229\n",
      "SubGD iter. 399/499: loss=5489.984703220805, w0=73.30400000000009, w1=14.706240194303916\n",
      "SubGD iter. 400/499: loss=5489.555792729425, w0=73.30400000000009, w1=14.701634947314604\n",
      "SubGD iter. 401/499: loss=5489.135365557977, w0=73.31100000000009, w1=14.702561482324752\n",
      "SubGD iter. 402/499: loss=5488.3099072123605, w0=73.30400000000009, w1=14.710378341966756\n",
      "SubGD iter. 403/499: loss=5489.9408132573735, w0=73.30400000000009, w1=14.705773094977443\n",
      "SubGD iter. 404/499: loss=5489.5127632091, w0=73.30400000000009, w1=14.701167847988131\n",
      "SubGD iter. 405/499: loss=5489.093196480757, w0=73.31100000000009, w1=14.702094382998279\n",
      "SubGD iter. 406/499: loss=5488.267565021591, w0=73.30400000000009, w1=14.709911242640283\n",
      "SubGD iter. 407/499: loss=5489.897010566654, w0=73.30400000000009, w1=14.70530599565097\n",
      "SubGD iter. 408/499: loss=5489.469820961488, w0=73.30400000000009, w1=14.700700748661658\n",
      "SubGD iter. 409/499: loss=5489.051114676253, w0=73.30400000000009, w1=14.714049390303122\n",
      "SubGD iter. 410/499: loss=5490.288107630642, w0=73.30400000000009, w1=14.70944414331381\n",
      "SubGD iter. 411/499: loss=5489.853295148648, w0=73.30400000000009, w1=14.704838896324498\n",
      "SubGD iter. 412/499: loss=5489.426965986586, w0=73.30400000000009, w1=14.700233649335186\n",
      "SubGD iter. 413/499: loss=5489.009120144459, w0=73.30400000000009, w1=14.71358229097665\n",
      "SubGD iter. 414/499: loss=5490.2436190422395, w0=73.30400000000009, w1=14.708977043987337\n",
      "SubGD iter. 415/499: loss=5489.809667003351, w0=73.30400000000009, w1=14.704371796998025\n",
      "SubGD iter. 416/499: loss=5489.384198284397, w0=73.30400000000009, w1=14.699766550008713\n",
      "SubGD iter. 417/499: loss=5488.967212885376, w0=73.30400000000009, w1=14.713115191650177\n",
      "SubGD iter. 418/499: loss=5490.199217726549, w0=73.30400000000009, w1=14.708509944660864\n",
      "SubGD iter. 419/499: loss=5489.766126130767, w0=73.30400000000009, w1=14.703904697671552\n",
      "SubGD iter. 420/499: loss=5489.341517854921, w0=73.30400000000009, w1=14.69929945068224\n",
      "SubGD iter. 421/499: loss=5488.9253928990065, w0=73.30400000000009, w1=14.712648092323704\n",
      "SubGD iter. 422/499: loss=5490.15490368357, w0=73.30400000000009, w1=14.708042845334392\n",
      "SubGD iter. 423/499: loss=5489.722672530899, w0=73.30400000000009, w1=14.70343759834508\n",
      "SubGD iter. 424/499: loss=5489.298924698159, w0=73.31100000000009, w1=14.704364133355227\n",
      "SubGD iter. 425/499: loss=5488.474134440256, w0=73.30400000000009, w1=14.712180992997231\n",
      "SubGD iter. 426/499: loss=5490.110676913306, w0=73.30400000000009, w1=14.707575746007919\n",
      "SubGD iter. 427/499: loss=5489.6793062037395, w0=73.30400000000009, w1=14.702970499018607\n",
      "SubGD iter. 428/499: loss=5489.256418814106, w0=73.31100000000009, w1=14.703897034028754\n",
      "SubGD iter. 429/499: loss=5488.431455442654, w0=73.30400000000009, w1=14.711713893670758\n",
      "SubGD iter. 430/499: loss=5490.066537415754, w0=73.30400000000009, w1=14.707108646681446\n",
      "SubGD iter. 431/499: loss=5489.636027149292, w0=73.30400000000009, w1=14.702503399692134\n",
      "SubGD iter. 432/499: loss=5489.214000202768, w0=73.31100000000009, w1=14.703429934702282\n",
      "SubGD iter. 433/499: loss=5488.388863717763, w0=73.30400000000009, w1=14.711246794344286\n",
      "SubGD iter. 434/499: loss=5490.022485190914, w0=73.30400000000009, w1=14.706641547354973\n",
      "SubGD iter. 435/499: loss=5489.592835367559, w0=73.30400000000009, w1=14.702036300365661\n",
      "SubGD iter. 436/499: loss=5489.171668864141, w0=73.31100000000009, w1=14.702962835375809\n",
      "SubGD iter. 437/499: loss=5488.346359265586, w0=73.30400000000009, w1=14.710779695017813\n",
      "SubGD iter. 438/499: loss=5489.978520238785, w0=73.30400000000009, w1=14.7061744480285\n",
      "SubGD iter. 439/499: loss=5489.54973085854, w0=73.30400000000009, w1=14.701569201039188\n",
      "SubGD iter. 440/499: loss=5489.129424798226, w0=73.31100000000009, w1=14.702495736049336\n",
      "SubGD iter. 441/499: loss=5488.30394208612, w0=73.30400000000009, w1=14.71031259569134\n",
      "SubGD iter. 442/499: loss=5489.93464255937, w0=73.30400000000009, w1=14.705707348702028\n",
      "SubGD iter. 443/499: loss=5489.506713622231, w0=73.30400000000009, w1=14.701102101712715\n",
      "SubGD iter. 444/499: loss=5489.087268005024, w0=73.30400000000009, w1=14.71445074335418\n",
      "SubGD iter. 445/499: loss=5490.326403966633, w0=73.30400000000009, w1=14.709845496364867\n",
      "SubGD iter. 446/499: loss=5489.8908521526655, w0=73.30400000000009, w1=14.705240249375555\n",
      "SubGD iter. 447/499: loss=5489.463783658633, w0=73.30400000000009, w1=14.700635002386242\n",
      "SubGD iter. 448/499: loss=5489.045198484535, w0=73.30400000000009, w1=14.713983644027707\n",
      "SubGD iter. 449/499: loss=5490.281840389534, w0=73.30400000000009, w1=14.709378397038394\n",
      "SubGD iter. 450/499: loss=5489.847149018674, w0=73.30400000000009, w1=14.704773150049082\n",
      "SubGD iter. 451/499: loss=5489.420940967747, w0=73.30400000000009, w1=14.70016790305977\n",
      "SubGD iter. 452/499: loss=5489.003216236756, w0=73.30400000000009, w1=14.713516544701234\n",
      "SubGD iter. 453/499: loss=5490.237364085148, w0=73.30400000000009, w1=14.708911297711921\n",
      "SubGD iter. 454/499: loss=5489.803533157396, w0=73.30400000000009, w1=14.704306050722609\n",
      "SubGD iter. 455/499: loss=5489.378185549577, w0=73.30400000000009, w1=14.699700803733297\n",
      "SubGD iter. 456/499: loss=5488.96132126169, w0=73.30400000000009, w1=14.713049445374761\n",
      "SubGD iter. 457/499: loss=5490.192975053475, w0=73.30400000000009, w1=14.708444198385449\n",
      "SubGD iter. 458/499: loss=5489.76000456883, w0=73.30400000000009, w1=14.703838951396136\n",
      "SubGD iter. 459/499: loss=5489.335517404118, w0=73.30400000000009, w1=14.699233704406824\n",
      "SubGD iter. 460/499: loss=5488.919513559338, w0=73.30400000000009, w1=14.712582346048288\n",
      "SubGD iter. 461/499: loss=5490.148673294515, w0=73.30400000000009, w1=14.707977099058976\n",
      "SubGD iter. 462/499: loss=5489.716563252975, w0=73.30400000000009, w1=14.703371852069663\n",
      "SubGD iter. 463/499: loss=5489.292936531371, w0=73.31100000000009, w1=14.704298387079811\n",
      "SubGD iter. 464/499: loss=5488.468121906979, w0=73.30400000000009, w1=14.712115246721815\n",
      "SubGD iter. 465/499: loss=5490.104458808266, w0=73.30400000000009, w1=14.707509999732503\n",
      "SubGD iter. 466/499: loss=5489.673209209835, w0=73.30400000000009, w1=14.70290475274319\n",
      "SubGD iter. 467/499: loss=5489.250442931334, w0=73.31100000000009, w1=14.703831287753339\n",
      "SubGD iter. 468/499: loss=5488.425455193393, w0=73.30400000000009, w1=14.711648147395342\n",
      "SubGD iter. 469/499: loss=5490.060331594728, w0=73.30400000000009, w1=14.70704290040603\n",
      "SubGD iter. 470/499: loss=5489.629942439404, w0=73.30400000000009, w1=14.702437653416718\n",
      "SubGD iter. 471/499: loss=5489.208036604014, w0=73.31100000000009, w1=14.703364188426866\n",
      "SubGD iter. 472/499: loss=5488.382875752519, w0=73.30400000000009, w1=14.71118104806887\n",
      "SubGD iter. 473/499: loss=5490.016291653903, w0=73.30400000000009, w1=14.706575801079557\n",
      "SubGD iter. 474/499: loss=5489.586762941688, w0=73.30400000000009, w1=14.701970554090245\n",
      "SubGD iter. 475/499: loss=5489.165717549401, w0=73.31100000000009, w1=14.702897089100393\n",
      "SubGD iter. 476/499: loss=5488.340383584356, w0=73.30400000000009, w1=14.710713948742397\n",
      "SubGD iter. 477/499: loss=5489.972338985792, w0=73.30400000000009, w1=14.706108701753084\n",
      "SubGD iter. 478/499: loss=5489.543670716682, w0=73.30400000000009, w1=14.701503454763772\n",
      "SubGD iter. 479/499: loss=5489.123485767504, w0=73.31100000000009, w1=14.70242998977392\n",
      "SubGD iter. 480/499: loss=5488.297978688905, w0=73.30400000000009, w1=14.710246849415924\n",
      "SubGD iter. 481/499: loss=5489.928473590396, w0=73.30400000000009, w1=14.705641602426612\n",
      "SubGD iter. 482/499: loss=5489.500665764389, w0=73.30400000000009, w1=14.7010363554373\n",
      "SubGD iter. 483/499: loss=5489.081341258317, w0=73.30400000000009, w1=14.714384997078763\n",
      "SubGD iter. 484/499: loss=5490.320126170539, w0=73.30400000000009, w1=14.709779750089451\n",
      "SubGD iter. 485/499: loss=5489.884695467708, w0=73.30400000000009, w1=14.705174503100139\n",
      "SubGD iter. 486/499: loss=5489.457748084809, w0=73.30400000000009, w1=14.700569256110827\n",
      "SubGD iter. 487/499: loss=5489.039284021846, w0=73.30400000000009, w1=14.71391789775229\n",
      "SubGD iter. 488/499: loss=5490.275574877456, w0=73.30400000000009, w1=14.709312650762978\n",
      "SubGD iter. 489/499: loss=5489.841004617734, w0=73.30400000000009, w1=14.704707403773666\n",
      "SubGD iter. 490/499: loss=5489.414917677942, w0=73.30400000000009, w1=14.700102156784354\n",
      "SubGD iter. 491/499: loss=5488.9973140580805, w0=73.30400000000009, w1=14.713450798425818\n",
      "SubGD iter. 492/499: loss=5490.231110857087, w0=73.30400000000009, w1=14.708845551436506\n",
      "SubGD iter. 493/499: loss=5489.7974010404705, w0=73.30400000000009, w1=14.704240304447193\n",
      "SubGD iter. 494/499: loss=5489.372174543785, w0=73.30400000000009, w1=14.699635057457881\n",
      "SubGD iter. 495/499: loss=5488.955431367035, w0=73.30400000000009, w1=14.712983699099345\n",
      "SubGD iter. 496/499: loss=5490.186734109429, w0=73.30400000000009, w1=14.708378452110033\n",
      "SubGD iter. 497/499: loss=5489.753884735919, w0=73.30400000000009, w1=14.70377320512072\n",
      "SubGD iter. 498/499: loss=5489.329518682343, w0=73.31100000000009, w1=14.704699740130868\n",
      "SubGD iter. 499/499: loss=5488.504852805013, w0=73.30400000000009, w1=14.712516599772872\n",
      "SubGD: execution time=0.012 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24de50c8d7924fc4aed13b2fbd5bc3c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses,\n",
    "        subgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Compute a stochastic subgradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic subgradient descent.\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "            \n",
    "            grad = compute_subgradient_mae(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * grad\n",
    "            \n",
    "            loss = compute_loss(y, tx, w)\n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "        \n",
    "\n",
    "\n",
    "        # ***************************************************\n",
    "        print(\n",
    "            \"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=1110698.452704146, w0=0.7, w1=0.08511671065222107\n",
      "SubSGD iter. 1/499: loss=1090749.2237528972, w0=1.4, w1=0.020323025907934056\n",
      "SubSGD iter. 2/499: loss=1069993.6307359545, w0=2.0999999999999996, w1=0.1294412233629399\n",
      "SubSGD iter. 3/499: loss=1050427.0275062707, w0=2.8, w1=0.06607455159113078\n",
      "SubSGD iter. 4/499: loss=1031312.3038440375, w0=3.5, w1=-0.04114771739943349\n",
      "SubSGD iter. 5/499: loss=1011411.960991275, w0=4.2, w1=0.021208122451195036\n",
      "SubSGD iter. 6/499: loss=992223.4849869765, w0=4.9, w1=-0.005499418548884704\n",
      "SubSGD iter. 7/499: loss=973059.2961680909, w0=5.6000000000000005, w1=-0.0025379445103320183\n",
      "SubSGD iter. 8/499: loss=955034.4284228226, w0=6.300000000000001, w1=-0.16158851747959477\n",
      "SubSGD iter. 9/499: loss=936208.2065046708, w0=7.000000000000001, w1=-0.14942519999173293\n",
      "SubSGD iter. 10/499: loss=918740.5079523559, w0=7.700000000000001, w1=-0.3348021518355016\n",
      "SubSGD iter. 11/499: loss=900255.3399075116, w0=8.4, w1=-0.31417205523189307\n",
      "SubSGD iter. 12/499: loss=881870.1371920416, w0=9.1, w1=-0.2772483031872522\n",
      "SubSGD iter. 13/499: loss=863353.7807309079, w0=9.799999999999999, w1=-0.18454841803710445\n",
      "SubSGD iter. 14/499: loss=844616.0262750816, w0=10.499999999999998, w1=-0.01944727256201806\n",
      "SubSGD iter. 15/499: loss=827363.890181204, w0=11.199999999999998, w1=-0.0757189988017205\n",
      "SubSGD iter. 16/499: loss=809307.9282465708, w0=11.899999999999997, w1=0.040355605729191404\n",
      "SubSGD iter. 17/499: loss=793454.9296873772, w0=12.599999999999996, w1=-0.1888611660676574\n",
      "SubSGD iter. 18/499: loss=775889.886750267, w0=13.299999999999995, w1=-0.09067641656273909\n",
      "SubSGD iter. 19/499: loss=759554.1903572162, w0=13.999999999999995, w1=-0.16929985048005047\n",
      "SubSGD iter. 20/499: loss=742225.475549009, w0=14.699999999999994, w1=-0.04420458795052065\n",
      "SubSGD iter. 21/499: loss=726138.7049336324, w0=15.399999999999993, w1=-0.09855665616731869\n",
      "SubSGD iter. 22/499: loss=709791.5678647675, w0=16.099999999999994, w1=-0.07450458322544462\n",
      "SubSGD iter. 23/499: loss=692500.8624598676, w0=16.799999999999994, w1=0.14705351843497674\n",
      "SubSGD iter. 24/499: loss=676831.7562699063, w0=17.499999999999993, w1=0.12161485885001327\n",
      "SubSGD iter. 25/499: loss=661163.6584260567, w0=18.199999999999992, w1=0.1301622775191513\n",
      "SubSGD iter. 26/499: loss=645822.6978954263, w0=18.89999999999999, w1=0.11586604559221451\n",
      "SubSGD iter. 27/499: loss=631131.6556330262, w0=19.59999999999999, w1=0.022852515070370574\n",
      "SubSGD iter. 28/499: loss=616178.907544251, w0=20.29999999999999, w1=0.009316855261561651\n",
      "SubSGD iter. 29/499: loss=601110.2572539421, w0=20.99999999999999, w1=0.04974520744835953\n",
      "SubSGD iter. 30/499: loss=585636.5214767016, w0=21.69999999999999, w1=0.19513013559731177\n",
      "SubSGD iter. 31/499: loss=572505.8742964845, w0=22.399999999999988, w1=-0.03280161014686808\n",
      "SubSGD iter. 32/499: loss=556832.2123969254, w0=23.099999999999987, w1=0.2151427410555634\n",
      "SubSGD iter. 33/499: loss=543421.0561730107, w0=23.799999999999986, w1=0.10340399101420247\n",
      "SubSGD iter. 34/499: loss=529135.307589073, w0=24.499999999999986, w1=0.1785208437311241\n",
      "SubSGD iter. 35/499: loss=514546.10777032137, w0=25.199999999999985, w1=0.34210471176114327\n",
      "SubSGD iter. 36/499: loss=500363.72750906844, w0=25.899999999999984, w1=0.4699227221878438\n",
      "SubSGD iter. 37/499: loss=487143.42437061324, w0=26.599999999999984, w1=0.4615834446942343\n",
      "SubSGD iter. 38/499: loss=474121.7533430135, w0=27.299999999999983, w1=0.45277986739352866\n",
      "SubSGD iter. 39/499: loss=462033.3315624665, w0=27.999999999999982, w1=0.3132369294248926\n",
      "SubSGD iter. 40/499: loss=449758.0777089173, w0=28.69999999999998, w1=0.24213854448692707\n",
      "SubSGD iter. 41/499: loss=437386.0964425904, w0=29.39999999999998, w1=0.22264605953694613\n",
      "SubSGD iter. 42/499: loss=424639.71158611344, w0=30.09999999999998, w1=0.3034452542295805\n",
      "SubSGD iter. 43/499: loss=412809.83262946695, w0=30.79999999999998, w1=0.25745148652858396\n",
      "SubSGD iter. 44/499: loss=400092.52551375487, w0=31.49999999999998, w1=0.4027742887909265\n",
      "SubSGD iter. 45/499: loss=389147.7104300989, w0=32.19999999999998, w1=0.2694217119337407\n",
      "SubSGD iter. 46/499: loss=377528.2092243242, w0=32.899999999999984, w1=0.28993183564717306\n",
      "SubSGD iter. 47/499: loss=365791.1173925285, w0=33.59999999999999, w1=0.36592532289879537\n",
      "SubSGD iter. 48/499: loss=354440.0471926293, w0=34.29999999999999, w1=0.40851931949018305\n",
      "SubSGD iter. 49/499: loss=344629.5005173387, w0=34.99999999999999, w1=0.21358169786560596\n",
      "SubSGD iter. 50/499: loss=333185.56515715196, w0=35.699999999999996, w1=0.34122001835201526\n",
      "SubSGD iter. 51/499: loss=322619.2776521863, w0=36.4, w1=0.3488720352716221\n",
      "SubSGD iter. 52/499: loss=311347.2897168138, w0=37.1, w1=0.5171000790731103\n",
      "SubSGD iter. 53/499: loss=301464.81035539904, w0=37.800000000000004, w1=0.47265404722337634\n",
      "SubSGD iter. 54/499: loss=292685.1899513314, w0=38.50000000000001, w1=0.2678721089517999\n",
      "SubSGD iter. 55/499: loss=282226.45217424346, w0=39.20000000000001, w1=0.39507973026503673\n",
      "SubSGD iter. 56/499: loss=271487.51078896684, w0=39.90000000000001, w1=0.6090369629023771\n",
      "SubSGD iter. 57/499: loss=261291.6388359522, w0=40.600000000000016, w1=0.7629261389672863\n",
      "SubSGD iter. 58/499: loss=251726.51494342508, w0=41.30000000000002, w1=0.8386281118298008\n",
      "SubSGD iter. 59/499: loss=242804.74774108047, w0=42.00000000000002, w1=0.8325304615257041\n",
      "SubSGD iter. 60/499: loss=234433.72475871528, w0=42.700000000000024, w1=0.7616145074957137\n",
      "SubSGD iter. 61/499: loss=224984.2780477318, w0=43.40000000000003, w1=0.9241653869105655\n",
      "SubSGD iter. 62/499: loss=216374.7389639166, w0=44.10000000000003, w1=0.9689766063953439\n",
      "SubSGD iter. 63/499: loss=207801.9965575129, w0=44.80000000000003, w1=1.0435353579881181\n",
      "SubSGD iter. 64/499: loss=200026.03721683132, w0=45.500000000000036, w1=1.0065340514543633\n",
      "SubSGD iter. 65/499: loss=191875.41890400066, w0=46.20000000000004, w1=1.0756805260101023\n",
      "SubSGD iter. 66/499: loss=184726.48336760976, w0=46.90000000000004, w1=0.9949350039963745\n",
      "SubSGD iter. 67/499: loss=178293.32985068136, w0=47.600000000000044, w1=0.8191837157356262\n",
      "SubSGD iter. 68/499: loss=170977.17426986317, w0=48.30000000000005, w1=0.8421076346149619\n",
      "SubSGD iter. 69/499: loss=164751.2471598757, w0=49.00000000000005, w1=0.7017976731085495\n",
      "SubSGD iter. 70/499: loss=158825.37672282604, w0=49.70000000000005, w1=0.5442171990825506\n",
      "SubSGD iter. 71/499: loss=151905.0973223544, w0=50.400000000000055, w1=0.6012631454863626\n",
      "SubSGD iter. 72/499: loss=144059.91238969937, w0=51.056250000000055, w1=0.9355460064987937\n",
      "SubSGD iter. 73/499: loss=136379.70596801065, w0=51.712500000000055, w1=1.2796738242372523\n",
      "SubSGD iter. 74/499: loss=130340.9832753873, w0=52.368750000000055, w1=1.3501321514698958\n",
      "SubSGD iter. 75/499: loss=124130.15579611216, w0=53.025000000000055, w1=1.4870855838439019\n",
      "SubSGD iter. 76/499: loss=117135.90433571397, w0=53.72500000000006, w1=1.7441238564083243\n",
      "SubSGD iter. 77/499: loss=110398.92410736231, w0=54.38125000000006, w1=2.061765728499579\n",
      "SubSGD iter. 78/499: loss=104904.73174200246, w0=54.90625000000006, w1=2.368976502321567\n",
      "SubSGD iter. 79/499: loss=99324.85480429936, w0=55.518750000000054, w1=2.591357671004691\n",
      "SubSGD iter. 80/499: loss=94656.9006032346, w0=56.087500000000055, w1=2.7207657983563656\n",
      "SubSGD iter. 81/499: loss=89436.7753458849, w0=56.78750000000006, w1=2.807343899579017\n",
      "SubSGD iter. 82/499: loss=85123.37853874652, w0=57.400000000000055, w1=2.8634977708200506\n",
      "SubSGD iter. 83/499: loss=80892.69131018975, w0=57.88125000000006, w1=3.1142842609968535\n",
      "SubSGD iter. 84/499: loss=76716.91079387335, w0=58.45000000000006, w1=3.2598104714057863\n",
      "SubSGD iter. 85/499: loss=72221.82327932955, w0=58.97500000000006, w1=3.5676064713615654\n",
      "SubSGD iter. 86/499: loss=68853.13178459874, w0=59.54375000000006, w1=3.59028691470492\n",
      "SubSGD iter. 87/499: loss=65501.22036045643, w0=59.93750000000006, w1=3.860977397343343\n",
      "SubSGD iter. 88/499: loss=63022.78554935559, w0=60.28750000000006, w1=3.9998227597971994\n",
      "SubSGD iter. 89/499: loss=59306.059070663505, w0=60.72500000000006, w1=4.344281315034684\n",
      "SubSGD iter. 90/499: loss=56268.70165308092, w0=61.16250000000006, w1=4.547894773252625\n",
      "SubSGD iter. 91/499: loss=52998.29037335262, w0=61.68750000000006, w1=4.727525911678204\n",
      "SubSGD iter. 92/499: loss=50128.510640264634, w0=62.12500000000006, w1=4.939544007628699\n",
      "SubSGD iter. 93/499: loss=47027.16684822679, w0=62.47500000000006, w1=5.345303139524577\n",
      "SubSGD iter. 94/499: loss=44580.13961099077, w0=62.73750000000006, w1=5.705427715332439\n",
      "SubSGD iter. 95/499: loss=41837.667081303785, w0=63.04375000000006, w1=6.121983675484533\n",
      "SubSGD iter. 96/499: loss=39062.07168264944, w0=63.43750000000006, w1=6.470075401841694\n",
      "SubSGD iter. 97/499: loss=37038.3122492295, w0=63.962500000000055, w1=6.450731842206476\n",
      "SubSGD iter. 98/499: loss=34364.69426000079, w0=64.35625000000006, w1=6.8276980116700825\n",
      "SubSGD iter. 99/499: loss=32764.629252721454, w0=64.48750000000005, w1=7.201819167938092\n",
      "SubSGD iter. 100/499: loss=30653.575785870653, w0=64.88125000000005, w1=7.447147754552819\n",
      "SubSGD iter. 101/499: loss=27996.59753653941, w0=65.36250000000005, w1=7.819355912934263\n",
      "SubSGD iter. 102/499: loss=26343.690313454, w0=65.84375000000006, w1=7.859777138130896\n",
      "SubSGD iter. 103/499: loss=24260.79551820073, w0=66.28125000000006, w1=8.152764360783976\n",
      "SubSGD iter. 104/499: loss=22848.032119227344, w0=66.54375000000006, w1=8.417145616103033\n",
      "SubSGD iter. 105/499: loss=21276.090231480015, w0=66.80625000000006, w1=8.774915552702893\n",
      "SubSGD iter. 106/499: loss=19163.923035290514, w0=67.33125000000007, w1=9.106360268817648\n",
      "SubSGD iter. 107/499: loss=17428.328289748744, w0=67.68125000000006, w1=9.532011569862052\n",
      "SubSGD iter. 108/499: loss=16105.618533810763, w0=68.11875000000006, w1=9.696343712845284\n",
      "SubSGD iter. 109/499: loss=14974.25113855491, w0=68.46875000000006, w1=9.901601480406121\n",
      "SubSGD iter. 110/499: loss=13666.441415250707, w0=68.77500000000006, w1=10.29811685161387\n",
      "SubSGD iter. 111/499: loss=12454.727131432426, w0=69.08125000000007, w1=10.697482992383653\n",
      "SubSGD iter. 112/499: loss=11650.819927413488, w0=69.38750000000007, w1=10.877213973415252\n",
      "SubSGD iter. 113/499: loss=11115.508374784857, w0=69.51875000000007, w1=11.103820355258001\n",
      "SubSGD iter. 114/499: loss=10007.38332376197, w0=69.95625000000007, w1=11.436477035139669\n",
      "SubSGD iter. 115/499: loss=9331.883677265978, w0=70.17500000000007, w1=11.751770974436205\n",
      "SubSGD iter. 116/499: loss=8589.358856154331, w0=70.48125000000007, w1=12.081757902087652\n",
      "SubSGD iter. 117/499: loss=7946.958439975057, w0=70.78750000000008, w1=12.388795144516282\n",
      "SubSGD iter. 118/499: loss=7546.82004820102, w0=71.13750000000007, w1=12.420627994145995\n",
      "SubSGD iter. 119/499: loss=7074.310753977241, w0=71.40000000000008, w1=12.714556440794253\n",
      "SubSGD iter. 120/499: loss=6744.2902517084785, w0=71.75000000000007, w1=12.775224937468355\n",
      "SubSGD iter. 121/499: loss=6392.723270165616, w0=71.96875000000007, w1=13.093880085172607\n",
      "SubSGD iter. 122/499: loss=6391.022716801361, w0=72.01250000000007, w1=13.045816370413515\n",
      "SubSGD iter. 123/499: loss=6245.305937084653, w0=72.01250000000007, w1=13.328535646953139\n",
      "SubSGD iter. 124/499: loss=6011.087046234556, w0=72.31875000000008, w1=13.454321326584443\n",
      "SubSGD iter. 125/499: loss=5838.167457113781, w0=72.49375000000008, w1=13.696131742464674\n",
      "SubSGD iter. 126/499: loss=5859.533878800283, w0=72.49375000000008, w1=13.63038459415109\n",
      "SubSGD iter. 127/499: loss=5899.158814582919, w0=72.36250000000008, w1=13.703304494266101\n",
      "SubSGD iter. 128/499: loss=5822.850263833636, w0=72.49375000000008, w1=13.74690565524455\n",
      "SubSGD iter. 129/499: loss=5800.672058950585, w0=72.53750000000008, w1=13.756007764366972\n",
      "SubSGD iter. 130/499: loss=5684.9864751856685, w0=72.88750000000007, w1=13.711913847353081\n",
      "SubSGD iter. 131/499: loss=5618.501524771227, w0=73.01875000000007, w1=13.820549786236008\n",
      "SubSGD iter. 132/499: loss=5642.911701918487, w0=72.97500000000007, w1=13.771658144872905\n",
      "SubSGD iter. 133/499: loss=5613.520335299252, w0=73.01875000000007, w1=13.839841491065274\n",
      "SubSGD iter. 134/499: loss=5529.356525271142, w0=73.19375000000007, w1=14.066301584288837\n",
      "SubSGD iter. 135/499: loss=5534.477163344767, w0=73.10625000000006, w1=14.145831316512965\n",
      "SubSGD iter. 136/499: loss=5515.974955432142, w0=73.28125000000006, w1=14.063651012385614\n",
      "SubSGD iter. 137/499: loss=5513.957384804642, w0=73.28125000000006, w1=14.076079384579701\n",
      "SubSGD iter. 138/499: loss=5496.934154432514, w0=73.28125000000006, w1=14.202603463255791\n",
      "SubSGD iter. 139/499: loss=5501.526448274471, w0=73.19375000000005, w1=14.307001970005528\n",
      "SubSGD iter. 140/499: loss=5479.39949683152, w0=73.32500000000005, w1=14.35035936753028\n",
      "SubSGD iter. 141/499: loss=5484.160546083732, w0=73.32500000000005, w1=14.276916690510053\n",
      "SubSGD iter. 142/499: loss=5464.444680261433, w0=73.50000000000004, w1=14.342071891517392\n",
      "SubSGD iter. 143/499: loss=5458.951967676415, w0=73.54375000000005, w1=14.468357124404735\n",
      "SubSGD iter. 144/499: loss=5460.906717530351, w0=73.50000000000004, w1=14.488664853409738\n",
      "SubSGD iter. 145/499: loss=5482.933919401032, w0=73.32500000000005, w1=14.658416697211422\n",
      "SubSGD iter. 146/499: loss=5495.097687462297, w0=73.23750000000004, w1=14.656739456846617\n",
      "SubSGD iter. 147/499: loss=5516.70430971181, w0=73.23750000000004, w1=14.850946972358672\n",
      "SubSGD iter. 148/499: loss=5532.454984048829, w0=73.32500000000005, w1=15.005788768561395\n",
      "SubSGD iter. 149/499: loss=5515.936285343335, w0=73.41250000000005, w1=14.970186291269966\n",
      "SubSGD iter. 150/499: loss=5513.178749100364, w0=73.23750000000005, w1=14.826673290250083\n",
      "SubSGD iter. 151/499: loss=5525.986962340533, w0=73.15000000000005, w1=14.808087883286568\n",
      "SubSGD iter. 152/499: loss=5515.132149548737, w0=73.28125000000004, w1=14.88261316765603\n",
      "SubSGD iter. 153/499: loss=5500.296941965888, w0=73.28125000000004, w1=14.778026426361148\n",
      "SubSGD iter. 154/499: loss=5494.957735424185, w0=73.28125000000004, w1=14.730071318484702\n",
      "SubSGD iter. 155/499: loss=5536.963382378324, w0=73.32500000000005, w1=15.02664128323657\n",
      "SubSGD iter. 156/499: loss=5525.579658776255, w0=73.36875000000005, w1=14.996842654815572\n",
      "SubSGD iter. 157/499: loss=5498.187612775051, w0=73.32500000000005, w1=14.806842425381278\n",
      "SubSGD iter. 158/499: loss=5502.247764248589, w0=73.23750000000004, w1=14.737475583804262\n",
      "SubSGD iter. 159/499: loss=5505.603119514442, w0=73.15000000000003, w1=14.568221086432018\n",
      "SubSGD iter. 160/499: loss=5518.9751980512765, w0=73.15000000000003, w1=14.750331244064089\n",
      "SubSGD iter. 161/499: loss=5531.649356341094, w0=73.10625000000003, w1=14.783402017338059\n",
      "SubSGD iter. 162/499: loss=5512.21059248059, w0=73.28125000000003, w1=14.864249046541309\n",
      "SubSGD iter. 163/499: loss=5502.07202549711, w0=73.19375000000002, w1=14.652307698474662\n",
      "SubSGD iter. 164/499: loss=5496.25238410615, w0=73.28125000000003, w1=14.742492301646314\n",
      "SubSGD iter. 165/499: loss=5506.241656017601, w0=73.15000000000003, w1=14.584111202948181\n",
      "SubSGD iter. 166/499: loss=5543.856922496713, w0=72.97500000000004, w1=14.45550826490651\n",
      "SubSGD iter. 167/499: loss=5533.762355695051, w0=73.01875000000004, w1=14.401325128955172\n",
      "SubSGD iter. 168/499: loss=5556.270529564971, w0=72.93125000000003, w1=14.530936531600823\n",
      "SubSGD iter. 169/499: loss=5534.856117881732, w0=73.01875000000004, w1=14.580591280555435\n",
      "SubSGD iter. 170/499: loss=5519.585781785712, w0=73.10625000000005, w1=14.661068459003147\n",
      "SubSGD iter. 171/499: loss=5495.586875121724, w0=73.28125000000004, w1=14.736181334735459\n",
      "SubSGD iter. 172/499: loss=5495.445348141687, w0=73.28125000000004, w1=14.734819427220105\n",
      "SubSGD iter. 173/499: loss=5488.754726690699, w0=73.23750000000004, w1=14.508242784268878\n",
      "SubSGD iter. 174/499: loss=5473.109481324853, w0=73.36875000000003, w1=14.379612749828121\n",
      "SubSGD iter. 175/499: loss=5471.264528912071, w0=73.36875000000003, w1=14.47271577417723\n",
      "SubSGD iter. 176/499: loss=5471.9308846743725, w0=73.36875000000003, w1=14.417905970358927\n",
      "SubSGD iter. 177/499: loss=5479.590961001948, w0=73.32500000000003, w1=14.346597101863905\n",
      "SubSGD iter. 178/499: loss=5477.744998656277, w0=73.36875000000003, w1=14.295673536239455\n",
      "SubSGD iter. 179/499: loss=5470.416430091339, w0=73.45625000000004, w1=14.290695089988915\n",
      "SubSGD iter. 180/499: loss=5467.07663957165, w0=73.41250000000004, w1=14.490321331434457\n",
      "SubSGD iter. 181/499: loss=5466.682039809511, w0=73.45625000000004, w1=14.600431255653612\n",
      "SubSGD iter. 182/499: loss=5473.439240626762, w0=73.45625000000004, w1=14.69783698523435\n",
      "SubSGD iter. 183/499: loss=5476.320725128475, w0=73.58750000000003, w1=14.78021994904713\n",
      "SubSGD iter. 184/499: loss=5475.293261292048, w0=73.67500000000004, w1=14.77196665378526\n",
      "SubSGD iter. 185/499: loss=5467.384781897734, w0=73.67500000000004, w1=14.695315085093073\n",
      "SubSGD iter. 186/499: loss=5467.285025947618, w0=73.54375000000005, w1=14.679957858828866\n",
      "SubSGD iter. 187/499: loss=5489.520615346582, w0=73.36875000000005, w1=14.777846179315215\n",
      "SubSGD iter. 188/499: loss=5528.87303241513, w0=73.50000000000004, w1=15.058799667553455\n",
      "SubSGD iter. 189/499: loss=5503.404322133514, w0=73.50000000000004, w1=14.936851256688474\n",
      "SubSGD iter. 190/499: loss=5496.488532026037, w0=73.41250000000004, w1=14.859466655446713\n",
      "SubSGD iter. 191/499: loss=5485.002276580613, w0=73.36875000000003, w1=14.737807285954887\n",
      "SubSGD iter. 192/499: loss=5501.6521239335625, w0=73.36875000000003, w1=14.865508557589273\n",
      "SubSGD iter. 193/499: loss=5484.773358105381, w0=73.45625000000004, w1=14.801301653374173\n",
      "SubSGD iter. 194/499: loss=5487.117352952984, w0=73.32500000000005, w1=14.708730148050334\n",
      "SubSGD iter. 195/499: loss=5506.867210759446, w0=73.15000000000005, w1=14.597687274088843\n",
      "SubSGD iter. 196/499: loss=5527.372605883634, w0=73.10625000000005, w1=14.746433113293412\n",
      "SubSGD iter. 197/499: loss=5539.568799161499, w0=73.06250000000004, w1=14.769530874085687\n",
      "SubSGD iter. 198/499: loss=5499.550058414645, w0=73.23750000000004, w1=14.710301070067302\n",
      "SubSGD iter. 199/499: loss=5528.4435871851265, w0=73.15000000000003, w1=14.826078373530196\n",
      "SubSGD iter. 200/499: loss=5496.193858094559, w0=73.19375000000004, w1=14.518104401657222\n",
      "SubSGD iter. 201/499: loss=5496.227149896686, w0=73.19375000000004, w1=14.520023932716773\n",
      "SubSGD iter. 202/499: loss=5494.1859685544105, w0=73.23750000000004, w1=14.643677864636441\n",
      "SubSGD iter. 203/499: loss=5486.666509503159, w0=73.32500000000005, w1=14.703842028083432\n",
      "SubSGD iter. 204/499: loss=5492.024008793561, w0=73.23750000000004, w1=14.607633364491686\n",
      "SubSGD iter. 205/499: loss=5482.326837293025, w0=73.28125000000004, w1=14.436362835511847\n",
      "SubSGD iter. 206/499: loss=5467.714348522402, w0=73.41250000000004, w1=14.41737661990168\n",
      "SubSGD iter. 207/499: loss=5467.439392928745, w0=73.45625000000004, w1=14.614785555428226\n",
      "SubSGD iter. 208/499: loss=5491.7193756030065, w0=73.23750000000004, w1=14.601728525041636\n",
      "SubSGD iter. 209/499: loss=5490.655120808437, w0=73.28125000000004, w1=14.683525024067716\n",
      "SubSGD iter. 210/499: loss=5492.5297056696545, w0=73.23750000000004, w1=14.616891314669788\n",
      "SubSGD iter. 211/499: loss=5481.235879958241, w0=73.36875000000003, w1=14.699011145859107\n",
      "SubSGD iter. 212/499: loss=5500.170729122934, w0=73.23750000000004, w1=14.71682461351235\n",
      "SubSGD iter. 213/499: loss=5495.535757007589, w0=73.23750000000004, w1=14.662691151416084\n",
      "SubSGD iter. 214/499: loss=5491.222692392347, w0=73.41250000000004, w1=14.82347477861406\n",
      "SubSGD iter. 215/499: loss=5515.049381611399, w0=73.19375000000004, w1=14.785665095373483\n",
      "SubSGD iter. 216/499: loss=5481.1008204062555, w0=73.45625000000004, w1=14.771763270434267\n",
      "SubSGD iter. 217/499: loss=5485.286700074621, w0=73.58750000000003, w1=14.846594757907896\n",
      "SubSGD iter. 218/499: loss=5473.141637959627, w0=73.63125000000004, w1=14.756485077535604\n",
      "SubSGD iter. 219/499: loss=5461.9559424692225, w0=73.63125000000004, w1=14.627062214801962\n",
      "SubSGD iter. 220/499: loss=5471.247684575204, w0=73.45625000000004, w1=14.671625507053006\n",
      "SubSGD iter. 221/499: loss=5457.781462973756, w0=73.58750000000003, w1=14.481459758980561\n",
      "SubSGD iter. 222/499: loss=5471.98474820242, w0=73.36875000000003, w1=14.53578841148682\n",
      "SubSGD iter. 223/499: loss=5490.6131367285325, w0=73.23750000000004, w1=14.577443589006261\n",
      "SubSGD iter. 224/499: loss=5550.600870378605, w0=73.01875000000004, w1=14.775246059337297\n",
      "SubSGD iter. 225/499: loss=5578.333084361422, w0=72.93125000000003, w1=14.812399559665659\n",
      "SubSGD iter. 226/499: loss=5554.451450158551, w0=73.01875000000004, w1=14.80582276318814\n",
      "SubSGD iter. 227/499: loss=5547.126318274345, w0=73.01875000000004, w1=14.7446884442102\n",
      "SubSGD iter. 228/499: loss=5543.186567806055, w0=73.01875000000004, w1=14.705168179364817\n",
      "SubSGD iter. 229/499: loss=5627.879019311236, w0=72.80000000000004, w1=14.875514864161339\n",
      "SubSGD iter. 230/499: loss=5546.728099414201, w0=72.97500000000004, w1=14.597211204275395\n",
      "SubSGD iter. 231/499: loss=5554.600792521938, w0=72.97500000000004, w1=14.70835752147566\n",
      "SubSGD iter. 232/499: loss=5507.689923864453, w0=73.15000000000003, w1=14.613520952646967\n",
      "SubSGD iter. 233/499: loss=5530.435116928934, w0=73.06250000000003, w1=14.677361281570573\n",
      "SubSGD iter. 234/499: loss=5495.96704293589, w0=73.19375000000002, w1=14.501466164167674\n",
      "SubSGD iter. 235/499: loss=5481.89909032458, w0=73.32500000000002, w1=14.643661609164297\n",
      "SubSGD iter. 236/499: loss=5492.687754044947, w0=73.23750000000001, w1=14.619662690456629\n",
      "SubSGD iter. 237/499: loss=5471.7546149351165, w0=73.41250000000001, w1=14.62933923969612\n",
      "SubSGD iter. 238/499: loss=5491.826205071961, w0=73.28125000000001, w1=14.697165110065693\n",
      "SubSGD iter. 239/499: loss=5494.517232704775, w0=73.23750000000001, w1=14.648537875359208\n",
      "SubSGD iter. 240/499: loss=5469.732869505537, w0=73.41250000000001, w1=14.359537796650653\n",
      "SubSGD iter. 241/499: loss=5459.737593958778, w0=73.63125000000001, w1=14.366997464892718\n",
      "SubSGD iter. 242/499: loss=5465.642247104791, w0=73.63125000000001, w1=14.272381077892161\n",
      "SubSGD iter. 243/499: loss=5463.007621793572, w0=73.71875000000001, w1=14.61954548597277\n",
      "SubSGD iter. 244/499: loss=5460.997739619697, w0=73.63125000000001, w1=14.6103081068191\n",
      "SubSGD iter. 245/499: loss=5470.036946350163, w0=73.67500000000001, w1=14.723675572302701\n",
      "SubSGD iter. 246/499: loss=5465.727278011983, w0=73.76250000000002, w1=14.633209018510005\n",
      "SubSGD iter. 247/499: loss=5475.893599966601, w0=73.45625000000001, w1=14.723927499213302\n",
      "SubSGD iter. 248/499: loss=5471.4891253841315, w0=73.45625000000001, w1=14.674682507203856\n",
      "SubSGD iter. 249/499: loss=5514.345556411295, w0=73.10625000000002, w1=14.566029462622746\n",
      "SubSGD iter. 250/499: loss=5539.448801410188, w0=73.01875000000001, w1=14.65999968214557\n",
      "SubSGD iter. 251/499: loss=5543.8602384383685, w0=73.01875000000001, w1=14.712394040853283\n",
      "SubSGD iter. 252/499: loss=5513.009873791785, w0=73.23750000000001, w1=14.825468297840006\n",
      "SubSGD iter. 253/499: loss=5518.312128401713, w0=73.23750000000001, w1=14.861510169300104\n",
      "SubSGD iter. 254/499: loss=5463.525286421369, w0=73.54375000000002, w1=14.627100329422621\n",
      "SubSGD iter. 255/499: loss=5470.352970114157, w0=73.50000000000001, w1=14.693418504678766\n",
      "SubSGD iter. 256/499: loss=5478.87241397561, w0=73.41250000000001, w1=14.718999991012936\n",
      "SubSGD iter. 257/499: loss=5487.949263062789, w0=73.325, w1=14.717490582713179\n",
      "SubSGD iter. 258/499: loss=5487.7091622578855, w0=73.325, w1=14.714995126415886\n",
      "SubSGD iter. 259/499: loss=5464.589489891143, w0=73.675, w1=14.660778331348856\n",
      "SubSGD iter. 260/499: loss=5470.388253376926, w0=73.5, w1=14.693823285187925\n",
      "SubSGD iter. 261/499: loss=5476.534777511288, w0=73.54375, w1=14.772298940157663\n",
      "SubSGD iter. 262/499: loss=5482.498485974755, w0=73.54375, w1=14.818905373161105\n",
      "SubSGD iter. 263/499: loss=5474.630509024895, w0=73.54375, w1=14.755788250099855\n",
      "SubSGD iter. 264/499: loss=5499.476977203221, w0=73.2375, w1=14.709520975131438\n",
      "SubSGD iter. 265/499: loss=5511.224821031655, w0=73.14999999999999, w1=14.667193144218313\n",
      "SubSGD iter. 266/499: loss=5469.580043084884, w0=73.4125, w1=14.58853509545153\n",
      "SubSGD iter. 267/499: loss=5481.812442819386, w0=73.45625, w1=14.777712632499439\n",
      "SubSGD iter. 268/499: loss=5510.594153396496, w0=73.28125, w1=14.853705374719159\n",
      "SubSGD iter. 269/499: loss=5563.501632063056, w0=73.15, w1=15.021646092920035\n",
      "SubSGD iter. 270/499: loss=5550.735396590603, w0=73.325, w1=15.08594335064083\n",
      "SubSGD iter. 271/499: loss=5538.692383828332, w0=73.28125, w1=15.008035158180173\n",
      "SubSGD iter. 272/499: loss=5603.203444522262, w0=73.15, w1=15.180375632287584\n",
      "SubSGD iter. 273/499: loss=5534.309293723021, w0=73.325, w1=15.01446315345991\n",
      "SubSGD iter. 274/499: loss=5523.71466256171, w0=73.2375, w1=14.895059612670135\n",
      "SubSGD iter. 275/499: loss=5486.503216324917, w0=73.325, w1=14.702045535776204\n",
      "SubSGD iter. 276/499: loss=5472.431581680999, w0=73.41250000000001, w1=14.639986194009808\n",
      "SubSGD iter. 277/499: loss=5516.200850133026, w0=73.19375000000001, w1=14.794817207648528\n",
      "SubSGD iter. 278/499: loss=5488.701116670578, w0=73.28125000000001, w1=14.658514047629112\n",
      "SubSGD iter. 279/499: loss=5501.51294910834, w0=73.36875000000002, w1=14.864614937651336\n",
      "SubSGD iter. 280/499: loss=5495.5558165960265, w0=73.41250000000002, w1=14.85334164090603\n",
      "SubSGD iter. 281/499: loss=5483.811918434716, w0=73.41250000000002, w1=14.765342641709546\n",
      "SubSGD iter. 282/499: loss=5485.213405483848, w0=73.45625000000003, w1=14.804663077496349\n",
      "SubSGD iter. 283/499: loss=5530.953057207753, w0=73.45625000000003, w1=15.056145878317274\n",
      "SubSGD iter. 284/499: loss=5519.030629149563, w0=73.41250000000002, w1=14.985590734886065\n",
      "SubSGD iter. 285/499: loss=5502.6374065222335, w0=73.41250000000002, w1=14.89762604858848\n",
      "SubSGD iter. 286/499: loss=5483.202971994431, w0=73.41250000000002, w1=14.760037960812777\n",
      "SubSGD iter. 287/499: loss=5500.804439820262, w0=73.36875000000002, w1=14.860033497897017\n",
      "SubSGD iter. 288/499: loss=5506.7095948553415, w0=73.32500000000002, w1=14.86591249645777\n",
      "SubSGD iter. 289/499: loss=5509.480856165758, w0=73.23750000000001, w1=14.799262232745576\n",
      "SubSGD iter. 290/499: loss=5531.40693392564, w0=73.06250000000001, w1=14.689069331571645\n",
      "SubSGD iter. 291/499: loss=5488.103274084123, w0=73.36875000000002, w1=14.765881823475583\n",
      "SubSGD iter. 292/499: loss=5528.860273542793, w0=73.15000000000002, w1=14.829039023178634\n",
      "SubSGD iter. 293/499: loss=5522.915724878317, w0=73.15000000000002, w1=14.78412387055256\n",
      "SubSGD iter. 294/499: loss=5488.723415797459, w0=73.23750000000003, w1=14.505740871174226\n",
      "SubSGD iter. 295/499: loss=5483.885909013467, w0=73.28125000000003, w1=14.572364872228336\n",
      "SubSGD iter. 296/499: loss=5457.864127876962, w0=73.58750000000003, w1=14.496834078720594\n",
      "SubSGD iter. 297/499: loss=5459.114169782362, w0=73.58750000000003, w1=14.55753795481087\n",
      "SubSGD iter. 298/499: loss=5467.152953615205, w0=73.41250000000004, w1=14.500101822443883\n",
      "SubSGD iter. 299/499: loss=5472.783666728141, w0=73.36875000000003, w1=14.388500382214202\n",
      "SubSGD iter. 300/499: loss=5471.353753399158, w0=73.36875000000003, w1=14.45437280743833\n",
      "SubSGD iter. 301/499: loss=5458.095815346755, w0=73.58750000000003, w1=14.435643916162677\n",
      "SubSGD iter. 302/499: loss=5465.728479070338, w0=73.45625000000004, w1=14.371835410391448\n",
      "SubSGD iter. 303/499: loss=5464.653049532443, w0=73.45625000000004, w1=14.402140494518065\n",
      "SubSGD iter. 304/499: loss=5461.659533503003, w0=73.50000000000004, w1=14.412998888952716\n",
      "SubSGD iter. 305/499: loss=5457.63127806392, w0=73.63125000000004, w1=14.51156184020129\n",
      "SubSGD iter. 306/499: loss=5459.630128447725, w0=73.54375000000003, w1=14.534397238090651\n",
      "SubSGD iter. 307/499: loss=5483.029678290341, w0=73.28125000000003, w1=14.546852618289654\n",
      "SubSGD iter. 308/499: loss=5537.108828397089, w0=73.01875000000003, w1=14.624918516382154\n",
      "SubSGD iter. 309/499: loss=5593.163931910683, w0=72.84375000000003, w1=14.714833224835475\n",
      "SubSGD iter. 310/499: loss=5597.795458693599, w0=72.80000000000003, w1=14.57281421273308\n",
      "SubSGD iter. 311/499: loss=5582.428861768411, w0=72.84375000000003, w1=14.534924943106919\n",
      "SubSGD iter. 312/499: loss=5533.992984925802, w0=73.01875000000003, w1=14.557469740463247\n",
      "SubSGD iter. 313/499: loss=5496.863631896507, w0=73.19375000000002, w1=14.403965147216654\n",
      "SubSGD iter. 314/499: loss=5523.307719933883, w0=73.10625000000002, w1=14.245554168242322\n",
      "SubSGD iter. 315/499: loss=5482.263833501523, w0=73.28125000000001, w1=14.440593948983787\n",
      "SubSGD iter. 316/499: loss=5482.223177845622, w0=73.28125000000001, w1=14.443619070429866\n",
      "SubSGD iter. 317/499: loss=5476.72049389202, w0=73.32500000000002, w1=14.427575032207335\n",
      "SubSGD iter. 318/499: loss=5471.286328673499, w0=73.36875000000002, w1=14.486564597402815\n",
      "SubSGD iter. 319/499: loss=5484.230391872216, w0=73.28125000000001, w1=14.580897885140626\n",
      "SubSGD iter. 320/499: loss=5482.120715674317, w0=73.32500000000002, w1=14.646928679738739\n",
      "SubSGD iter. 321/499: loss=5477.642944668976, w0=73.45625000000001, w1=14.740961721720629\n",
      "SubSGD iter. 322/499: loss=5481.040814357425, w0=73.41250000000001, w1=14.740345226129948\n",
      "SubSGD iter. 323/499: loss=5472.088382762556, w0=73.5875, w1=14.74322631037667\n",
      "SubSGD iter. 324/499: loss=5498.515390994246, w0=73.7625, w1=14.910156392352778\n",
      "SubSGD iter. 325/499: loss=5496.847070294189, w0=73.54375, w1=14.911054738672961\n",
      "SubSGD iter. 326/499: loss=5479.349451568305, w0=73.36875, w1=14.676785825344433\n",
      "SubSGD iter. 327/499: loss=5480.2174905973025, w0=73.45625000000001, w1=14.764207776962236\n",
      "SubSGD iter. 328/499: loss=5465.000590637039, w0=73.50000000000001, w1=14.619361851682198\n",
      "SubSGD iter. 329/499: loss=5479.73932946356, w0=73.36875000000002, w1=14.681576043650411\n",
      "SubSGD iter. 330/499: loss=5465.323312859149, w0=73.50000000000001, w1=14.624872358306023\n",
      "SubSGD iter. 331/499: loss=5463.640775044337, w0=73.45625000000001, w1=14.494422404227333\n",
      "SubSGD iter. 332/499: loss=5506.8212846066035, w0=73.15, w1=14.596742368862419\n",
      "SubSGD iter. 333/499: loss=5483.581877602368, w0=73.36875, w1=14.72388964265732\n",
      "SubSGD iter. 334/499: loss=5472.330960104809, w0=73.41250000000001, w1=14.638447751619026\n",
      "SubSGD iter. 335/499: loss=5471.791802077885, w0=73.45625000000001, w1=14.678449743238517\n",
      "SubSGD iter. 336/499: loss=5480.868360316, w0=73.32500000000002, w1=14.62754628720993\n",
      "SubSGD iter. 337/499: loss=5473.053054968151, w0=73.36875000000002, w1=14.570317587066839\n",
      "SubSGD iter. 338/499: loss=5482.364855280433, w0=73.36875000000002, w1=14.711311681462192\n",
      "SubSGD iter. 339/499: loss=5488.887303420981, w0=73.23750000000003, w1=14.434212719377385\n",
      "SubSGD iter. 340/499: loss=5464.117616621791, w0=73.45625000000003, w1=14.423411915783962\n",
      "SubSGD iter. 341/499: loss=5459.551761015747, w0=73.71875000000003, w1=14.41730267560135\n",
      "SubSGD iter. 342/499: loss=5463.589496800124, w0=73.80625000000003, w1=14.504159690393386\n",
      "SubSGD iter. 343/499: loss=5477.893876364695, w0=73.71875000000003, w1=14.784122440475192\n",
      "SubSGD iter. 344/499: loss=5471.4520565408675, w0=73.67500000000003, w1=14.737554102064346\n",
      "SubSGD iter. 345/499: loss=5490.607334661886, w0=73.45625000000003, w1=14.84337748071207\n",
      "SubSGD iter. 346/499: loss=5466.41061806771, w0=73.50000000000003, w1=14.642100380252353\n",
      "SubSGD iter. 347/499: loss=5488.398887394793, w0=73.50000000000003, w1=14.846688447940375\n",
      "SubSGD iter. 348/499: loss=5512.680296431401, w0=73.54375000000003, w1=14.994063471653623\n",
      "SubSGD iter. 349/499: loss=5537.499689433822, w0=73.28125000000003, w1=15.002404085557574\n",
      "SubSGD iter. 350/499: loss=5496.066817996053, w0=73.45625000000003, w1=14.878793651529087\n",
      "SubSGD iter. 351/499: loss=5477.074442120447, w0=73.45625000000003, w1=14.735548447123657\n",
      "SubSGD iter. 352/499: loss=5496.216580185087, w0=73.58750000000002, w1=14.914121008359436\n",
      "SubSGD iter. 353/499: loss=5475.475634354907, w0=73.41250000000002, w1=14.681152228994794\n",
      "SubSGD iter. 354/499: loss=5515.281237519941, w0=73.45625000000003, w1=14.984185611789586\n",
      "SubSGD iter. 355/499: loss=5536.950205250794, w0=73.19375000000002, w1=14.929113583367235\n",
      "SubSGD iter. 356/499: loss=5541.485351169272, w0=73.10625000000002, w1=14.854990395089592\n",
      "SubSGD iter. 357/499: loss=5539.575828995989, w0=73.10625000000002, w1=14.842188019014051\n",
      "SubSGD iter. 358/499: loss=5510.456168999717, w0=73.15000000000002, w1=14.656880196329721\n",
      "SubSGD iter. 359/499: loss=5564.857148300027, w0=72.97500000000002, w1=14.800372448168332\n",
      "SubSGD iter. 360/499: loss=5554.913985472125, w0=73.06250000000003, w1=14.87951132567645\n",
      "SubSGD iter. 361/499: loss=5547.243473060372, w0=73.06250000000003, w1=14.828843781669217\n",
      "SubSGD iter. 362/499: loss=5554.335830112345, w0=73.06250000000003, w1=14.87591591032847\n",
      "SubSGD iter. 363/499: loss=5598.165297950542, w0=72.88750000000003, w1=14.86206073563107\n",
      "SubSGD iter. 364/499: loss=5556.772041919587, w0=72.97500000000004, w1=14.73062349135505\n",
      "SubSGD iter. 365/499: loss=5597.839715696276, w0=72.97500000000004, w1=14.995629745158656\n",
      "SubSGD iter. 366/499: loss=5560.05673401508, w0=72.97500000000004, w1=14.761023927166537\n",
      "SubSGD iter. 367/499: loss=5552.991594427838, w0=72.97500000000004, w1=14.690370343554973\n",
      "SubSGD iter. 368/499: loss=5573.265174410936, w0=72.97500000000004, w1=14.859695973584454\n",
      "SubSGD iter. 369/499: loss=5580.025643931838, w0=72.88750000000003, w1=14.717722602730014\n",
      "SubSGD iter. 370/499: loss=5619.262176677142, w0=72.75625000000004, w1=14.680707096881457\n",
      "SubSGD iter. 371/499: loss=5579.909017783141, w0=72.88750000000003, w1=14.716514861895545\n",
      "SubSGD iter. 372/499: loss=5580.221155975428, w0=72.88750000000003, w1=14.719733850245404\n",
      "SubSGD iter. 373/499: loss=5620.641230500798, w0=72.75625000000004, w1=14.696886203644844\n",
      "SubSGD iter. 374/499: loss=5687.47344024533, w0=72.58125000000004, w1=14.689829530137184\n",
      "SubSGD iter. 375/499: loss=5679.42458674612, w0=72.62500000000004, w1=14.78498711413045\n",
      "SubSGD iter. 376/499: loss=5586.526080073671, w0=72.93125000000005, w1=14.868551269160312\n",
      "SubSGD iter. 377/499: loss=5605.09525102439, w0=72.93125000000005, w1=14.972870988545749\n",
      "SubSGD iter. 378/499: loss=5673.764766053105, w0=72.84375000000004, w1=15.154075378242613\n",
      "SubSGD iter. 379/499: loss=5673.72259606367, w0=72.93125000000005, w1=15.24402178255439\n",
      "SubSGD iter. 380/499: loss=5685.872329102967, w0=72.88750000000005, w1=15.242389952394397\n",
      "SubSGD iter. 381/499: loss=5606.239510703854, w0=73.06250000000004, w1=15.123537628469563\n",
      "SubSGD iter. 382/499: loss=5556.595672761679, w0=73.01875000000004, w1=14.821680211061718\n",
      "SubSGD iter. 383/499: loss=5511.377633520184, w0=73.15000000000003, w1=14.669177921434429\n",
      "SubSGD iter. 384/499: loss=5476.1977611618795, w0=73.41250000000004, w1=14.689759188955762\n",
      "SubSGD iter. 385/499: loss=5503.032227831437, w0=73.32500000000003, w1=14.84159441113668\n",
      "SubSGD iter. 386/499: loss=5519.482406225545, w0=73.32500000000003, w1=14.940598459500167\n",
      "SubSGD iter. 387/499: loss=5509.446220215258, w0=73.41250000000004, w1=14.936206281402088\n",
      "SubSGD iter. 388/499: loss=5497.997473642789, w0=73.41250000000004, w1=14.869173821987298\n",
      "SubSGD iter. 389/499: loss=5528.691064313641, w0=73.15000000000003, w1=14.82783975557835\n",
      "SubSGD iter. 390/499: loss=5498.732404759729, w0=73.28125000000003, w1=14.76479938264169\n",
      "SubSGD iter. 391/499: loss=5510.453671597088, w0=73.19375000000002, w1=14.746069051673961\n",
      "SubSGD iter. 392/499: loss=5510.148195397002, w0=73.28125000000003, w1=14.850744323301065\n",
      "SubSGD iter. 393/499: loss=5504.316862430982, w0=73.23750000000003, w1=14.756541829396392\n",
      "SubSGD iter. 394/499: loss=5503.354554475057, w0=73.19375000000002, w1=14.669615169205079\n",
      "SubSGD iter. 395/499: loss=5477.466620083857, w0=73.32500000000002, w1=14.553468060944846\n",
      "SubSGD iter. 396/499: loss=5488.647798706626, w0=73.23750000000001, w1=14.498597572339635\n",
      "SubSGD iter. 397/499: loss=5497.992619945708, w0=73.19375000000001, w1=14.371822592121555\n",
      "SubSGD iter. 398/499: loss=5476.401654029629, w0=73.325, w1=14.502580192130567\n",
      "SubSGD iter. 399/499: loss=5497.493384404378, w0=73.28125, w1=14.753878440495393\n",
      "SubSGD iter. 400/499: loss=5499.90706523746, w0=73.325, w1=14.819578892077685\n",
      "SubSGD iter. 401/499: loss=5524.588966220417, w0=73.28125, w1=14.937070719148812\n",
      "SubSGD iter. 402/499: loss=5479.610112518124, w0=73.36875, w1=14.680000869453542\n",
      "SubSGD iter. 403/499: loss=5512.947759794838, w0=73.45625000000001, w1=14.972580383580576\n",
      "SubSGD iter. 404/499: loss=5505.052468916241, w0=73.41250000000001, w1=14.911701192338533\n",
      "SubSGD iter. 405/499: loss=5517.179352617523, w0=73.28125000000001, w1=14.895002351753408\n",
      "SubSGD iter. 406/499: loss=5515.254972894131, w0=73.41250000000001, w1=14.966729628734866\n",
      "SubSGD iter. 407/499: loss=5505.116634318171, w0=73.325, w1=14.855569568184476\n",
      "SubSGD iter. 408/499: loss=5487.915745989253, w0=73.325, w1=14.717143776351227\n",
      "SubSGD iter. 409/499: loss=5495.218135310471, w0=73.28125, w1=14.732617870019288\n",
      "SubSGD iter. 410/499: loss=5499.942362909107, w0=73.2375, w1=14.714445104589087\n",
      "SubSGD iter. 411/499: loss=5484.492310950755, w0=73.4125, w1=14.771157068258717\n",
      "SubSGD iter. 412/499: loss=5465.490450596019, w0=73.54374999999999, w1=14.656664551663193\n",
      "SubSGD iter. 413/499: loss=5462.93323890186, w0=73.58749999999999, w1=14.636303750385872\n",
      "SubSGD iter. 414/499: loss=5474.274157138707, w0=73.45625, w1=14.707042846339178\n",
      "SubSGD iter. 415/499: loss=5459.458874890493, w0=73.54375, w1=14.52658243072476\n",
      "SubSGD iter. 416/499: loss=5461.4616666772745, w0=73.54375, w1=14.58796566417804\n",
      "SubSGD iter. 417/499: loss=5464.086013988652, w0=73.54375, w1=14.636092597354581\n",
      "SubSGD iter. 418/499: loss=5471.270302778328, w0=73.36875, w1=14.469556375513852\n",
      "SubSGD iter. 419/499: loss=5466.972442121963, w0=73.45625000000001, w1=14.345287541566627\n",
      "SubSGD iter. 420/499: loss=5459.274051743644, w0=73.63125000000001, w1=14.573170717884295\n",
      "SubSGD iter. 421/499: loss=5459.860578026378, w0=73.54375, w1=14.543506360268443\n",
      "SubSGD iter. 422/499: loss=5481.404826522411, w0=73.675, w1=14.819694508358328\n",
      "SubSGD iter. 423/499: loss=5532.121598904517, w0=73.58749999999999, w1=15.085404637243103\n",
      "SubSGD iter. 424/499: loss=5500.555040152634, w0=73.58749999999999, w1=14.938199205664466\n",
      "SubSGD iter. 425/499: loss=5470.63034123535, w0=73.45625, w1=14.663582966702142\n",
      "SubSGD iter. 426/499: loss=5464.728976241062, w0=73.54375, w1=14.645819638533585\n",
      "SubSGD iter. 427/499: loss=5487.038141903085, w0=73.36875, w1=14.756555369916061\n",
      "SubSGD iter. 428/499: loss=5479.464118791518, w0=73.41250000000001, w1=14.725005973217685\n",
      "SubSGD iter. 429/499: loss=5481.17727759067, w0=73.45625000000001, w1=14.77240819387432\n",
      "SubSGD iter. 430/499: loss=5494.987066982598, w0=73.50000000000001, w1=14.888705733899105\n",
      "SubSGD iter. 431/499: loss=5483.591112014853, w0=73.63125000000001, w1=14.837761352149165\n",
      "SubSGD iter. 432/499: loss=5499.4579190381255, w0=73.45625000000001, w1=14.899303820977252\n",
      "SubSGD iter. 433/499: loss=5510.994480878521, w0=73.23750000000001, w1=14.810753360070652\n",
      "SubSGD iter. 434/499: loss=5514.701582265948, w0=73.32500000000002, w1=14.91413608401158\n",
      "SubSGD iter. 435/499: loss=5480.447396738707, w0=73.63125000000002, w1=14.815361048505727\n",
      "SubSGD iter. 436/499: loss=5462.108872346976, w0=73.63125000000002, w1=14.629567455596769\n",
      "SubSGD iter. 437/499: loss=5477.584563807802, w0=73.50000000000003, w1=14.764767486247452\n",
      "SubSGD iter. 438/499: loss=5481.640862096704, w0=73.76250000000003, w1=14.798776811451125\n",
      "SubSGD iter. 439/499: loss=5498.939380608833, w0=73.63125000000004, w1=14.931584503723666\n",
      "SubSGD iter. 440/499: loss=5510.0551385867575, w0=73.80625000000003, w1=14.958548417732258\n",
      "SubSGD iter. 441/499: loss=5482.827429683648, w0=73.85000000000004, w1=14.758303962743783\n",
      "SubSGD iter. 442/499: loss=5477.927549961754, w0=73.76250000000003, w1=14.768636463192374\n",
      "SubSGD iter. 443/499: loss=5475.910030569938, w0=73.76250000000003, w1=14.750879943548698\n",
      "SubSGD iter. 444/499: loss=5463.118657570432, w0=73.76250000000003, w1=14.584168937592633\n",
      "SubSGD iter. 445/499: loss=5458.94125248835, w0=73.54375000000003, w1=14.476347931333613\n",
      "SubSGD iter. 446/499: loss=5471.958822355342, w0=73.41250000000004, w1=14.318782747101695\n",
      "SubSGD iter. 447/499: loss=5489.517102791812, w0=73.23750000000004, w1=14.405915221389739\n",
      "SubSGD iter. 448/499: loss=5498.110789681367, w0=73.19375000000004, w1=14.369016649940756\n",
      "SubSGD iter. 449/499: loss=5477.020101766931, w0=73.32500000000003, w1=14.537466687292538\n",
      "SubSGD iter. 450/499: loss=5483.95255715441, w0=73.28125000000003, w1=14.574073547725655\n",
      "SubSGD iter. 451/499: loss=5476.27736113631, w0=73.36875000000003, w1=14.634049740116083\n",
      "SubSGD iter. 452/499: loss=5472.949021971677, w0=73.41250000000004, w1=14.64768031874449\n",
      "SubSGD iter. 453/499: loss=5491.821858846137, w0=73.23750000000004, w1=14.603745402143808\n",
      "SubSGD iter. 454/499: loss=5491.622551899787, w0=73.23750000000004, w1=14.599792915713607\n",
      "SubSGD iter. 455/499: loss=5513.7313564466895, w0=73.19375000000004, w1=14.77484566519641\n",
      "SubSGD iter. 456/499: loss=5526.186856092332, w0=73.28125000000004, w1=14.945649437189426\n",
      "SubSGD iter. 457/499: loss=5496.917958631888, w0=73.41250000000004, w1=14.862254014397424\n",
      "SubSGD iter. 458/499: loss=5504.988470817056, w0=73.32500000000003, w1=14.854725148362983\n",
      "SubSGD iter. 459/499: loss=5467.601980076883, w0=73.41250000000004, w1=14.52899974751162\n",
      "SubSGD iter. 460/499: loss=5476.35510266903, w0=73.32500000000003, w1=14.497830106168195\n",
      "SubSGD iter. 461/499: loss=5482.959433410227, w0=73.28125000000003, w1=14.407069777761887\n",
      "SubSGD iter. 462/499: loss=5481.821520511292, w0=73.32500000000003, w1=14.308906749942937\n",
      "SubSGD iter. 463/499: loss=5476.647350280813, w0=73.32500000000003, w1=14.519872328815103\n",
      "SubSGD iter. 464/499: loss=5489.250312359121, w0=73.23750000000003, w1=14.535174564414838\n",
      "SubSGD iter. 465/499: loss=5504.157925799051, w0=73.15000000000002, w1=14.512218990624062\n",
      "SubSGD iter. 466/499: loss=5479.459676798014, w0=73.32500000000002, w1=14.349164778303903\n",
      "SubSGD iter. 467/499: loss=5492.129949446294, w0=73.36875000000002, w1=14.152693982807435\n",
      "SubSGD iter. 468/499: loss=5481.810569728375, w0=73.32500000000002, w1=14.309070963059993\n",
      "SubSGD iter. 469/499: loss=5508.889841982682, w0=73.15000000000002, w1=14.317613569108024\n",
      "SubSGD iter. 470/499: loss=5494.215817395452, w0=73.23750000000003, w1=14.307288223549618\n",
      "SubSGD iter. 471/499: loss=5514.870416966956, w0=73.10625000000003, w1=14.371864452478333\n",
      "SubSGD iter. 472/499: loss=5493.477055933952, w0=73.28125000000003, w1=14.236333012199081\n",
      "SubSGD iter. 473/499: loss=5496.000298667325, w0=73.28125000000003, w1=14.211290234150303\n",
      "SubSGD iter. 474/499: loss=5491.683874112796, w0=73.23750000000003, w1=14.350387469200383\n",
      "SubSGD iter. 475/499: loss=5471.2855102530775, w0=73.36875000000002, w1=14.46503522614528\n",
      "SubSGD iter. 476/499: loss=5485.7211770106, w0=73.28125000000001, w1=14.33961852601201\n",
      "SubSGD iter. 477/499: loss=5484.3511527215605, w0=73.28125000000001, w1=14.367680002788383\n",
      "SubSGD iter. 478/499: loss=5480.785061544532, w0=73.32500000000002, w1=14.32524119255241\n",
      "SubSGD iter. 479/499: loss=5507.667806231527, w0=73.28125000000001, w1=14.117581097728323\n",
      "SubSGD iter. 480/499: loss=5492.133578495958, w0=73.45625000000001, w1=14.097796999958316\n",
      "SubSGD iter. 481/499: loss=5476.63630947691, w0=73.32500000000002, w1=14.432166849035237\n",
      "SubSGD iter. 482/499: loss=5481.794848833178, w0=73.32500000000002, w1=14.30930698999807\n",
      "SubSGD iter. 483/499: loss=5468.013745213202, w0=73.41250000000002, w1=14.405710812657043\n",
      "SubSGD iter. 484/499: loss=5471.496113287242, w0=73.36875000000002, w1=14.441545593538017\n",
      "SubSGD iter. 485/499: loss=5471.48276795484, w0=73.36875000000002, w1=14.508873083294858\n",
      "SubSGD iter. 486/499: loss=5475.928373810045, w0=73.36875000000002, w1=14.628440462409035\n",
      "SubSGD iter. 487/499: loss=5497.719820919136, w0=73.19375000000002, w1=14.572800364806813\n",
      "SubSGD iter. 488/499: loss=5512.456043795648, w0=73.19375000000002, w1=14.763990582368418\n",
      "SubSGD iter. 489/499: loss=5504.862401200768, w0=73.15000000000002, w1=14.545387424779127\n",
      "SubSGD iter. 490/499: loss=5489.503033233151, w0=73.23750000000003, w1=14.544988699645039\n",
      "SubSGD iter. 491/499: loss=5512.805641385265, w0=73.15000000000002, w1=14.686825359730248\n",
      "SubSGD iter. 492/499: loss=5497.488209902911, w0=73.23750000000003, w1=14.687190411420255\n",
      "SubSGD iter. 493/499: loss=5480.344339151956, w0=73.36875000000002, w1=14.68879637305707\n",
      "SubSGD iter. 494/499: loss=5490.757945796621, w0=73.32500000000002, w1=14.744970064362668\n",
      "SubSGD iter. 495/499: loss=5496.596068009374, w0=73.45625000000001, w1=14.882062859835672\n",
      "SubSGD iter. 496/499: loss=5501.3049506646175, w0=73.45625000000001, w1=14.910067885852014\n",
      "SubSGD iter. 497/499: loss=5551.909090511218, w0=73.32500000000002, w1=15.090732896125287\n",
      "SubSGD iter. 498/499: loss=5538.754467094834, w0=73.41250000000002, w1=15.074539393121459\n",
      "SubSGD iter. 499/499: loss=5583.765818354301, w0=73.23750000000003, w1=15.165715147204171\n",
      "SubSGD: execution time=0.021 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2969f38a505942bf8ceb12d3e9bc5c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses,\n",
    "        subsgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
